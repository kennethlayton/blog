[{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"","title":""},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"LLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish - through a relatively simple decoder-only transformer architecture, parameters optimized through gradient descent over lots and lots of data - we\u0026rsquo;ve created what is essentially a natural language computer that mimics intelligence. These systems can be orchestrated with external tooling to create agents that can make our lives a lot easier in a lot of ways.\nHowever, as this paper from Google points out, the LLM architectures of today don\u0026rsquo;t yet mimic human intelligence. They have a couple key limitations, the biggest of which being the lack of neuroplasticity, the ability to change their structure over time in response to new experiences. In layman\u0026rsquo;s terms, once an LLM is trained, its memory becomes frozen, and the LLM can no longer learn. A fully pretrained LLM, then, ends up with long-term semantic memory encoded in their weights, a short-term working memory through their context window and the self-attention mechanism, but no way to consolidate useful short-term memories back into long-term memory.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish - through a relatively simple decoder-only transformer architecture, parameters optimized through gradient descent over lots and lots of data - we\u0026rsquo;ve created what is essentially a natural language computer that mimics intelligence. These systems can be orchestrated with external tooling to create agents that can make our lives a lot easier in a lot of ways.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"LLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish - through a relatively simple decoder-only transformer architecture, parameters optimized through gradient descent over lots and lots of data - we\u0026rsquo;ve created what is essentially a natural language computer that mimics intelligence. These systems can be orchestrated with external tooling to create agents that can make our lives a lot easier in a lot of ways.\nHowever, as this paper from Google points out, the LLM architectures of today don\u0026rsquo;t yet mimic human intelligence. They have a couple key limitations, the biggest of which being the lack of neuroplasticity, the ability to change their structure over time in response to new experiences. In layman\u0026rsquo;s terms, once an LLM is trained, its memory becomes frozen, and the LLM can no longer learn. A fully pretrained LLM, then, ends up with long-term semantic memory encoded in their weights, a short-term working memory through their context window and the self-attention mechanism, but no way to consolidate useful short-term memories back into long-term memory.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish - through a relatively simple decoder-only transformer architecture, parameters optimized through gradient descent over lots and lots of data - we\u0026rsquo;ve created what is essentially a natural language computer that mimics intelligence. These systems can be orchestrated with external tooling to create agents that can make our lives a lot easier in a lot of ways.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"LLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture - ust layers of masked self-attention and MLPs optimized via gradient descent - we\u0026rsquo;ve created what is essentially a natural language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today\u0026rsquo;s LLMs still fall short of anything resembling human intelligence. I was reading this paper from Google the other day, and what stood out to me was how they pointed out that LLms lack neuroplasticity, or the ability to change their structure over time in response to new experiences. In layman\u0026rsquo;s terms, once an LLM is trained, its memory becomes frozen, and the LLM can no longer learn. A fully pretrained LLM, then, ends up with long-term semantic memory encoded in their weights, a short-term working memory through their context window and the self-attention mechanism, but no way to consolidate useful short-term memories back into long-term memory.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture - ust layers of masked self-attention and MLPs optimized via gradient descent - we\u0026rsquo;ve created what is essentially a natural language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"LLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture - just layers of masked self-attention and MLPs optimized via gradient descent - we\u0026rsquo;ve created what is essentially a natural language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today\u0026rsquo;s LLMs still fall short of anything resembling human intelligence. I was reading this paper from Google the other day, and what stood out to me was how they pointed out that LLms lack neuroplasticity, or the ability to change their structure over time in response to new experiences. In layman\u0026rsquo;s terms, once an LLM is trained, its memory becomes frozen, and the LLM can no longer learn. A fully pretrained LLM, then, ends up with long-term semantic memory encoded in their weights, a short-term working memory through their context window and the self-attention mechanism, but no way to consolidate useful short-term memories back into long-term memory.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture - just layers of masked self-attention and MLPs optimized via gradient descent - we\u0026rsquo;ve created what is essentially a natural language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"LLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture - just layers of masked self-attention and MLPs optimized via gradient descent - we\u0026rsquo;ve created what is essentially a natural language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today\u0026rsquo;s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity, or the ability to change their structure over time in response to new experiences. In layman\u0026rsquo;s terms, once an LLM is trained, its memory becomes frozen, and the LLM can no longer learn. A fully pretrained LLM, then, ends up with long-term semantic memory encoded in their weights, a short-term working memory through their context window and the self-attention mechanism, but no way to consolidate useful short-term memories back into long-term memory.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture - just layers of masked self-attention and MLPs optimized via gradient descent - we\u0026rsquo;ve created what is essentially a natural language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"LLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture - just layers of masked self-attention and MLPs optimized via gradient descent - we\u0026rsquo;ve created what is essentially a natural language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today\u0026rsquo;s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity, or the ability to change their structure over time in response to new experiences. In layman\u0026rsquo;s terms, once an LLM is trained, its memory becomes frozen, and the LLM can no longer learn. A fully pretrained LLM, then, ends up with long-term semantic memory encoded in their weights, a short-term working memory through their context window and the self-attention mechanism, but no way to consolidate useful short-term memories back into long-term memory.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture - just layers of masked self-attention and MLPs optimized via gradient descent - we\u0026rsquo;ve created what is essentially a natural language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"LLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture - just layers of masked self-attention and MLPs optimized via gradient descent - we\u0026rsquo;ve created what is essentially a natural language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today\u0026rsquo;s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity, or the ability to change their structure over time in response to new experiences. In layman\u0026rsquo;s terms, once an LLM is trained, its memory becomes frozen, and the LLM can no longer learn. A fully pretrained LLM, then, ends up with long-term semantic memory encoded in their weights, a short-term working memory through their context window and the self-attention mechanism, but no built-in mechanism to convert useful short-term memories back into lasting knowledge.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture - just layers of masked self-attention and MLPs optimized via gradient descent - we\u0026rsquo;ve created what is essentially a natural language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"LLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture - just layers of masked self-attention and MLPs optimized via gradient descent - we\u0026rsquo;ve created what is essentially a natural language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today\u0026rsquo;s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity, or the ability to change their structure over time in response to new experiences. In layman\u0026rsquo;s terms, once an LLM is trained, its memory becomes frozen, and the LLM can no longer learn. A fully pretrained LLM, then, ends up with long-term semantic memory encoded in their weights, a short-term working memory through their context window and the self-attention mechanism, but no built-in mechanism to convert useful short-term memories back into lasting knowledge.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture - just layers of masked self-attention and MLPs optimized via gradient descent - we\u0026rsquo;ve created what is essentially a natural language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"LLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture - just layers of masked self-attention and MLPs optimized via gradient descent - we\u0026rsquo;ve created what is essentially a natural language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today\u0026rsquo;s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity, or the ability to change their structure over time in response to new experiences. In layman\u0026rsquo;s terms, once an LLM is trained, its memory becomes frozen, and the LLM can no longer learn. A fully pretrained LLM, then, ends up with long-term semantic memory encoded in its weights, a short-term working memory through its context window and the self-attention mechanism, but no built-in mechanism to convert useful short-term memories back into lasting knowledge.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture - just layers of masked self-attention and MLPs optimized via gradient descent - we\u0026rsquo;ve created what is essentially a natural language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"LLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture - just layers of masked self-attention and MLPs optimized via gradient descent - we\u0026rsquo;ve created what is essentially a natural language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today\u0026rsquo;s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity, or the ability to change their structure over time in response to new experiences. In layman\u0026rsquo;s terms, once an LLM is trained, its memory becomes frozen, and the LLM can no longer learn. A fully pretrained LLM, then, ends up with long-term semantic memory baked into its weights, a short-term working memory limited to its context window, but no built-in mechanism to convert useful short-term memories back into lasting knowledge.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture - just layers of masked self-attention and MLPs optimized via gradient descent - we\u0026rsquo;ve created what is essentially a natural language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"LLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As [this paper](https://abehrouz.github.io/files/NL.pdf) from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights, a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been stuck in my head for weeks. So I decided to try something: build a system that allows my personal LLM—KenLM—to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to build a loop where my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, short-term interactions become long-term semantic memory. It’s not biological neuroplasticity, but it’s the closest thing we can build today—and, as you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"LLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As [this paper](https://abehrouz.github.io/files/NL.pdf) from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights, a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to build a loop where my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, short-term interactions become long-term semantic memory. It’s not biological neuroplasticity, but it’s the closest thing we can build today—and, as you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"LLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As [this paper](https://abehrouz.github.io/files/NL.pdf) from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights, a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to build a loop where my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, short-term interactions become long-term semantic memory. It’s not biological neuroplasticity, but it’s the closest thing we can build today—and, as you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"LLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As [this paper](https://abehrouz.github.io/files/NL.pdf) from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights, a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"LLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As [this paper](https://abehrouz.github.io/files/NL.pdf) from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights, a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nThe Approach To understand what I did, we must first understand why I did it. Let\u0026rsquo;s start by looking at the biological processes I was trying to mimic.\nHow do humans learn? — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"LLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights, a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nThe Approach To understand what I did, we must first understand why I did it. Let\u0026rsquo;s start by looking at the biological processes I was trying to mimic.\nHow do humans learn? — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"LLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nThe Approach To understand what I did, we must first understand why I did it. Let\u0026rsquo;s start by looking at the biological processes I was trying to mimic.\nHow do humans learn? — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"LLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nThe Approach To understand what I did, we must first understand why I did it. Let\u0026rsquo;s start by looking at the biological processes I was trying to mimic.\nHow do humans learn? Humans are\nDesign — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"LLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes I was trying to mimic actually are. As it turns out, the word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. We juggle several, each serving a different purpose:\nSensory memory, which holds raw impressions for a fraction of a second. Short-term (or working) memory, which is where active reasoning and conscious thought happen. Long-term memory, which itself has multiple subtypes: Semantic memory: facts, concepts, stable knowledge. Episodic memory: lived experiences, situated in time and place. Procedural memory: skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo; - how to drive, how to multiply, what an apple is. Episodic memory is everything we personally experienced that shaped what we know - your first day at a new job, the last conversation you had with a friend, the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nDesign — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"LLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes I was trying to mimic actually are. As it turns out, the word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. We juggle several, each serving a different purpose:\nSensory memory, which holds raw impressions for a fraction of a second. Short-term (or working) memory, which is where active reasoning and conscious thought happen. Long-term memory, which itself has multiple subtypes: Semantic memory: facts, concepts, stable knowledge. Episodic memory: lived experiences, situated in time and place. Procedural memory: skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo; - how to drive, how to multiply, what an apple is. Episodic memory is everything we personally experienced that shaped what we know - your first day at a new job, the last conversation you had with a friend, the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nDesign — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"LLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes I was trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. We juggle several, each serving a different purpose:\nSensory memory, which holds raw impressions for a fraction of a second. Short-term (or working) memory, which is where active reasoning and conscious thought happen. Long-term memory, which itself has multiple subtypes: Semantic memory: facts, concepts, stable knowledge. Episodic memory: lived experiences, situated in time and place. Procedural memory: skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo; - how to drive, how to multiply, what an apple is. Episodic memory is everything we personally experienced that shaped what we know - your first day at a new job, the last conversation you had with a friend, the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nDesign — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"LLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes I was trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. We\u0026rsquo;re actually constantly moving information between several different systems, each serving a different purpose:\nSensory memory, which holds raw impressions for a fraction of a second. Short-term (or working) memory, which is where active reasoning and conscious thought happen. Long-term memory, which itself has multiple subtypes: Semantic memory: facts, concepts, stable knowledge. Episodic memory: lived experiences, situated in time and place. Procedural memory: skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo; - how to drive, how to multiply, what an apple is. Episodic memory is everything we personally experienced that shaped what we know - your first day at a new job, the last conversation you had with a friend, the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nDesign — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"LLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes I was trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different systems, each serving a different purpose:\nSensory memory, which holds raw impressions for a fraction of a second. Short-term (or working) memory, which is where active reasoning and conscious thought happen. Long-term memory, which itself has multiple subtypes: Semantic memory: facts, concepts, stable knowledge. Episodic memory: lived experiences, situated in time and place. Procedural memory: skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo; - how to drive, how to multiply, what an apple is. Episodic memory is everything we personally experienced that shaped what we know - your first day at a new job, the last conversation you had with a friend, the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nDesign — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"LLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes I was trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory, which holds raw impressions for a fraction of a second. Short-term (or working) memory, which is where active reasoning and conscious thought happen. Long-term memory, which itself has multiple subtypes: Semantic memory: facts, concepts, stable knowledge. Episodic memory: lived experiences, situated in time and place. Procedural memory: skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo; - how to drive, how to multiply, what an apple is. Episodic memory is everything we personally experienced that shaped what we know - your first day at a new job, the last conversation you had with a friend, the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nDesign — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"LLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes I was trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo; - how to drive, how to multiply, what an apple is. Episodic memory is everything we personally experienced that shaped what we know - your first day at a new job, the last conversation you had with a friend, the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nDesign — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"LLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes I was trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo; - how to drive, how to multiply, what an apple is. Episodic memory is everything we personally experienced that shaped what we know - your first day at a new job, the last conversation you had with a friend, the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nDesign — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"LLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes I was trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nDesign — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"LLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes I was trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nDesign — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes I was trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nDesign — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, it\u0026rsquo;s remarkable what they can accomplish. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes I was trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nDesign — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes I was trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, sort of-facts and concepts are encoded into their weights after training. Working memory, whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences.\nDesign — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes I was trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights after training. Working memory, whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences.\nDesign — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes I was trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences.\nDesign — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes I was trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences.\nDesign — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes I was trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nDesign — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes I was trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions\nIdeally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning\nIf I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time.\nReflect on new experiences\nThe model should recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning\nThis could be explicit “learning goals,” or—eventually—actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time\nNot by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThis whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights)\nAll the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions)\nEach meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals)\nA distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass involves a coordinated dance between these components.\nI’ll break down how the system works end-to-end.\nDesign — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes I was trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning\nIf I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time.\nReflect on new experiences\nThe model should recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning\nThis could be explicit “learning goals,” or—eventually—actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time\nNot by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThis whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights)\nAll the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions)\nEach meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals)\nA distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass involves a coordinated dance between these components.\nI’ll break down how the system works end-to-end.\nDesign — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes I was trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning\nIf I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time.\nReflect on new experiences\nThe model should recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning\nThis could be explicit “learning goals,” or—eventually—actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time\nNot by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThis whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights)\nAll the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions)\nEach meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals)\nA distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass involves a coordinated dance between these components.\nI’ll break down how the system works end-to-end.\nDesign — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes I was trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning\nIf I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time.\nReflect on new experiences\nThe model should recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning\nThis could be explicit “learning goals,” or—eventually—actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time\nNot by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThis whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights)\nAll the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions)\nEach meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals)\nA distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass involves a coordinated dance between these components.\nI’ll break down how the system works end-to-end.\nDesign — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes I was trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team, it should remember that. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time.\nReflect on new experiences\nThe model should recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning\nThis could be explicit “learning goals,” or—eventually—actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time\nNot by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThis whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights)\nAll the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions)\nEach meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals)\nA distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass involves a coordinated dance between these components.\nI’ll break down how the system works end-to-end.\nDesign — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes I was trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team, it should remember that. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. It should actually use its memory to shape its responses.\nReflect on new experiences\nThe model should recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning\nThis could be explicit “learning goals,” or—eventually—actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time\nNot by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThis whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights)\nAll the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions)\nEach meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals)\nA distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass involves a coordinated dance between these components.\nI’ll break down how the system works end-to-end.\nDesign — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes I was trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team, it should remember that and tend towards using metaphors and examples that include things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. It should actually use its memory to shape its responses.\nReflect on new experiences\nThe model should recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning\nThis could be explicit “learning goals,” or—eventually—actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time\nNot by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThis whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights)\nAll the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions)\nEach meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals)\nA distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass involves a coordinated dance between these components.\nI’ll break down how the system works end-to-end.\nDesign — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes I was trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team, it should remember that and tend towards using metaphors and examples that include things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences\nThe model should recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning\nThis could be explicit “learning goals,” or—eventually—actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time\nNot by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThis whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights)\nAll the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions)\nEach meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals)\nA distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass involves a coordinated dance between these components.\nI’ll break down how the system works end-to-end.\nDesign — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes I was trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences\nThe model should recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning\nThis could be explicit “learning goals,” or—eventually—actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time\nNot by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThis whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights)\nAll the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions)\nEach meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals)\nA distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass involves a coordinated dance between these components.\nI’ll break down how the system works end-to-end.\nDesign — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes I was trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning\nThis could be explicit “learning goals,” or—eventually—actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time\nNot by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThis whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights)\nAll the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions)\nEach meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals)\nA distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass involves a coordinated dance between these components.\nI’ll break down how the system works end-to-end.\nDesign — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes I was trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights)\nAll the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions)\nEach meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals)\nA distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass involves a coordinated dance between these components.\nI’ll break down how the system works end-to-end.\nDesign — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes I was trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights). All the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions). Each meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals). A distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass involves a coordinated dance between these components.\nI’ll break down how the system works end-to-end.\nDesign — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes I was trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights). All the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions). Each meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals). A distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass involves a coordinated dance between these components.\nI’ll break down how the system works end-to-end.\nCode Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes I was trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights). All the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions). Each meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals). A distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass involves a coordinated dance between these components.\nI’ll break down how the system works end-to-end.\nCode Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture—just layers of masked self-attention and MLPs optimized via gradient descent—we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes we\u0026rsquo;re trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights). All the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions). Each meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals). A distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass passes information between these components, using its working memory (context) as the vehicle. Let me break down how it works piece by piece.\nLayer 1: Episodic Memory This piece of the architecture is the most straightforward. To enable the model to remember specific experiences, every interaction is logged as an episode. Every episode captures the user prompt, agent response, and some metadata for memory management. It looks something like this:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-01T14:30:00\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;Can you help me write a README for this project?\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;Sure. Here\u0026#39;s a concise template...\u0026#34;, \u0026#34;access_count\u0026#34;: 3, \u0026#34;last_accessed\u0026#34;: \u0026#34;2025-12-03T10:15:00\u0026#34;, \u0026#34;is_protected\u0026#34;: false } To enable the model to retrieve relevant episodes at inference time, I utilize a technique known as retrieval-augmented generation (RAG). Simply put, I store the episode data in a vector store called ChromaDB. Embeddings are generated by sentence-transformers after every conversation. Every time I send a message, KenLM embeds my query and searches for the top-k most semantically similar episodes in its memory. This allows it to reference episodes related to my query, so if I ask about sports, it will pull up discussions where we talked about sports. If I ask about documentation, it will remember episodes where we talked about writing documentation.\nThis gives the model something resembling episodic recall: the ability to remember relevant experiences and bring them into working memory (the context window) when they\u0026rsquo;re useful.\nLayer 2: Reflective Memory Episodic memory alone would essentially be just a fancy search engine. Something that humans do that enables more complex learning is reflect on past episodes and draw generalized conclusions based on them. Taking inspiration from this process, I built an agentic reflection process into KenLM.\nWhen I exit a chat session, KenLM runs a reflection loop. A KenLM \u0026ldquo;reflection agent\u0026rdquo; analyzes the session\u0026rsquo;s episodes, searches for related patterns in past conversations, and updates a profile of hypotheses and learning goals.\nHere\u0026rsquo;s what the reflection system actually does:\nReview the session — What patterns appeared in my behavior? What did I ask about? How did I respond to KenLM\u0026rsquo;s answers?\nSearch for evidence — The reflection agent can call search_episodes(query) to find supporting patterns across all past conversations. This prevents overfitting to a single session.\nUpdate hypotheses — For each insight, it calls upsert_hypothesis() with a confidence score. Crucially, confidence only increases when patterns repeat across multiple episodes.\nMaintain learning goals — If the model notices something it doesn\u0026rsquo;t understand about me yet, it creates a learning goal—a question to explore in future interactions.\nHere\u0026rsquo;s what my actual profile looks like after a few weeks of use:\nCode Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e\n\u003cp\u003eHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As \u003ca href=\"https://abehrouz.github.io/files/NL.pdf\"\u003ethis paper\u003c/a\u003e from Google points out, LLMs lack \u003cstrong\u003eneuroplasticity\u003c/strong\u003e—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes \u003cem\u003efrozen\u003c/em\u003e. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes we\u0026rsquo;re trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights). All the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions). Each meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals). A distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass passes information between these components, using its working memory (context) as the vehicle. Let me break down how it works piece by piece.\nLayer 1: Episodic Memory This piece of the architecture is the most straightforward. To enable the model to remember specific experiences, every interaction is logged as an episode. Every episode captures the user prompt, agent response, and some metadata for memory management. It looks something like this:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-01T14:30:00\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;Can you help me write a README for this project?\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;Sure. Here\u0026#39;s a concise template...\u0026#34;, \u0026#34;access_count\u0026#34;: 3, \u0026#34;last_accessed\u0026#34;: \u0026#34;2025-12-03T10:15:00\u0026#34;, \u0026#34;is_protected\u0026#34;: false } To enable the model to retrieve relevant episodes at inference time, I utilize a technique known as retrieval-augmented generation (RAG). Simply put, I store the episode data in a vector store called ChromaDB. Embeddings are generated by sentence-transformers after every conversation. Every time I send a message, KenLM embeds my query and searches for the top-k most semantically similar episodes in its memory. This allows it to reference episodes related to my query, so if I ask about sports, it will pull up discussions where we talked about sports. If I ask about documentation, it will remember episodes where we talked about writing documentation.\nThis gives the model something resembling episodic recall: the ability to remember relevant experiences and bring them into working memory (the context window) when they\u0026rsquo;re useful.\nLayer 2: Reflective Memory Episodic memory alone would essentially be just a fancy search engine. Something that humans do that enables more complex learning is reflect on past episodes and draw generalized conclusions based on them. Taking inspiration from this process, I built an agentic reflection process into KenLM.\nWhen I exit a chat session, KenLM runs a reflection loop. A KenLM \u0026ldquo;reflection agent\u0026rdquo; analyzes the session\u0026rsquo;s episodes, searches for related patterns in past conversations, and updates a profile of hypotheses and learning goals.\nHere\u0026rsquo;s what the reflection system actually does:\nReviews the session — What patterns appeared in my behavior? What did I ask about? How did I respond to KenLM\u0026rsquo;s answers?\nSearches for evidence — The reflection agent can call a search_episodes(query) tool to find supporting patterns across all past conversations. This prevents overfitting to a single session.\nUpdates hypotheses — For each insight, it calls upsert_hypothesis() with a confidence score. Crucially, confidence only increases when patterns repeat across multiple episodes.\nMaintain learning goals — If the model notices something it doesn\u0026rsquo;t understand about me yet, it creates a learning goal—a question to explore in future interactions.\nHere\u0026rsquo;s what my actual profile looks like after a few weeks of use:\nCode Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e\n\u003cp\u003eHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As \u003ca href=\"https://abehrouz.github.io/files/NL.pdf\"\u003ethis paper\u003c/a\u003e from Google points out, LLMs lack \u003cstrong\u003eneuroplasticity\u003c/strong\u003e—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes \u003cem\u003efrozen\u003c/em\u003e. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes we\u0026rsquo;re trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights). All the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions). Each meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals). A distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass passes information between these components, using its working memory (context) as the vehicle. Let me break down how it works piece by piece.\nLayer 1: Episodic Memory This piece of the architecture is the most straightforward. To enable the model to remember specific experiences, every interaction is logged as an episode. Every episode captures the user prompt, agent response, and some metadata for memory management. It looks something like this:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-01T14:30:00\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;Can you help me write a README for this project?\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;Sure. Here\u0026#39;s a concise template...\u0026#34;, \u0026#34;access_count\u0026#34;: 3, \u0026#34;last_accessed\u0026#34;: \u0026#34;2025-12-03T10:15:00\u0026#34;, \u0026#34;is_protected\u0026#34;: false } To enable the model to retrieve relevant episodes at inference time, I utilize a technique known as retrieval-augmented generation (RAG). Simply put, I store the episode data in a vector store called ChromaDB. Embeddings are generated by sentence-transformers after every conversation. Every time I send a message, KenLM embeds my query and searches for the top-k most semantically similar episodes in its memory. This allows it to reference episodes related to my query, so if I ask about sports, it will pull up discussions where we talked about sports. If I ask about documentation, it will remember episodes where we talked about writing documentation.\nThis gives the model something resembling episodic recall: the ability to remember relevant experiences and bring them into working memory (the context window) when they\u0026rsquo;re useful.\nLayer 2: Reflective Memory Episodic memory alone would essentially be just a fancy search engine. Something that humans do that enables more complex learning is reflect on past episodes and draw generalized conclusions based on them. Taking inspiration from this process, I built an agentic reflection process into KenLM.\nWhen I exit a chat session, KenLM runs a reflection loop. A KenLM \u0026ldquo;reflection agent\u0026rdquo; analyzes the session\u0026rsquo;s episodes, searches for related patterns in past conversations, and updates a profile of hypotheses and learning goals.\nHere\u0026rsquo;s what the reflection system actually does:\nReviews the session — What patterns appeared in my behavior? What did I ask about? How did I respond to KenLM\u0026rsquo;s answers?\nSearches for evidence — The reflection agent can call a search_episodes(query) tool to find supporting patterns across all past conversations. This prevents overfitting to a single session.\nUpdates hypotheses — For each insight, it calls upsert_hypothesis() with a confidence score. Crucially, confidence only increases when patterns repeat across multiple episodes.\nMaintains learning goals — If the model notices something it doesn\u0026rsquo;t understand about me yet, it creates a learning goal—a question to explore in future interactions.\nHere\u0026rsquo;s what my actual profile looks like after a few weeks of use:\nCode Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e\n\u003cp\u003eHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As \u003ca href=\"https://abehrouz.github.io/files/NL.pdf\"\u003ethis paper\u003c/a\u003e from Google points out, LLMs lack \u003cstrong\u003eneuroplasticity\u003c/strong\u003e—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes \u003cem\u003efrozen\u003c/em\u003e. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes we\u0026rsquo;re trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights). All the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions). Each meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals). A distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass passes information between these components, using its working memory (context) as the vehicle. Let me break down how it works piece by piece.\nLayer 1: Episodic Memory This piece of the architecture is the most straightforward. To enable the model to remember specific experiences, every interaction is logged as an episode. Every episode captures the user prompt, agent response, and some metadata for memory management. It looks something like this:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-01T14:30:00\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;Can you help me write a README for this project?\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;Sure. Here\u0026#39;s a concise template...\u0026#34;, \u0026#34;access_count\u0026#34;: 3, \u0026#34;last_accessed\u0026#34;: \u0026#34;2025-12-03T10:15:00\u0026#34;, \u0026#34;is_protected\u0026#34;: false } To enable the model to retrieve relevant episodes at inference time, I utilize a technique known as retrieval-augmented generation (RAG). Simply put, I store the episode data in a vector store called ChromaDB. Embeddings are generated by sentence-transformers after every conversation. Every time I send a message, KenLM embeds my query and searches for the top-k most semantically similar episodes in its memory. This allows it to reference episodes related to my query, so if I ask about sports, it will pull up discussions where we talked about sports. If I ask about documentation, it will remember episodes where we talked about writing documentation.\nThis gives the model something resembling episodic recall: the ability to remember relevant experiences and bring them into working memory (the context window) when they\u0026rsquo;re useful.\nLayer 2: Reflective Memory Episodic memory alone would essentially be just a fancy search engine. Something that humans do that enables more complex learning is reflect on past episodes and draw generalized conclusions based on them. Taking inspiration from this process, I built an agentic reflection process into KenLM.\nWhen I exit a chat session, KenLM runs a reflection loop. A KenLM \u0026ldquo;reflection agent\u0026rdquo; analyzes the session\u0026rsquo;s episodes, searches for related patterns in past conversations, and updates a profile of hypotheses and learning goals.\nHere\u0026rsquo;s what the reflection system actually does:\nReviews the session — What patterns appeared in my behavior? What did I ask about? How did I respond to KenLM\u0026rsquo;s answers?\nSearches for evidence — The reflection agent can call a search_episodes(query) tool to find supporting patterns across all past conversations. This prevents overfitting to a single session.\nUpdates hypotheses — For each insight, it calls upsert_hypothesis() with a confidence score. Crucially, confidence only increases when patterns repeat across multiple episodes.\nMaintains learning goals — If the model notices something it doesn\u0026rsquo;t understand about me yet, it creates a learning goal—a question to explore in future interactions.\nHere\u0026rsquo;s what my actual profile looks like after a few weeks of use:\nCode Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e\n\u003cp\u003eHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As \u003ca href=\"https://abehrouz.github.io/files/NL.pdf\"\u003ethis paper\u003c/a\u003e from Google points out, LLMs lack \u003cstrong\u003eneuroplasticity\u003c/strong\u003e—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes \u003cem\u003efrozen\u003c/em\u003e. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes we\u0026rsquo;re trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights). All the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions). Each meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals). A distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass passes information between these components, using its working memory (context) as the vehicle. Let me break down how it works piece by piece.\nLayer 1: Episodic Memory This piece of the architecture is the most straightforward. To enable the model to remember specific experiences, every interaction is logged as an episode. Every episode captures the user prompt, agent response, and some metadata for memory management. It looks something like this:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-01T14:30:00\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;Can you help me write a README for this project?\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;Sure. Here\u0026#39;s a concise template...\u0026#34;, \u0026#34;access_count\u0026#34;: 3, \u0026#34;last_accessed\u0026#34;: \u0026#34;2025-12-03T10:15:00\u0026#34;, \u0026#34;is_protected\u0026#34;: false } To enable the model to retrieve relevant episodes at inference time, I utilize a technique known as retrieval-augmented generation (RAG). Simply put, I store the episode data in a vector store called ChromaDB. Embeddings are generated by sentence-transformers after every conversation. Every time I send a message, KenLM embeds my query and searches for the top-k most semantically similar episodes in its memory. This allows it to reference episodes related to my query, so if I ask about sports, it will pull up discussions where we talked about sports. If I ask about documentation, it will remember episodes where we talked about writing documentation.\nThis gives the model something resembling episodic recall: the ability to remember relevant experiences and bring them into working memory (the context window) when they\u0026rsquo;re useful.\nLayer 2: Reflective Memory Episodic memory alone would essentially be just a fancy search engine. Something that humans do that enables more complex learning is reflect on past episodes and draw generalized conclusions based on them. Taking inspiration from this process, I built an agentic reflection process into KenLM.\nWhen I exit a chat session, KenLM runs a reflection loop. A KenLM \u0026ldquo;reflection agent\u0026rdquo; analyzes the session\u0026rsquo;s episodes, searches for related patterns in past conversations, and updates a profile of hypotheses and learning goals.\nHere\u0026rsquo;s what the reflection system actually does:\nReviews the session — What patterns appeared in my behavior? What did I ask about? How did I respond to KenLM\u0026rsquo;s answers?\nSearches for evidence — The reflection agent can call a search_episodes(query) tool to find supporting patterns across all past conversations. This prevents overfitting to a single session.\nUpdates hypotheses — For each insight, it calls upsert_hypothesis() with a confidence score. Crucially, confidence only increases when patterns repeat across multiple episodes.\nMaintains learning goals — If the model notices something it doesn\u0026rsquo;t understand about me yet, it creates a learning goal—a question to explore in future interactions.\nHere\u0026rsquo;s what my actual profile looks like after a few weeks of use:\nCode Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e\n\u003cp\u003eHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As \u003ca href=\"https://abehrouz.github.io/files/NL.pdf\"\u003ethis paper\u003c/a\u003e from Google points out, LLMs lack \u003cstrong\u003eneuroplasticity\u003c/strong\u003e—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes \u003cem\u003efrozen\u003c/em\u003e. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes we\u0026rsquo;re trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights). All the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions). Each meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals). A distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass passes information between these components, using its working memory (context) as the vehicle. Let me break down how it works piece by piece.\nLayer 1: Episodic Memory This piece of the architecture is the most straightforward. To enable the model to remember specific experiences, every interaction is logged as an episode. Every episode captures the user prompt, agent response, and some metadata for memory management. It looks something like this:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-01T14:30:00\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;Can you help me write a README for this project?\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;Sure. Here\u0026#39;s a concise template...\u0026#34;, \u0026#34;access_count\u0026#34;: 3, \u0026#34;last_accessed\u0026#34;: \u0026#34;2025-12-03T10:15:00\u0026#34;, \u0026#34;is_protected\u0026#34;: false } To enable the model to retrieve relevant episodes at inference time, I utilize a technique known as retrieval-augmented generation (RAG). Simply put, I store the episode data in a vector store called ChromaDB. Embeddings are generated by sentence-transformers after every conversation. Every time I send a message, KenLM embeds my query and searches for the top-k most semantically similar episodes in its memory. This allows it to reference episodes related to my query, so if I ask about sports, it will pull up discussions where we talked about sports. If I ask about documentation, it will remember episodes where we talked about writing documentation.\nThis gives the model something resembling episodic recall: the ability to remember relevant experiences and bring them into working memory (the context window) when they\u0026rsquo;re useful.\nLayer 2: Reflective Memory Episodic memory alone would essentially be just a fancy search engine. Something that humans do that enables more complex learning is reflect on past episodes and draw generalized conclusions based on them. Taking inspiration from this process, I built an agentic reflection process into KenLM.\nWhen I exit a chat session, KenLM runs a reflection loop. A KenLM \u0026ldquo;reflection agent\u0026rdquo; analyzes the session\u0026rsquo;s episodes, searches for related patterns in past conversations, and updates a profile of hypotheses and learning goals.\nHere\u0026rsquo;s what the reflection system actually does:\nReviews the session — What patterns appeared in my behavior? What did I ask about? How did I respond to KenLM\u0026rsquo;s answers?\nSearches for evidence — The reflection agent can call a search_episodes(query) tool to find supporting patterns across all past conversations. This prevents overfitting to a single session.\nUpdates hypotheses — For each insight, it calls upsert_hypothesis() with a confidence score. Crucially, confidence only increases when patterns repeat across multiple episodes.\nMaintains learning goals — If the model notices something it doesn\u0026rsquo;t understand about me yet, it creates a learning goal—a question to explore in future interactions.\nHere\u0026rsquo;s what my actual profile looks like after a few weeks of use:\nThe reflection prompt is designed to be conservative. I explicitly instruct the model to start hypotheses with low confidence (0.2–0.4) and only raise confidence when it sees repeated evidence. This should hopefully prevent the kind of overconfident conclusions that can come from a single unusual interaction.\nCode Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e\n\u003cp\u003eHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As \u003ca href=\"https://abehrouz.github.io/files/NL.pdf\"\u003ethis paper\u003c/a\u003e from Google points out, LLMs lack \u003cstrong\u003eneuroplasticity\u003c/strong\u003e—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes \u003cem\u003efrozen\u003c/em\u003e. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes we\u0026rsquo;re trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights). All the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions). Each meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals). A distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass passes information between these components, using its working memory (context) as the vehicle. Let me break down how it works piece by piece.\nLayer 1: Episodic Memory This piece of the architecture is the most straightforward. To enable the model to remember specific experiences, every interaction is logged as an episode. Every episode captures the user prompt, agent response, and some metadata for memory management. It looks something like this:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-01T14:30:00\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;Can you help me write a README for this project?\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;Sure. Here\u0026#39;s a concise template...\u0026#34;, \u0026#34;access_count\u0026#34;: 3, \u0026#34;last_accessed\u0026#34;: \u0026#34;2025-12-03T10:15:00\u0026#34;, \u0026#34;is_protected\u0026#34;: false } To enable the model to retrieve relevant episodes at inference time, I utilize a technique known as retrieval-augmented generation (RAG). Simply put, I store the episode data in a vector store called ChromaDB. Embeddings are generated by sentence-transformers after every conversation. Every time I send a message, KenLM embeds my query and searches for the top-k most semantically similar episodes in its memory. This allows it to reference episodes related to my query, so if I ask about sports, it will pull up discussions where we talked about sports. If I ask about documentation, it will remember episodes where we talked about writing documentation.\nThis gives the model something resembling episodic recall: the ability to remember relevant experiences and bring them into working memory (the context window) when they\u0026rsquo;re useful.\nLayer 2: Reflective Memory Episodic memory alone would essentially be just a fancy search engine. Something that humans do that enables more complex learning is reflect on past episodes and draw generalized conclusions based on them. Taking inspiration from this process, I built an agentic reflection process into KenLM.\nWhen I exit a chat session, KenLM runs a reflection loop. A KenLM \u0026ldquo;reflection agent\u0026rdquo; analyzes the session\u0026rsquo;s episodes, searches for related patterns in past conversations, and updates a profile of hypotheses and learning goals.\nHere\u0026rsquo;s what the reflection system actually does:\nReviews the session — What patterns appeared in my behavior? What did I ask about? How did I respond to KenLM\u0026rsquo;s answers?\nSearches for evidence — The reflection agent can call a search_episodes(query) tool to find supporting patterns across all past conversations. This prevents overfitting to a single session.\nUpdates hypotheses — For each insight, it calls upsert_hypothesis() with a confidence score. Crucially, confidence only increases when patterns repeat across multiple episodes.\nMaintains learning goals — If the model notices something it doesn\u0026rsquo;t understand about me yet, it creates a learning goal—a question to explore in future interactions.\nHere\u0026rsquo;s what my actual profile looks like after a few weeks of use:\nThe reflection prompt is designed to be conservative. I explicitly instruct the model to start hypotheses with low confidence (0.2–0.4) and only raise confidence when it sees repeated evidence. This should hopefully prevent the kind of overconfident conclusions that can come from a single unusual interaction.\nCode Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e\n\u003cp\u003eHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As \u003ca href=\"https://abehrouz.github.io/files/NL.pdf\"\u003ethis paper\u003c/a\u003e from Google points out, LLMs lack \u003cstrong\u003eneuroplasticity\u003c/strong\u003e—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes \u003cem\u003efrozen\u003c/em\u003e. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes we\u0026rsquo;re trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights). All the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions). Each meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals). A distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass passes information between these components, using its working memory (context) as the vehicle. Let me break down how it works piece by piece.\nLayer 1: Episodic Memory This piece of the architecture is the most straightforward. To enable the model to remember specific experiences, every interaction is logged as an episode. Every episode captures the user prompt, agent response, and some metadata for memory management. It looks something like this:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-01T14:30:00\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;Can you help me write a README for this project?\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;Sure. Here\u0026#39;s a concise template...\u0026#34;, \u0026#34;access_count\u0026#34;: 3, \u0026#34;last_accessed\u0026#34;: \u0026#34;2025-12-03T10:15:00\u0026#34;, \u0026#34;is_protected\u0026#34;: false } To enable the model to retrieve relevant episodes at inference time, I utilize a technique known as retrieval-augmented generation (RAG). Simply put, I store the episode data in a vector store called ChromaDB. Embeddings are generated by sentence-transformers after every conversation. Every time I send a message, KenLM embeds my query and searches for the top-k most semantically similar episodes in its memory. This allows it to reference episodes related to my query, so if I ask about sports, it will pull up discussions where we talked about sports. If I ask about documentation, it will remember episodes where we talked about writing documentation.\nThis gives the model something resembling episodic recall: the ability to remember relevant experiences and bring them into working memory (the context window) when they\u0026rsquo;re useful.\nLayer 2: Reflective Memory Episodic memory alone would essentially be just a fancy search engine. Something that humans do that enables more complex learning is reflect on past episodes and draw generalized conclusions based on them. Taking inspiration from this process, I built an agentic reflection process into KenLM.\nWhen I exit a chat session, KenLM runs a reflection loop. A KenLM \u0026ldquo;reflection agent\u0026rdquo; analyzes the session\u0026rsquo;s episodes, searches for related patterns in past conversations, and updates a profile of hypotheses and learning goals.\nHere\u0026rsquo;s what the reflection system actually does:\nReviews the session — What patterns appeared in my behavior? What did I ask about? How did I respond to KenLM\u0026rsquo;s answers?\nSearches for evidence — The reflection agent can call a search_episodes(query) tool to find supporting patterns across all past conversations. This prevents overfitting to a single session.\nUpdates hypotheses — For each insight, it calls upsert_hypothesis() with a confidence score. Crucially, confidence only increases when patterns repeat across multiple episodes.\nMaintains learning goals — If the model notices something it doesn\u0026rsquo;t understand about me yet, it creates a learning goal—a question to explore in future interactions.\nHere\u0026rsquo;s what my actual profile looks like after a few weeks of use:\n{ \u0026#34;hypotheses\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_004\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;response_verbosity\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth prefers detailed and useful responses over concise ones.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.6, \u0026#34;evidence_episode_ids\u0026#34;: [ 91, 90, 88, 89, 159, 160 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_005\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;response_quality\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth is frustrated with inaccurate or misleading information.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.9, \u0026#34;evidence_episode_ids\u0026#34;: [ 90, 91, 89, 159, 160 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_006\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;control\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth values precision and control over changes in the project.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.9, \u0026#34;evidence_episode_ids\u0026#34;: [ 153, 88, 92 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_007\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;accuracy\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth is detail-oriented and expects accurate and precise information.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.6, \u0026#34;evidence_episode_ids\u0026#34;: [ 90, 91, 89, 92 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_008\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;transparency\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth values transparency and honesty in communication.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 89 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_009\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;task_compliance\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth expects tasks to be completed accurately and without unnecessary steps.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 154, 155, 156, 157, 158 ] } ], \u0026#34;learning_goals\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;learning_goal_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;How can we ensure that Kenneth\u0026#39;s instructions are followed without unnecessary repetition or recapitulation?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;closed\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-11-30T15:10:55.704097\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;lg_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;What are Kenneth\u0026#39;s specific expectations for the precision and detail in project updates?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-12-02T15:51:56.904564\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;lg_004\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;What are Kenneth\u0026#39;s specific expectations for the accuracy and completeness of tasks?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-12-02T13:57:40.150136\u0026#34; } ] } The reflection prompt is designed to be conservative. I explicitly instruct the model to start hypotheses with low confidence (0.2–0.4) and only raise confidence when it sees repeated evidence. This should hopefully prevent the kind of overconfident conclusions that can come from a single unusual interaction.\nCode Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e\n\u003cp\u003eHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As \u003ca href=\"https://abehrouz.github.io/files/NL.pdf\"\u003ethis paper\u003c/a\u003e from Google points out, LLMs lack \u003cstrong\u003eneuroplasticity\u003c/strong\u003e—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes \u003cem\u003efrozen\u003c/em\u003e. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes we\u0026rsquo;re trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights). All the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions). Each meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals). A distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass passes information between these components, using its working memory (context) as the vehicle. Let me break down how it works piece by piece.\nLayer 1: Episodic Memory This piece of the architecture is the most straightforward. To enable the model to remember specific experiences, every interaction is logged as an episode. Every episode captures the user prompt, agent response, and some metadata for memory management. It looks something like this:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-01T14:30:00\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;Can you help me write a README for this project?\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;Sure. Here\u0026#39;s a concise template...\u0026#34;, \u0026#34;access_count\u0026#34;: 3, \u0026#34;last_accessed\u0026#34;: \u0026#34;2025-12-03T10:15:00\u0026#34;, \u0026#34;is_protected\u0026#34;: false } To enable the model to retrieve relevant episodes at inference time, I utilize a technique known as retrieval-augmented generation (RAG). Simply put, I store the episode data in a vector store called ChromaDB. Embeddings are generated by sentence-transformers after every conversation. Every time I send a message, KenLM embeds my query and searches for the top-k most semantically similar episodes in its memory. This allows it to reference episodes related to my query, so if I ask about sports, it will pull up discussions where we talked about sports. If I ask about documentation, it will remember episodes where we talked about writing documentation.\nThis gives the model something resembling episodic recall: the ability to remember relevant experiences and bring them into working memory (the context window) when they\u0026rsquo;re useful.\nLayer 2: Reflective Memory Episodic memory alone would essentially be just a fancy search engine. Something that humans do that enables more complex learning is reflect on past episodes and draw generalized conclusions based on them. Taking inspiration from this process, I built an agentic reflection process into KenLM.\nWhen I exit a chat session, KenLM runs a reflection loop. A KenLM \u0026ldquo;reflection agent\u0026rdquo; analyzes the session\u0026rsquo;s episodes, searches for related patterns in past conversations, and updates a profile of hypotheses and learning goals.\nHere\u0026rsquo;s what the reflection system actually does:\nReviews the session — What patterns appeared in my behavior? What did I ask about? How did I respond to KenLM\u0026rsquo;s answers?\nSearches for evidence — The reflection agent can call a search_episodes(query) tool to find supporting patterns across all past conversations. This prevents overfitting to a single session.\nUpdates hypotheses — For each insight, it calls upsert_hypothesis() with a confidence score. Crucially, confidence only increases when patterns repeat across multiple episodes.\nMaintains learning goals — If the model notices something it doesn\u0026rsquo;t understand about me yet, it creates a learning goal—a question to explore in future interactions.\nHere\u0026rsquo;s what my actual profile looks like after a few weeks of use:\n{ \u0026#34;hypotheses\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_004\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;response_verbosity\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth prefers detailed and useful responses over concise ones.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.6, \u0026#34;evidence_episode_ids\u0026#34;: [ 91, 90, 88, 89, 159, 160 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_005\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;response_quality\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth is frustrated with inaccurate or misleading information.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.9, \u0026#34;evidence_episode_ids\u0026#34;: [ 90, 91, 89, 159, 160 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_006\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;control\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth values precision and control over changes in the project.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.9, \u0026#34;evidence_episode_ids\u0026#34;: [ 153, 88, 92 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_007\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;accuracy\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth is detail-oriented and expects accurate and precise information.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.6, \u0026#34;evidence_episode_ids\u0026#34;: [ 90, 91, 89, 92 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_008\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;transparency\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth values transparency and honesty in communication.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 89 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_009\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;task_compliance\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth expects tasks to be completed accurately and without unnecessary steps.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 154, 155, 156, 157, 158 ] } ], \u0026#34;learning_goals\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;learning_goal_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;How can we ensure that Kenneth\u0026#39;s instructions are followed without unnecessary repetition or recapitulation?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;closed\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-11-30T15:10:55.704097\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;lg_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;What are Kenneth\u0026#39;s specific expectations for the precision and detail in project updates?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-12-02T15:51:56.904564\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;lg_004\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;What are Kenneth\u0026#39;s specific expectations for the accuracy and completeness of tasks?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-12-02T13:57:40.150136\u0026#34; } ] } The reflection prompt is designed to be conservative. I explicitly instruct the model to start hypotheses with low confidence (0.2–0.4) and only raise confidence when it sees repeated evidence. This should hopefully prevent the kind of overconfident conclusions that can come from a single unusual interaction.\nCode Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e\n\u003cp\u003eHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As \u003ca href=\"https://abehrouz.github.io/files/NL.pdf\"\u003ethis paper\u003c/a\u003e from Google points out, LLMs lack \u003cstrong\u003eneuroplasticity\u003c/strong\u003e—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes \u003cem\u003efrozen\u003c/em\u003e. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes we\u0026rsquo;re trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights). All the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions). Each meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals). A distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass passes information between these components, using its working memory (context) as the vehicle. Let me break down how it works piece by piece.\nLayer 1: Episodic Memory This piece of the architecture is the most straightforward. To enable the model to remember specific experiences, every interaction is logged as an episode. Every episode captures the user prompt, agent response, and some metadata for memory management. It looks something like this:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-01T14:30:00\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;Can you help me write a README for this project?\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;Sure. Here\u0026#39;s a concise template...\u0026#34;, \u0026#34;access_count\u0026#34;: 3, \u0026#34;last_accessed\u0026#34;: \u0026#34;2025-12-03T10:15:00\u0026#34;, \u0026#34;is_protected\u0026#34;: false } To enable the model to retrieve relevant episodes at inference time, I utilize a technique known as retrieval-augmented generation (RAG). Simply put, I store the episode data in a vector store called ChromaDB. Embeddings are generated by sentence-transformers after every conversation. Every time I send a message, KenLM embeds my query and searches for the top-k most semantically similar episodes in its memory. This allows it to reference episodes related to my query, so if I ask about sports, it will pull up discussions where we talked about sports. If I ask about documentation, it will remember episodes where we talked about writing documentation.\nThis gives the model something resembling episodic recall: the ability to remember relevant experiences and bring them into working memory (the context window) when they\u0026rsquo;re useful.\nLayer 2: Reflective Memory Episodic memory alone would essentially be just a fancy search engine. Something that humans do that enables more complex learning is reflect on past episodes and draw generalized conclusions based on them. Taking inspiration from this process, I built an agentic reflection process into KenLM.\nWhen I exit a chat session, KenLM runs a reflection loop. A KenLM \u0026ldquo;reflection agent\u0026rdquo; analyzes the session\u0026rsquo;s episodes, searches for related patterns in past conversations, and updates a profile of hypotheses and learning goals.\nHere\u0026rsquo;s what the reflection system actually does:\nReviews the session — What patterns appeared in my behavior? What did I ask about? How did I respond to KenLM\u0026rsquo;s answers?\nSearches for evidence — The reflection agent can call a search_episodes(query) tool to find supporting patterns across all past conversations. This prevents overfitting to a single session.\nUpdates hypotheses — For each insight, it calls upsert_hypothesis() with a confidence score. Crucially, confidence only increases when patterns repeat across multiple episodes.\nMaintains learning goals — If the model notices something it doesn\u0026rsquo;t understand about me yet, it creates a learning goal—a question to explore in future interactions.\nHere\u0026rsquo;s what my actual profile looks like after some use:\n{ \u0026#34;hypotheses\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_004\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;response_verbosity\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth prefers detailed and useful responses over concise ones.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.6, \u0026#34;evidence_episode_ids\u0026#34;: [ 91, 90, 88, 89, 159, 160 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_005\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;response_quality\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth is frustrated with inaccurate or misleading information.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.9, \u0026#34;evidence_episode_ids\u0026#34;: [ 90, 91, 89, 159, 160 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_006\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;control\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth values precision and control over changes in the project.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.9, \u0026#34;evidence_episode_ids\u0026#34;: [ 153, 88, 92 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_007\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;accuracy\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth is detail-oriented and expects accurate and precise information.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.6, \u0026#34;evidence_episode_ids\u0026#34;: [ 90, 91, 89, 92 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_008\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;transparency\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth values transparency and honesty in communication.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 89 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_009\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;task_compliance\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth expects tasks to be completed accurately and without unnecessary steps.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 154, 155, 156, 157, 158 ] } ], \u0026#34;learning_goals\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;learning_goal_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;How can we ensure that Kenneth\u0026#39;s instructions are followed without unnecessary repetition or recapitulation?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;closed\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-11-30T15:10:55.704097\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;lg_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;What are Kenneth\u0026#39;s specific expectations for the precision and detail in project updates?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-12-02T15:51:56.904564\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;lg_004\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;What are Kenneth\u0026#39;s specific expectations for the accuracy and completeness of tasks?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-12-02T13:57:40.150136\u0026#34; } ] } The reflection prompt is designed to be conservative. I explicitly instruct the model to start hypotheses with low confidence (0.2–0.4) and only raise confidence when it sees repeated evidence. This should hopefully prevent the kind of overconfident conclusions that can come from a single unusual interaction.\nCode Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e\n\u003cp\u003eHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As \u003ca href=\"https://abehrouz.github.io/files/NL.pdf\"\u003ethis paper\u003c/a\u003e from Google points out, LLMs lack \u003cstrong\u003eneuroplasticity\u003c/strong\u003e—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes \u003cem\u003efrozen\u003c/em\u003e. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes we\u0026rsquo;re trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights). All the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions). Each meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals). A distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass passes information between these components, using its working memory (context) as the vehicle. Let me break down how it works piece by piece.\nLayer 1: Episodic Memory This piece of the architecture is the most straightforward. To enable the model to remember specific experiences, every interaction is logged as an episode. Every episode captures the user prompt, agent response, and some metadata for memory management. It looks something like this:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-01T14:30:00\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;Can you help me write a README for this project?\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;Sure. Here\u0026#39;s a concise template...\u0026#34;, \u0026#34;access_count\u0026#34;: 3, \u0026#34;last_accessed\u0026#34;: \u0026#34;2025-12-03T10:15:00\u0026#34;, \u0026#34;is_protected\u0026#34;: false } To enable the model to retrieve relevant episodes at inference time, I utilize a technique known as retrieval-augmented generation (RAG). Simply put, I store the episode data in a vector store called ChromaDB. Embeddings are generated by sentence-transformers after every conversation. Every time I send a message, KenLM embeds my query and searches for the top-k most semantically similar episodes in its memory. This allows it to reference episodes related to my query, so if I ask about sports, it will pull up discussions where we talked about sports. If I ask about documentation, it will remember episodes where we talked about writing documentation.\nThis gives the model something resembling episodic recall: the ability to remember relevant experiences and bring them into working memory (the context window) when they\u0026rsquo;re useful.\nLayer 2: Reflective Memory Episodic memory alone would essentially be just a fancy search engine. Something that humans do that enables more complex learning is reflect on past episodes and draw generalized conclusions based on them. Taking inspiration from this process, I built an agentic reflection process into KenLM.\nWhen I exit a chat session, KenLM runs a reflection loop. A KenLM \u0026ldquo;reflection agent\u0026rdquo; analyzes the session\u0026rsquo;s episodes, searches for related patterns in past conversations, and updates a profile of hypotheses and learning goals.\nHere\u0026rsquo;s what the reflection system actually does:\nReviews the session — What patterns appeared in my behavior? What did I ask about? How did I respond to KenLM\u0026rsquo;s answers?\nSearches for evidence — The reflection agent can call a search_episodes(query) tool to find supporting patterns across all past conversations. This prevents overfitting to a single session.\nUpdates hypotheses — For each insight, it calls upsert_hypothesis() with a confidence score. Crucially, confidence only increases when patterns repeat across multiple episodes.\nMaintains learning goals — If the model notices something it doesn\u0026rsquo;t understand about me yet, it creates a learning goal—a question to explore in future interactions.\nHere\u0026rsquo;s what my actual profile looks like after some use:\n{ \u0026#34;hypotheses\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_004\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;response_verbosity\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth prefers detailed and useful responses over concise ones.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.6, \u0026#34;evidence_episode_ids\u0026#34;: [ 91, 90, 88, 89, 159, 160 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_005\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;response_quality\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth is frustrated with inaccurate or misleading information.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.9, \u0026#34;evidence_episode_ids\u0026#34;: [ 90, 91, 89, 159, 160 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_006\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;control\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth values precision and control over changes in the project.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.9, \u0026#34;evidence_episode_ids\u0026#34;: [ 153, 88, 92 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_007\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;accuracy\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth is detail-oriented and expects accurate and precise information.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.6, \u0026#34;evidence_episode_ids\u0026#34;: [ 90, 91, 89, 92 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_008\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;transparency\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth values transparency and honesty in communication.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 89 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_009\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;task_compliance\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth expects tasks to be completed accurately and without unnecessary steps.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 154, 155, 156, 157, 158 ] } ], \u0026#34;learning_goals\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;learning_goal_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;How can we ensure that Kenneth\u0026#39;s instructions are followed without unnecessary repetition or recapitulation?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;closed\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-11-30T15:10:55.704097\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;lg_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;What are Kenneth\u0026#39;s specific expectations for the precision and detail in project updates?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-12-02T15:51:56.904564\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;lg_004\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;What are Kenneth\u0026#39;s specific expectations for the accuracy and completeness of tasks?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-12-02T13:57:40.150136\u0026#34; } ] } The reflection prompt is designed to be conservative. I explicitly instruct the model to start hypotheses with low confidence (0.2–0.4) and only raise confidence when it sees repeated evidence. This should hopefully prevent the kind of overconfident conclusions that can come from a single unusual interaction.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e\n\u003cp\u003eHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As \u003ca href=\"https://abehrouz.github.io/files/NL.pdf\"\u003ethis paper\u003c/a\u003e from Google points out, LLMs lack \u003cstrong\u003eneuroplasticity\u003c/strong\u003e—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes \u003cem\u003efrozen\u003c/em\u003e. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes we\u0026rsquo;re trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights). All the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions). Each meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals). A distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass passes information between these components, using its working memory (context) as the vehicle. Let me break down how it works piece by piece.\nLayer 1: Episodic Memory This piece of the architecture is the most straightforward. To enable the model to remember specific experiences, every interaction is logged as an episode. Every episode captures the user prompt, agent response, and some metadata for memory management. It looks something like this:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-01T14:30:00\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;Can you help me write a README for this project?\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;Sure. Here\u0026#39;s a concise template...\u0026#34;, \u0026#34;access_count\u0026#34;: 3, \u0026#34;last_accessed\u0026#34;: \u0026#34;2025-12-03T10:15:00\u0026#34;, \u0026#34;is_protected\u0026#34;: false } To enable the model to retrieve relevant episodes at inference time, I utilize a technique known as retrieval-augmented generation (RAG). Simply put, I store the episode data in a vector store called ChromaDB. Embeddings are generated by sentence-transformers after every conversation. Every time I send a message, KenLM embeds my query and searches for the top-k most semantically similar episodes in its memory. This allows it to reference episodes related to my query, so if I ask about sports, it will pull up discussions where we talked about sports. If I ask about documentation, it will remember episodes where we talked about writing documentation.\nThis gives the model something resembling episodic recall: the ability to remember relevant experiences and bring them into working memory (the context window) when they\u0026rsquo;re useful.\nLayer 2: Reflective Memory Episodic memory alone would essentially be just a fancy search engine. Something that humans do that enables more complex learning is reflect on past episodes and draw generalized conclusions based on them. Taking inspiration from this process, I built an agentic reflection process into KenLM.\nWhen I exit a chat session, KenLM runs a reflection loop. A KenLM \u0026ldquo;reflection agent\u0026rdquo; analyzes the session\u0026rsquo;s episodes, searches for related patterns in past conversations, and updates a profile of hypotheses and learning goals.\nHere\u0026rsquo;s what the reflection system actually does:\nReviews the session — What patterns appeared in my behavior? What did I ask about? How did I respond to KenLM\u0026rsquo;s answers?\nSearches for evidence — The reflection agent can call a search_episodes(query) tool to find supporting patterns across all past conversations. This prevents overfitting to a single session.\nUpdates hypotheses — For each insight, it calls upsert_hypothesis() with a confidence score. Crucially, confidence only increases when patterns repeat across multiple episodes.\nMaintains learning goals — If the model notices something it doesn\u0026rsquo;t understand about me yet, it creates a learning goal—a question to explore in future interactions.\nHere\u0026rsquo;s what my actual profile looks like after some use:\n{ \u0026#34;hypotheses\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_004\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;response_verbosity\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth prefers detailed and useful responses over concise ones.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.6, \u0026#34;evidence_episode_ids\u0026#34;: [ 91, 90, 88, 89, 159, 160 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_005\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;response_quality\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth is frustrated with inaccurate or misleading information.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.9, \u0026#34;evidence_episode_ids\u0026#34;: [ 90, 91, 89, 159, 160 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_006\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;control\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth values precision and control over changes in the project.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.9, \u0026#34;evidence_episode_ids\u0026#34;: [ 153, 88, 92 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_007\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;accuracy\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth is detail-oriented and expects accurate and precise information.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.6, \u0026#34;evidence_episode_ids\u0026#34;: [ 90, 91, 89, 92 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_008\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;transparency\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth values transparency and honesty in communication.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 89 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_009\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;task_compliance\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth expects tasks to be completed accurately and without unnecessary steps.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 154, 155, 156, 157, 158 ] } ], \u0026#34;learning_goals\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;learning_goal_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;How can we ensure that Kenneth\u0026#39;s instructions are followed without unnecessary repetition or recapitulation?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;closed\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-11-30T15:10:55.704097\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;lg_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;What are Kenneth\u0026#39;s specific expectations for the precision and detail in project updates?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-12-02T15:51:56.904564\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;lg_004\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;What are Kenneth\u0026#39;s specific expectations for the accuracy and completeness of tasks?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-12-02T13:57:40.150136\u0026#34; } ] } The reflection prompt is designed to be conservative. I explicitly instruct the model to start hypotheses with low confidence (0.2–0.4) and only raise confidence when it sees repeated evidence. This should hopefully prevent the kind of overconfident conclusions that can come from a single unusual interaction.\nLayer 3: The Forgetting System A pretty important piece of learning is actually the process by which we decide which pieces of information aren\u0026rsquo;t relevant. If we were to allow our episodic vector database to grow unbounded with every user interaction, it would quickly become slow and stop being useful during inference or reflection. Moreover, humans also don\u0026rsquo;t retain every bit of information that we are exposed to - we have complex systems that decide what information is worth retaining based on various factors.\nMy algorithm is a bit more rudimentary, but it seems to be working fine so far. It essentially calculates an \u0026ldquo;importance\u0026rdquo; factor for every memory, which is just a weighted sum of a couple factors:\nimportance = w1 * log(1 + access_count) # Access frequency + w2 * is_protected # Protection bonus + w3 * is_evidence # Evidence bonus + w4 * exp(-age_days / tau) # Recency decayEvery time an episode is retrieved by RAG, its `access_count` increments. Episodes that are frequently useful stay \u0026#34;fresh\u0026#34; and score higher. Episodes that are never accessed gradually decay in importance. ","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e\n\u003cp\u003eHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As \u003ca href=\"https://abehrouz.github.io/files/NL.pdf\"\u003ethis paper\u003c/a\u003e from Google points out, LLMs lack \u003cstrong\u003eneuroplasticity\u003c/strong\u003e—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes \u003cem\u003efrozen\u003c/em\u003e. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes we\u0026rsquo;re trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights). All the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions). Each meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals). A distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass passes information between these components, using its working memory (context) as the vehicle. Let me break down how it works piece by piece.\nLayer 1: Episodic Memory This piece of the architecture is the most straightforward. To enable the model to remember specific experiences, every interaction is logged as an episode. Every episode captures the user prompt, agent response, and some metadata for memory management. It looks something like this:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-01T14:30:00\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;Can you help me write a README for this project?\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;Sure. Here\u0026#39;s a concise template...\u0026#34;, \u0026#34;access_count\u0026#34;: 3, \u0026#34;last_accessed\u0026#34;: \u0026#34;2025-12-03T10:15:00\u0026#34;, \u0026#34;is_protected\u0026#34;: false } To enable the model to retrieve relevant episodes at inference time, I utilize a technique known as retrieval-augmented generation (RAG). Simply put, I store the episode data in a vector store called ChromaDB. Embeddings are generated by sentence-transformers after every conversation. Every time I send a message, KenLM embeds my query and searches for the top-k most semantically similar episodes in its memory. This allows it to reference episodes related to my query, so if I ask about sports, it will pull up discussions where we talked about sports. If I ask about documentation, it will remember episodes where we talked about writing documentation.\nThis gives the model something resembling episodic recall: the ability to remember relevant experiences and bring them into working memory (the context window) when they\u0026rsquo;re useful.\nLayer 2: Reflective Memory Episodic memory alone would essentially be just a fancy search engine. Something that humans do that enables more complex learning is reflect on past episodes and draw generalized conclusions based on them. Taking inspiration from this process, I built an agentic reflection process into KenLM.\nWhen I exit a chat session, KenLM runs a reflection loop. A KenLM \u0026ldquo;reflection agent\u0026rdquo; analyzes the session\u0026rsquo;s episodes, searches for related patterns in past conversations, and updates a profile of hypotheses and learning goals.\nHere\u0026rsquo;s what the reflection system actually does:\nReviews the session — What patterns appeared in my behavior? What did I ask about? How did I respond to KenLM\u0026rsquo;s answers?\nSearches for evidence — The reflection agent can call a search_episodes(query) tool to find supporting patterns across all past conversations. This prevents overfitting to a single session.\nUpdates hypotheses — For each insight, it calls upsert_hypothesis() with a confidence score. Crucially, confidence only increases when patterns repeat across multiple episodes.\nMaintains learning goals — If the model notices something it doesn\u0026rsquo;t understand about me yet, it creates a learning goal—a question to explore in future interactions.\nHere\u0026rsquo;s what my actual profile looks like after some use:\n{ \u0026#34;hypotheses\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_004\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;response_verbosity\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth prefers detailed and useful responses over concise ones.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.6, \u0026#34;evidence_episode_ids\u0026#34;: [ 91, 90, 88, 89, 159, 160 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_005\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;response_quality\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth is frustrated with inaccurate or misleading information.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.9, \u0026#34;evidence_episode_ids\u0026#34;: [ 90, 91, 89, 159, 160 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_006\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;control\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth values precision and control over changes in the project.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.9, \u0026#34;evidence_episode_ids\u0026#34;: [ 153, 88, 92 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_007\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;accuracy\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth is detail-oriented and expects accurate and precise information.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.6, \u0026#34;evidence_episode_ids\u0026#34;: [ 90, 91, 89, 92 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_008\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;transparency\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth values transparency and honesty in communication.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 89 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_009\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;task_compliance\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth expects tasks to be completed accurately and without unnecessary steps.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 154, 155, 156, 157, 158 ] } ], \u0026#34;learning_goals\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;learning_goal_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;How can we ensure that Kenneth\u0026#39;s instructions are followed without unnecessary repetition or recapitulation?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;closed\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-11-30T15:10:55.704097\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;lg_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;What are Kenneth\u0026#39;s specific expectations for the precision and detail in project updates?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-12-02T15:51:56.904564\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;lg_004\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;What are Kenneth\u0026#39;s specific expectations for the accuracy and completeness of tasks?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-12-02T13:57:40.150136\u0026#34; } ] } The reflection prompt is designed to be conservative. I explicitly instruct the model to start hypotheses with low confidence (0.2–0.4) and only raise confidence when it sees repeated evidence. This should hopefully prevent the kind of overconfident conclusions that can come from a single unusual interaction.\nLayer 3: The Forgetting System A pretty important piece of learning is actually the process by which we decide which pieces of information aren\u0026rsquo;t relevant. If we were to allow our episodic vector database to grow unbounded with every user interaction, it would quickly become slow and stop being useful during inference or reflection. Moreover, humans also don\u0026rsquo;t retain every bit of information that we are exposed to - we have complex systems that decide what information is worth retaining based on various factors.\nMy algorithm is a bit more rudimentary, but it seems to be working fine so far. It essentially calculates an \u0026ldquo;importance\u0026rdquo; factor for every memory, which is just a weighted sum of a couple factors:\nimportance = w1 * log(1 + access_count) # Access frequency + w2 * is_protected # Protection bonus + w3 * is_evidence # Evidence bonus + w4 * exp(-age_days / tau) # Recency decayEvery time an episode is retrieved by RAG, its `access_count` increments. Episodes that are frequently useful stay \u0026#34;fresh\u0026#34; and score higher. Episodes that are never accessed gradually decay in importance. ","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e\n\u003cp\u003eHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As \u003ca href=\"https://abehrouz.github.io/files/NL.pdf\"\u003ethis paper\u003c/a\u003e from Google points out, LLMs lack \u003cstrong\u003eneuroplasticity\u003c/strong\u003e—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes \u003cem\u003efrozen\u003c/em\u003e. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes we\u0026rsquo;re trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights). All the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions). Each meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals). A distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass passes information between these components, using its working memory (context) as the vehicle. Let me break down how it works piece by piece.\nLayer 1: Episodic Memory This piece of the architecture is the most straightforward. To enable the model to remember specific experiences, every interaction is logged as an episode. Every episode captures the user prompt, agent response, and some metadata for memory management. It looks something like this:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-01T14:30:00\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;Can you help me write a README for this project?\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;Sure. Here\u0026#39;s a concise template...\u0026#34;, \u0026#34;access_count\u0026#34;: 3, \u0026#34;last_accessed\u0026#34;: \u0026#34;2025-12-03T10:15:00\u0026#34;, \u0026#34;is_protected\u0026#34;: false } To enable the model to retrieve relevant episodes at inference time, I utilize a technique known as retrieval-augmented generation (RAG). Simply put, I store the episode data in a vector store called ChromaDB. Embeddings are generated by sentence-transformers after every conversation. Every time I send a message, KenLM embeds my query and searches for the top-k most semantically similar episodes in its memory. This allows it to reference episodes related to my query, so if I ask about sports, it will pull up discussions where we talked about sports. If I ask about documentation, it will remember episodes where we talked about writing documentation.\nThis gives the model something resembling episodic recall: the ability to remember relevant experiences and bring them into working memory (the context window) when they\u0026rsquo;re useful.\nLayer 2: Reflective Memory Episodic memory alone would essentially be just a fancy search engine. Something that humans do that enables more complex learning is reflect on past episodes and draw generalized conclusions based on them. Taking inspiration from this process, I built an agentic reflection process into KenLM.\nWhen I exit a chat session, KenLM runs a reflection loop. A KenLM \u0026ldquo;reflection agent\u0026rdquo; analyzes the session\u0026rsquo;s episodes, searches for related patterns in past conversations, and updates a profile of hypotheses and learning goals.\nHere\u0026rsquo;s what the reflection system actually does:\nReviews the session — What patterns appeared in my behavior? What did I ask about? How did I respond to KenLM\u0026rsquo;s answers?\nSearches for evidence — The reflection agent can call a search_episodes(query) tool to find supporting patterns across all past conversations. This prevents overfitting to a single session.\nUpdates hypotheses — For each insight, it calls upsert_hypothesis() with a confidence score. Crucially, confidence only increases when patterns repeat across multiple episodes.\nMaintains learning goals — If the model notices something it doesn\u0026rsquo;t understand about me yet, it creates a learning goal—a question to explore in future interactions.\nHere\u0026rsquo;s what my actual profile looks like after some use:\n{ \u0026#34;hypotheses\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_004\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;response_verbosity\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth prefers detailed and useful responses over concise ones.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.6, \u0026#34;evidence_episode_ids\u0026#34;: [ 91, 90, 88, 89, 159, 160 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_005\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;response_quality\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth is frustrated with inaccurate or misleading information.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.9, \u0026#34;evidence_episode_ids\u0026#34;: [ 90, 91, 89, 159, 160 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_006\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;control\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth values precision and control over changes in the project.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.9, \u0026#34;evidence_episode_ids\u0026#34;: [ 153, 88, 92 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_007\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;accuracy\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth is detail-oriented and expects accurate and precise information.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.6, \u0026#34;evidence_episode_ids\u0026#34;: [ 90, 91, 89, 92 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_008\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;transparency\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth values transparency and honesty in communication.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 89 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_009\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;task_compliance\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth expects tasks to be completed accurately and without unnecessary steps.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 154, 155, 156, 157, 158 ] } ], \u0026#34;learning_goals\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;learning_goal_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;How can we ensure that Kenneth\u0026#39;s instructions are followed without unnecessary repetition or recapitulation?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;closed\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-11-30T15:10:55.704097\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;lg_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;What are Kenneth\u0026#39;s specific expectations for the precision and detail in project updates?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-12-02T15:51:56.904564\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;lg_004\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;What are Kenneth\u0026#39;s specific expectations for the accuracy and completeness of tasks?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-12-02T13:57:40.150136\u0026#34; } ] } The reflection prompt is designed to be conservative. I explicitly instruct the model to start hypotheses with low confidence (0.2–0.4) and only raise confidence when it sees repeated evidence. This should hopefully prevent the kind of overconfident conclusions that can come from a single unusual interaction.\nLayer 3: The Forgetting System A pretty important piece of learning is actually the process by which we decide which pieces of information aren\u0026rsquo;t relevant. If we were to allow our episodic vector database to grow unbounded with every user interaction, it would quickly become slow and stop being useful during inference or reflection. Moreover, humans also don\u0026rsquo;t retain every bit of information that we are exposed to - we have complex systems that decide what information is worth retaining based on various factors.\nMy algorithm is a bit more rudimentary, but it seems to be working fine so far. It essentially calculates an \u0026ldquo;importance\u0026rdquo; factor for every memory, which is just a weighted sum of a couple factors:\nimportance = w1 * log(1 + access_count) # Access frequency + w2 * is_protected # Protection bonus + w3 * is_evidence # Evidence bonus + w4 * exp(-age_days / tau) # Recency decay This system assigns a high score to frequently accessed memories, memories that the AI has decided it wants to \u0026ldquo;protect,\u0026rdquo; memories that are evidence for hypotheses that it has reasoned about, and memories that are recent. Memories that are old, infrequently accessed, and not useful for any of the learning goals or hypotheses the AI has created become candidates for forgetting.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e\n\u003cp\u003eHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As \u003ca href=\"https://abehrouz.github.io/files/NL.pdf\"\u003ethis paper\u003c/a\u003e from Google points out, LLMs lack \u003cstrong\u003eneuroplasticity\u003c/strong\u003e—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes \u003cem\u003efrozen\u003c/em\u003e. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes we\u0026rsquo;re trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights). All the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions). Each meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals). A distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass passes information between these components, using its working memory (context) as the vehicle. Let me break down how it works piece by piece.\nLayer 1: Episodic Memory This piece of the architecture is the most straightforward. To enable the model to remember specific experiences, every interaction is logged as an episode. Every episode captures the user prompt, agent response, and some metadata for memory management. It looks something like this:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-01T14:30:00\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;Can you help me write a README for this project?\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;Sure. Here\u0026#39;s a concise template...\u0026#34;, \u0026#34;access_count\u0026#34;: 3, \u0026#34;last_accessed\u0026#34;: \u0026#34;2025-12-03T10:15:00\u0026#34;, \u0026#34;is_protected\u0026#34;: false } To enable the model to retrieve relevant episodes at inference time, I utilize a technique known as retrieval-augmented generation (RAG). Simply put, I store the episode data in a vector store called ChromaDB. Embeddings are generated by sentence-transformers after every conversation. Every time I send a message, KenLM embeds my query and searches for the top-k most semantically similar episodes in its memory. This allows it to reference episodes related to my query, so if I ask about sports, it will pull up discussions where we talked about sports. If I ask about documentation, it will remember episodes where we talked about writing documentation.\nThis gives the model something resembling episodic recall: the ability to remember relevant experiences and bring them into working memory (the context window) when they\u0026rsquo;re useful.\nLayer 2: Reflective Memory Episodic memory alone would essentially be just a fancy search engine. Something that humans do that enables more complex learning is reflect on past episodes and draw generalized conclusions based on them. Taking inspiration from this process, I built an agentic reflection process into KenLM.\nWhen I exit a chat session, KenLM runs a reflection loop. A KenLM \u0026ldquo;reflection agent\u0026rdquo; analyzes the session\u0026rsquo;s episodes, searches for related patterns in past conversations, and updates a profile of hypotheses and learning goals.\nHere\u0026rsquo;s what the reflection system actually does:\nReviews the session — What patterns appeared in my behavior? What did I ask about? How did I respond to KenLM\u0026rsquo;s answers?\nSearches for evidence — The reflection agent can call a search_episodes(query) tool to find supporting patterns across all past conversations. This prevents overfitting to a single session.\nUpdates hypotheses — For each insight, it calls upsert_hypothesis() with a confidence score. Crucially, confidence only increases when patterns repeat across multiple episodes.\nMaintains learning goals — If the model notices something it doesn\u0026rsquo;t understand about me yet, it creates a learning goal—a question to explore in future interactions.\nHere\u0026rsquo;s what my actual profile looks like after some use:\n{ \u0026#34;hypotheses\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_007\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;accuracy\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth is detail-oriented and expects accurate and precise information.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.6, \u0026#34;evidence_episode_ids\u0026#34;: [ 90, 91, 89, 92 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_008\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;transparency\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth values transparency and honesty in communication.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 89 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_009\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;task_compliance\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth expects tasks to be completed accurately and without unnecessary steps.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 154, 155, 156, 157, 158 ] } ], \u0026#34;learning_goals\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;learning_goal_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;How can we ensure that Kenneth\u0026#39;s instructions are followed without unnecessary repetition or recapitulation?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;closed\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-11-30T15:10:55.704097\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;lg_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;What are Kenneth\u0026#39;s specific expectations for the precision and detail in project updates?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-12-02T15:51:56.904564\u0026#34; } ] } The reflection prompt is designed to be conservative. I explicitly instruct the model to start hypotheses with low confidence (0.2–0.4) and only raise confidence when it sees repeated evidence. This should hopefully prevent the kind of overconfident conclusions that can come from a single unusual interaction.\nLayer 3: The Forgetting System A pretty important piece of learning is actually the process by which we decide which pieces of information aren\u0026rsquo;t relevant. If we were to allow our episodic vector database to grow unbounded with every user interaction, it would quickly become slow and stop being useful during inference or reflection. Moreover, humans also don\u0026rsquo;t retain every bit of information that we are exposed to - we have complex systems that decide what information is worth retaining based on various factors.\nMy algorithm is a bit more rudimentary, but it seems to be working fine so far. It essentially calculates an \u0026ldquo;importance\u0026rdquo; factor for every memory, which is just a weighted sum of a couple factors:\nimportance = w1 * log(1 + access_count) # Access frequency + w2 * is_protected # Protection bonus + w3 * is_evidence # Evidence bonus + w4 * exp(-age_days / tau) # Recency decay This system assigns a high score to frequently accessed memories, memories that the AI has decided it wants to \u0026ldquo;protect,\u0026rdquo; memories that are evidence for hypotheses that it has reasoned about, and memories that are recent. Memories that are old, infrequently accessed, and not useful for any of the learning goals or hypotheses the AI has created become candidates for forgetting.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e\n\u003cp\u003eHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As \u003ca href=\"https://abehrouz.github.io/files/NL.pdf\"\u003ethis paper\u003c/a\u003e from Google points out, LLMs lack \u003cstrong\u003eneuroplasticity\u003c/strong\u003e—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes \u003cem\u003efrozen\u003c/em\u003e. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes we\u0026rsquo;re trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights). All the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions). Each meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals). A distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass passes information between these components, using its working memory (context) as the vehicle. Let me break down how it works piece by piece.\nLayer 1: Episodic Memory This piece of the architecture is the most straightforward. To enable the model to remember specific experiences, every interaction is logged as an episode. Every episode captures the user prompt, agent response, and some metadata for memory management. It looks something like this:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-01T14:30:00\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;Can you help me write a README for this project?\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;Sure. Here\u0026#39;s a concise template...\u0026#34;, \u0026#34;access_count\u0026#34;: 3, \u0026#34;last_accessed\u0026#34;: \u0026#34;2025-12-03T10:15:00\u0026#34;, \u0026#34;is_protected\u0026#34;: false } To enable the model to retrieve relevant episodes at inference time, I utilize a technique known as retrieval-augmented generation (RAG). Simply put, I store the episode data in a vector store called ChromaDB. Embeddings are generated by sentence-transformers after every conversation. Every time I send a message, KenLM embeds my query and searches for the top-k most semantically similar episodes in its memory. This allows it to reference episodes related to my query, so if I ask about sports, it will pull up discussions where we talked about sports. If I ask about documentation, it will remember episodes where we talked about writing documentation.\nThis gives the model something resembling episodic recall: the ability to remember relevant experiences and bring them into working memory (the context window) when they\u0026rsquo;re useful.\nLayer 2: Reflective Memory Episodic memory alone would essentially be just a fancy search engine. Something that humans do that enables more complex learning is reflect on past episodes and draw generalized conclusions based on them. Taking inspiration from this process, I built an agentic reflection process into KenLM.\nWhen I exit a chat session, KenLM runs a reflection loop. A KenLM \u0026ldquo;reflection agent\u0026rdquo; analyzes the session\u0026rsquo;s episodes, searches for related patterns in past conversations, and updates a profile of hypotheses and learning goals.\nHere\u0026rsquo;s what the reflection system actually does:\nReviews the session — What patterns appeared in my behavior? What did I ask about? How did I respond to KenLM\u0026rsquo;s answers?\nSearches for evidence — The reflection agent can call a search_episodes(query) tool to find supporting patterns across all past conversations. This prevents overfitting to a single session.\nUpdates hypotheses — For each insight, it calls upsert_hypothesis() with a confidence score. Crucially, confidence only increases when patterns repeat across multiple episodes.\nMaintains learning goals — If the model notices something it doesn\u0026rsquo;t understand about me yet, it creates a learning goal—a question to explore in future interactions.\nHere\u0026rsquo;s what my actual profile looks like after some use:\n{ \u0026#34;hypotheses\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_007\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;accuracy\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth is detail-oriented and expects accurate and precise information.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.6, \u0026#34;evidence_episode_ids\u0026#34;: [ 90, 91, 89, 92 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_008\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;transparency\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth values transparency and honesty in communication.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 89 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_009\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;task_compliance\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth expects tasks to be completed accurately and without unnecessary steps.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 154, 155, 156, 157, 158 ] } ], \u0026#34;learning_goals\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;learning_goal_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;How can we ensure that Kenneth\u0026#39;s instructions are followed without unnecessary repetition or recapitulation?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;closed\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-11-30T15:10:55.704097\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;lg_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;What are Kenneth\u0026#39;s specific expectations for the precision and detail in project updates?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-12-02T15:51:56.904564\u0026#34; } ] } The reflection prompt is designed to be conservative. I explicitly instruct the model to start hypotheses with low confidence (0.2–0.4) and only raise confidence when it sees repeated evidence. This should hopefully prevent the kind of overconfident conclusions that can come from a single unusual interaction.\nLayer 3: The Forgetting System A pretty important piece of learning is actually the process by which we decide which pieces of information aren\u0026rsquo;t relevant. If we were to allow our episodic vector database to grow unbounded with every user interaction, it would quickly become slow and stop being useful during inference or reflection. Moreover, humans also don\u0026rsquo;t retain every bit of information that we are exposed to - we have complex systems that decide what information is worth retaining based on various factors.\nMy algorithm is a bit more rudimentary than the process that the human brain has, but it seems to be working fine so far. It essentially calculates an \u0026ldquo;importance\u0026rdquo; factor for every memory, which is just a weighted sum of a couple factors:\nimportance = w1 * log(1 + access_count) # Access frequency + w2 * is_protected # Protection bonus + w3 * is_evidence # Evidence bonus + w4 * exp(-age_days / tau) # Recency decay This system assigns a high score to frequently accessed memories, memories that the AI has decided it wants to \u0026ldquo;protect,\u0026rdquo; memories that are evidence for hypotheses that it has reasoned about, and memories that are recent. Memories that are old, infrequently accessed, and not useful for any of the learning goals or hypotheses the AI has created become candidates for forgetting.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e\n\u003cp\u003eHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As \u003ca href=\"https://abehrouz.github.io/files/NL.pdf\"\u003ethis paper\u003c/a\u003e from Google points out, LLMs lack \u003cstrong\u003eneuroplasticity\u003c/strong\u003e—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes \u003cem\u003efrozen\u003c/em\u003e. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes we\u0026rsquo;re trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights). All the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions). Each meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals). A distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass passes information between these components, using its working memory (context) as the vehicle. Let me break down how it works piece by piece.\nLayer 1: Episodic Memory This piece of the architecture is the most straightforward. To enable the model to remember specific experiences, every interaction is logged as an episode. Every episode captures the user prompt, agent response, and some metadata for memory management. It looks something like this:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-01T14:30:00\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;Can you help me write a README for this project?\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;Sure. Here\u0026#39;s a concise template...\u0026#34;, \u0026#34;access_count\u0026#34;: 3, \u0026#34;last_accessed\u0026#34;: \u0026#34;2025-12-03T10:15:00\u0026#34;, \u0026#34;is_protected\u0026#34;: false } To enable the model to retrieve relevant episodes at inference time, I utilize a technique known as retrieval-augmented generation (RAG). Simply put, I store the episode data in a vector store called ChromaDB. Embeddings are generated by sentence-transformers after every conversation. Every time I send a message, KenLM embeds my query and searches for the top-k most semantically similar episodes in its memory. This allows it to reference episodes related to my query, so if I ask about sports, it will pull up discussions where we talked about sports. If I ask about documentation, it will remember episodes where we talked about writing documentation.\nThis gives the model something resembling episodic recall: the ability to remember relevant experiences and bring them into working memory (the context window) when they\u0026rsquo;re useful.\nLayer 2: Reflective Memory Episodic memory alone would essentially be just a fancy search engine. Something that humans do that enables more complex learning is reflect on past episodes and draw generalized conclusions based on them. Taking inspiration from this process, I built an agentic reflection process into KenLM.\nWhen I exit a chat session, KenLM runs a reflection loop. A KenLM \u0026ldquo;reflection agent\u0026rdquo; analyzes the session\u0026rsquo;s episodes, searches for related patterns in past conversations, and updates a profile of hypotheses and learning goals.\nHere\u0026rsquo;s what the reflection system actually does:\nReviews the session — What patterns appeared in my behavior? What did I ask about? How did I respond to KenLM\u0026rsquo;s answers?\nSearches for evidence — The reflection agent can call a search_episodes(query) tool to find supporting patterns across all past conversations. This prevents overfitting to a single session.\nUpdates hypotheses — For each insight, it calls upsert_hypothesis() with a confidence score. Crucially, confidence only increases when patterns repeat across multiple episodes.\nMaintains learning goals — If the model notices something it doesn\u0026rsquo;t understand about me yet, it creates a learning goal—a question to explore in future interactions.\nHere\u0026rsquo;s what my actual profile looks like after some use:\n{ \u0026#34;hypotheses\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_007\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;accuracy\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth is detail-oriented and expects accurate and precise information.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.6, \u0026#34;evidence_episode_ids\u0026#34;: [ 90, 91, 89, 92 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_008\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;transparency\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth values transparency and honesty in communication.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 89 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_009\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;task_compliance\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth expects tasks to be completed accurately and without unnecessary steps.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 154, 155, 156, 157, 158 ] } ], \u0026#34;learning_goals\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;learning_goal_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;How can we ensure that Kenneth\u0026#39;s instructions are followed without unnecessary repetition or recapitulation?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;closed\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-11-30T15:10:55.704097\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;lg_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;What are Kenneth\u0026#39;s specific expectations for the precision and detail in project updates?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-12-02T15:51:56.904564\u0026#34; } ] } The reflection prompt is designed to be conservative. I explicitly instruct the model to start hypotheses with low confidence (0.2–0.4) and only raise confidence when it sees repeated evidence. This should hopefully prevent the kind of overconfident conclusions that can come from a single unusual interaction.\nLayer 3: The Forgetting System A pretty important piece of learning is actually the process by which we decide which pieces of information aren\u0026rsquo;t relevant. If we were to allow our episodic vector database to grow unbounded with every user interaction, it would quickly become slow and stop being useful during inference or reflection. Moreover, humans also don\u0026rsquo;t retain every bit of information that we are exposed to—we have complex systems that decide what information is worth retaining based on various factors.\nMy algorithm is a bit more rudimentary than the process that the human brain has, but it seems to be working fine so far. It essentially calculates an \u0026ldquo;importance\u0026rdquo; factor for every memory, which is just a weighted sum of a couple factors:\nimportance = w1 * log(1 + access_count) # Access frequency + w2 * is_protected # Protection bonus + w3 * is_evidence # Evidence bonus + w4 * exp(-age_days / tau) # Recency decay This system assigns a high score to frequently accessed memories, memories that the AI has decided it wants to \u0026ldquo;protect,\u0026rdquo; memories that are evidence for hypotheses that it has reasoned about, and memories that are recent. Memories that are old, infrequently accessed, and not useful for any of the learning goals or hypotheses the AI has created become candidates for forgetting.\nThe forgetting process is actually better understood as a summarization process. Instead of just deleting irrelevant memories, the system compacts them. It does this by summarizing \u0026ldquo;forgotten\u0026rdquo; episodes into condensed summaries and deleting the originals. This preserves knowledge while bounding storage growth—similar to how human memories become \u0026ldquo;fuzzy\u0026rdquo; over time.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e\n\u003cp\u003eHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As \u003ca href=\"https://abehrouz.github.io/files/NL.pdf\"\u003ethis paper\u003c/a\u003e from Google points out, LLMs lack \u003cstrong\u003eneuroplasticity\u003c/strong\u003e—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes \u003cem\u003efrozen\u003c/em\u003e. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes we\u0026rsquo;re trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights). All the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions). Each meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals). A distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass passes information between these components, using its working memory (context) as the vehicle. Let me break down how it works piece by piece.\nLayer 1: Episodic Memory This piece of the architecture is the most straightforward. To enable the model to remember specific experiences, every interaction is logged as an episode. Every episode captures the user prompt, agent response, and some metadata for memory management. It looks something like this:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-01T14:30:00\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;Can you help me write a README for this project?\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;Sure. Here\u0026#39;s a concise template...\u0026#34;, \u0026#34;access_count\u0026#34;: 3, \u0026#34;last_accessed\u0026#34;: \u0026#34;2025-12-03T10:15:00\u0026#34;, \u0026#34;is_protected\u0026#34;: false } To enable the model to retrieve relevant episodes at inference time, I utilize a technique known as retrieval-augmented generation (RAG). Simply put, I store the episode data in a vector store called ChromaDB. Embeddings are generated by sentence-transformers after every conversation. Every time I send a message, KenLM embeds my query and searches for the top-k most semantically similar episodes in its memory. This allows it to reference episodes related to my query, so if I ask about sports, it will pull up discussions where we talked about sports. If I ask about documentation, it will remember episodes where we talked about writing documentation.\nThis gives the model something resembling episodic recall: the ability to remember relevant experiences and bring them into working memory (the context window) when they\u0026rsquo;re useful.\nLayer 2: Reflective Memory Episodic memory alone would essentially be just a fancy search engine. Something that humans do that enables more complex learning is reflect on past episodes and draw generalized conclusions based on them. Taking inspiration from this process, I built an agentic reflection process into KenLM.\nWhen I exit a chat session, KenLM runs a reflection loop. A KenLM \u0026ldquo;reflection agent\u0026rdquo; analyzes the session\u0026rsquo;s episodes, searches for related patterns in past conversations, and updates a profile of hypotheses and learning goals.\nHere\u0026rsquo;s what the reflection system actually does:\nReviews the session — What patterns appeared in my behavior? What did I ask about? How did I respond to KenLM\u0026rsquo;s answers?\nSearches for evidence — The reflection agent can call a search_episodes(query) tool to find supporting patterns across all past conversations. This prevents overfitting to a single session.\nUpdates hypotheses — For each insight, it calls upsert_hypothesis() with a confidence score. Crucially, confidence only increases when patterns repeat across multiple episodes.\nMaintains learning goals — If the model notices something it doesn\u0026rsquo;t understand about me yet, it creates a learning goal—a question to explore in future interactions.\nHere\u0026rsquo;s what my actual profile looks like after some use:\n{ \u0026#34;hypotheses\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_007\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;accuracy\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth is detail-oriented and expects accurate and precise information.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.6, \u0026#34;evidence_episode_ids\u0026#34;: [ 90, 91, 89, 92 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_008\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;transparency\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth values transparency and honesty in communication.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 89 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_009\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;task_compliance\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth expects tasks to be completed accurately and without unnecessary steps.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 154, 155, 156, 157, 158 ] } ], \u0026#34;learning_goals\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;learning_goal_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;How can we ensure that Kenneth\u0026#39;s instructions are followed without unnecessary repetition or recapitulation?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;closed\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-11-30T15:10:55.704097\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;lg_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;What are Kenneth\u0026#39;s specific expectations for the precision and detail in project updates?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-12-02T15:51:56.904564\u0026#34; } ] } The reflection prompt is designed to be conservative. I explicitly instruct the model to start hypotheses with low confidence (0.2–0.4) and only raise confidence when it sees repeated evidence. This should hopefully prevent the kind of overconfident conclusions that can come from a single unusual interaction.\nLayer 3: The Forgetting System A pretty important piece of learning is actually the process by which we decide which pieces of information aren\u0026rsquo;t relevant. If we were to allow our episodic vector database to grow unbounded with every user interaction, it would quickly become slow and stop being useful during inference or reflection. Moreover, humans also don\u0026rsquo;t retain every bit of information that we are exposed to—we have complex systems that decide what information is worth retaining based on various factors.\nMy algorithm is a bit more rudimentary than the process that the human brain has, but it seems to be working fine so far. It essentially calculates an \u0026ldquo;importance\u0026rdquo; factor for every memory, which is just a weighted sum of a couple factors:\nimportance = w1 * log(1 + access_count) # Access frequency + w2 * is_protected # Protection bonus + w3 * is_evidence # Evidence bonus + w4 * exp(-age_days / tau) # Recency decay This system assigns a high score to frequently accessed memories, memories that the AI has decided it wants to \u0026ldquo;protect,\u0026rdquo; memories that are evidence for hypotheses that it has reasoned about, and memories that are recent. Memories that are old, infrequently accessed, and not useful for any of the learning goals or hypotheses the AI has created become candidates for forgetting.\nThe forgetting process is actually better understood as a summarization process. Instead of just deleting irrelevant memories, the system compacts them. It does this by summarizing \u0026ldquo;forgotten\u0026rdquo; episodes into condensed summaries and deleting the originals. This preserves knowledge while bounding storage growth—similar to how human memories become \u0026ldquo;fuzzy\u0026rdquo; over time.\nHow It Works End-to-End Let me walk through a complete cycle to show how these pieces fit together.\nStep 1: Message Received\nI type: \u0026ldquo;Can you help me refactor this function to be more readable?\u0026rdquo;\nStep 2: RAG Retrieval\nKenLM embeds my query and searches the episodic index. It finds 3 relevant past conversations about code refactoring and readability preferences.\nStep 3: Context Assembly\nThe system prompt is built from:\nCore identity document (who KenLM is, its purpose) Current profile (hypotheses + learning goals) Recent episodes (last 10 conversations) RAG episodes (the 3 relevant past conversations) Behavioral instructions (generated at session start) Step 4: Deliberation\nKenLM plans its approach: \u0026ldquo;Kenneth wants code refactoring help. Based on my hypothesis about preferring conciseness, I should provide a clean solution without excessive explanation. I don\u0026rsquo;t need tools for this.\u0026rdquo;\nStep 5: Response Generation\nKenLM generates a response, informed by the plan and all the context. The response is concise and code-focused—shaped by what it\u0026rsquo;s learned about me.\nStep 6: Episode Logging\nThe conversation is saved as a new episode and indexed in the vector store.\nStep 7: Session End → Reflection\nWhen I exit the chat, the reflection agent wakes up. It reviews the session, searches for patterns, and decides whether to update any hypotheses or learning goals.\nStep 8: Next Session → Meta-Prompting\nThe next time I start a chat, KenLM reads the (potentially updated) profile and generates fresh behavioral instructions. The cycle continues.\nResults and Observations ","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e\n\u003cp\u003eHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As \u003ca href=\"https://abehrouz.github.io/files/NL.pdf\"\u003ethis paper\u003c/a\u003e from Google points out, LLMs lack \u003cstrong\u003eneuroplasticity\u003c/strong\u003e—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes \u003cem\u003efrozen\u003c/em\u003e. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes we\u0026rsquo;re trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights). All the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions). Each meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals). A distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass passes information between these components, using its working memory (context) as the vehicle. Let me break down how it works piece by piece.\nLayer 1: Episodic Memory This piece of the architecture is the most straightforward. To enable the model to remember specific experiences, every interaction is logged as an episode. Every episode captures the user prompt, agent response, and some metadata for memory management. It looks something like this:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-01T14:30:00\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;Can you help me write a README for this project?\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;Sure. Here\u0026#39;s a concise template...\u0026#34;, \u0026#34;access_count\u0026#34;: 3, \u0026#34;last_accessed\u0026#34;: \u0026#34;2025-12-03T10:15:00\u0026#34;, \u0026#34;is_protected\u0026#34;: false } To enable the model to retrieve relevant episodes at inference time, I utilize a technique known as retrieval-augmented generation (RAG). Simply put, I store the episode data in a vector store called ChromaDB. Embeddings are generated by sentence-transformers after every conversation. Every time I send a message, KenLM embeds my query and searches for the top-k most semantically similar episodes in its memory. This allows it to reference episodes related to my query, so if I ask about sports, it will pull up discussions where we talked about sports. If I ask about documentation, it will remember episodes where we talked about writing documentation.\nThis gives the model something resembling episodic recall: the ability to remember relevant experiences and bring them into working memory (the context window) when they\u0026rsquo;re useful.\nLayer 2: Reflective Memory Episodic memory alone would essentially be just a fancy search engine. Something that humans do that enables more complex learning is reflect on past episodes and draw generalized conclusions based on them. Taking inspiration from this process, I built an agentic reflection process into KenLM.\nWhen I exit a chat session, KenLM runs a reflection loop. A KenLM \u0026ldquo;reflection agent\u0026rdquo; analyzes the session\u0026rsquo;s episodes, searches for related patterns in past conversations, and updates a profile of hypotheses and learning goals.\nHere\u0026rsquo;s what the reflection system actually does:\nReviews the session — What patterns appeared in my behavior? What did I ask about? How did I respond to KenLM\u0026rsquo;s answers?\nSearches for evidence — The reflection agent can call a search_episodes(query) tool to find supporting patterns across all past conversations. This prevents overfitting to a single session.\nUpdates hypotheses — For each insight, it calls upsert_hypothesis() with a confidence score. Crucially, confidence only increases when patterns repeat across multiple episodes.\nMaintains learning goals — If the model notices something it doesn\u0026rsquo;t understand about me yet, it creates a learning goal—a question to explore in future interactions.\nHere\u0026rsquo;s what my actual profile looks like after some use:\n{ \u0026#34;hypotheses\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_007\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;accuracy\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth is detail-oriented and expects accurate and precise information.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.6, \u0026#34;evidence_episode_ids\u0026#34;: [ 90, 91, 89, 92 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_008\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;transparency\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth values transparency and honesty in communication.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 89 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_009\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;task_compliance\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth expects tasks to be completed accurately and without unnecessary steps.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 154, 155, 156, 157, 158 ] } ], \u0026#34;learning_goals\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;learning_goal_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;How can we ensure that Kenneth\u0026#39;s instructions are followed without unnecessary repetition or recapitulation?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;closed\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-11-30T15:10:55.704097\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;lg_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;What are Kenneth\u0026#39;s specific expectations for the precision and detail in project updates?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-12-02T15:51:56.904564\u0026#34; } ] } The reflection prompt is designed to be conservative. I explicitly instruct the model to start hypotheses with low confidence (0.2–0.4) and only raise confidence when it sees repeated evidence. This should hopefully prevent the kind of overconfident conclusions that can come from a single unusual interaction.\nLayer 3: The Forgetting System A pretty important piece of learning is actually the process by which we decide which pieces of information aren\u0026rsquo;t relevant. If we were to allow our episodic vector database to grow unbounded with every user interaction, it would quickly become slow and stop being useful during inference or reflection. Moreover, humans also don\u0026rsquo;t retain every bit of information that we are exposed to—we have complex systems that decide what information is worth retaining based on various factors.\nMy algorithm is a bit more rudimentary than the process that the human brain has, but it seems to be working fine so far. It essentially calculates an \u0026ldquo;importance\u0026rdquo; factor for every memory, which is just a weighted sum of a couple factors:\nimportance = w1 * log(1 + access_count) # Access frequency + w2 * is_protected # Protection bonus + w3 * is_evidence # Evidence bonus + w4 * exp(-age_days / tau) # Recency decay This system assigns a high score to frequently accessed memories, memories that the AI has decided it wants to \u0026ldquo;protect,\u0026rdquo; memories that are evidence for hypotheses that it has reasoned about, and memories that are recent. Memories that are old, infrequently accessed, and not useful for any of the learning goals or hypotheses the AI has created become candidates for forgetting.\nThe forgetting process is actually better understood as a summarization process. Instead of just deleting irrelevant memories, the system compacts them. It does this by summarizing \u0026ldquo;forgotten\u0026rdquo; episodes into condensed summaries and deleting the originals. This preserves knowledge while bounding storage growth—similar to how human memories become \u0026ldquo;fuzzy\u0026rdquo; over time.\nHow It Works End-to-End Let me walk through a complete cycle to show how these pieces fit together.\nStep 1: Message Received\nI type something like: \u0026ldquo;Can you write me a short essay about my favorite sports team?\u0026rdquo;\nStep 2: RAG Retrieval\nKenLM embeds my query and searches the episodic index. It finds 3 relevant past conversations about sports and essay preferences.\nStep 3: Context Assembly\nThe system prompt is built from:\nCore identity document (who KenLM is, its purpose) Current profile (hypotheses + learning goals) RAG episodes (the relevant past conversations) Behavioral instructions (meta-prompt generated at session start) Step 4: Deliberation\nKenLM plans its approach: \u0026ldquo;Kenneth wants an essay about his favorite sports team. Based on my hypothesis about preferring conciseness, I should provide a short essay without excessive explanation. Based on our past conversations, his favorite sports team is the Seattle Sounders. I don\u0026rsquo;t need any external tools for this.\u0026rdquo;\nStep 5: Response Generation\nKenLM generates a response, informed by the plan and all the context. The response is concise and focused—because that\u0026rsquo;s what it\u0026rsquo;s learned that I prefer.\nStep 6: Episode Logging\nThe conversation is saved as a new episode and indexed in the vector store.\nStep 7: Session End → Reflection\nWhen I exit the chat, the reflection agent wakes up. It reviews the session, searches for patterns, and decides whether to update any hypotheses or learning goals.\nStep 8: Next Session → Meta-Prompting\nThe next time I start a chat, KenLM reads the (potentially updated) profile and generates fresh behavioral instructions. Then, the cycle continues.\nResults and Observations ","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e\n\u003cp\u003eHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As \u003ca href=\"https://abehrouz.github.io/files/NL.pdf\"\u003ethis paper\u003c/a\u003e from Google points out, LLMs lack \u003cstrong\u003eneuroplasticity\u003c/strong\u003e—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes \u003cem\u003efrozen\u003c/em\u003e. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes we\u0026rsquo;re trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights). All the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions). Each meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals). A distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass passes information between these components, using its working memory (context) as the vehicle. Let me break down how it works piece by piece.\nLayer 1: Episodic Memory This piece of the architecture is the most straightforward. To enable the model to remember specific experiences, every interaction is logged as an episode. Every episode captures the user prompt, agent response, and some metadata for memory management. It looks something like this:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-01T14:30:00\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;Can you help me write a README for this project?\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;Sure. Here\u0026#39;s a concise template...\u0026#34;, \u0026#34;access_count\u0026#34;: 3, \u0026#34;last_accessed\u0026#34;: \u0026#34;2025-12-03T10:15:00\u0026#34;, \u0026#34;is_protected\u0026#34;: false } To enable the model to retrieve relevant episodes at inference time, I utilize a technique known as retrieval-augmented generation (RAG). Simply put, I store the episode data in a vector store called ChromaDB. Embeddings are generated by sentence-transformers after every conversation. Every time I send a message, KenLM embeds my query and searches for the top-k most semantically similar episodes in its memory. This allows it to reference episodes related to my query, so if I ask about sports, it will pull up discussions where we talked about sports. If I ask about documentation, it will remember episodes where we talked about writing documentation.\nThis gives the model something resembling episodic recall: the ability to remember relevant experiences and bring them into working memory (the context window) when they\u0026rsquo;re useful.\nLayer 2: Reflective Memory Episodic memory alone would essentially be just a fancy search engine. Something that humans do that enables more complex learning is reflect on past episodes and draw generalized conclusions based on them. Taking inspiration from this process, I built an agentic reflection process into KenLM.\nWhen I exit a chat session, KenLM runs a reflection loop. A KenLM \u0026ldquo;reflection agent\u0026rdquo; analyzes the session\u0026rsquo;s episodes, searches for related patterns in past conversations, and updates a profile of hypotheses and learning goals.\nHere\u0026rsquo;s what the reflection system actually does:\nReviews the session — What patterns appeared in my behavior? What did I ask about? How did I respond to KenLM\u0026rsquo;s answers?\nSearches for evidence — The reflection agent can call a search_episodes(query) tool to find supporting patterns across all past conversations. This prevents overfitting to a single session.\nUpdates hypotheses — For each insight, it calls upsert_hypothesis() with a confidence score. Crucially, confidence only increases when patterns repeat across multiple episodes.\nMaintains learning goals — If the model notices something it doesn\u0026rsquo;t understand about me yet, it creates a learning goal—a question to explore in future interactions.\nHere\u0026rsquo;s what my actual profile looks like after some use:\n{ \u0026#34;hypotheses\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_007\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;accuracy\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth is detail-oriented and expects accurate and precise information.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.6, \u0026#34;evidence_episode_ids\u0026#34;: [ 90, 91, 89, 92 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_008\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;transparency\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth values transparency and honesty in communication.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 89 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_009\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;task_compliance\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth expects tasks to be completed accurately and without unnecessary steps.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 154, 155, 156, 157, 158 ] } ], \u0026#34;learning_goals\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;learning_goal_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;How can we ensure that Kenneth\u0026#39;s instructions are followed without unnecessary repetition or recapitulation?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;closed\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-11-30T15:10:55.704097\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;lg_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;What are Kenneth\u0026#39;s specific expectations for the precision and detail in project updates?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-12-02T15:51:56.904564\u0026#34; } ] } The reflection prompt is designed to be conservative. I explicitly instruct the model to start hypotheses with low confidence (0.2–0.4) and only raise confidence when it sees repeated evidence. This should hopefully prevent the kind of overconfident conclusions that can come from a single unusual interaction.\nLayer 3: The Forgetting System A pretty important piece of learning is actually the process by which we decide which pieces of information aren\u0026rsquo;t relevant. If we were to allow our episodic vector database to grow unbounded with every user interaction, it would quickly become slow and stop being useful during inference or reflection. Moreover, humans also don\u0026rsquo;t retain every bit of information that we are exposed to—we have complex systems that decide what information is worth retaining based on various factors.\nMy algorithm is a bit more rudimentary than the process that the human brain has, but it seems to be working fine so far. It essentially calculates an \u0026ldquo;importance\u0026rdquo; factor for every memory, which is just a weighted sum of a couple factors:\nimportance = w1 * log(1 + access_count) # Access frequency + w2 * is_protected # Protection bonus + w3 * is_evidence # Evidence bonus + w4 * exp(-age_days / tau) # Recency decay This system assigns a high score to frequently accessed memories, memories that the AI has decided it wants to \u0026ldquo;protect,\u0026rdquo; memories that are evidence for hypotheses that it has reasoned about, and memories that are recent. Memories that are old, infrequently accessed, and not useful for any of the learning goals or hypotheses the AI has created become candidates for forgetting.\nThe forgetting process is actually better understood as a summarization process. Instead of just deleting irrelevant memories, the system compacts them. It does this by summarizing \u0026ldquo;forgotten\u0026rdquo; episodes into condensed summaries and deleting the originals. This preserves knowledge while bounding storage growth—similar to how human memories become \u0026ldquo;fuzzy\u0026rdquo; over time.\nHow It Works End-to-End Let me walk through a complete cycle to show how these pieces fit together.\nStep 1: Message Received I type something like: \u0026ldquo;Can you write me a short essay about my favorite sports team?\u0026rdquo;\nStep 2: RAG Retrieval KenLM embeds my query and searches the episodic index. It finds 3 relevant past conversations about sports and essay preferences.\nStep 3: Context Assembly The system prompt is built from:\nCore identity document (who KenLM is, its purpose) Current profile (hypotheses + learning goals) RAG episodes (the relevant past conversations) Behavioral instructions (meta-prompt generated at session start) Step 4: Deliberation KenLM plans its approach: \u0026ldquo;Kenneth wants an essay about his favorite sports team. Based on my hypothesis about preferring conciseness, I should provide a short essay without excessive explanation. Based on our past conversations, his favorite sports team is the Seattle Sounders. I don\u0026rsquo;t need any external tools for this.\u0026rdquo;\nStep 5: Response Generation KenLM generates a response, informed by the plan and all the context. The response is concise and focused—because that\u0026rsquo;s what it\u0026rsquo;s learned that I prefer.\nStep 6: Episode Logging The conversation is saved as a new episode and indexed in the vector store.\nStep 7: Session End → Reflection When I exit the chat, the reflection agent wakes up. It reviews the session, searches for patterns, and decides whether to update any hypotheses or learning goals.\nStep 8: Next Session → Meta-Prompting The next time I start a chat, KenLM reads the (potentially updated) profile and generates fresh behavioral instructions. Then, the cycle continues.\nResults and Observations After using this system for a little bit, here are some of the results and observations I\u0026rsquo;ve observed:\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e\n\u003cp\u003eHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As \u003ca href=\"https://abehrouz.github.io/files/NL.pdf\"\u003ethis paper\u003c/a\u003e from Google points out, LLMs lack \u003cstrong\u003eneuroplasticity\u003c/strong\u003e—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes \u003cem\u003efrozen\u003c/em\u003e. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes we\u0026rsquo;re trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights). All the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions). Each meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals). A distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass passes information between these components, using its working memory (context) as the vehicle. Let me break down how it works piece by piece.\nLayer 1: Episodic Memory This piece of the architecture is the most straightforward. To enable the model to remember specific experiences, every interaction is logged as an episode. Every episode captures the user prompt, agent response, and some metadata for memory management. It looks something like this:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-01T14:30:00\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;Can you help me write a README for this project?\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;Sure. Here\u0026#39;s a concise template...\u0026#34;, \u0026#34;access_count\u0026#34;: 3, \u0026#34;last_accessed\u0026#34;: \u0026#34;2025-12-03T10:15:00\u0026#34;, \u0026#34;is_protected\u0026#34;: false } To enable the model to retrieve relevant episodes at inference time, I utilize a technique known as retrieval-augmented generation (RAG). Simply put, I store the episode data in a vector store called ChromaDB. Embeddings are generated by sentence-transformers after every conversation. Every time I send a message, KenLM embeds my query and searches for the top-k most semantically similar episodes in its memory. This allows it to reference episodes related to my query, so if I ask about sports, it will pull up discussions where we talked about sports. If I ask about documentation, it will remember episodes where we talked about writing documentation.\nThis gives the model something resembling episodic recall: the ability to remember relevant experiences and bring them into working memory (the context window) when they\u0026rsquo;re useful.\nLayer 2: Reflective Memory Episodic memory alone would essentially be just a fancy search engine. Something that humans do that enables more complex learning is reflect on past episodes and draw generalized conclusions based on them. Taking inspiration from this process, I built an agentic reflection process into KenLM.\nWhen I exit a chat session, KenLM runs a reflection loop. A KenLM \u0026ldquo;reflection agent\u0026rdquo; analyzes the session\u0026rsquo;s episodes, searches for related patterns in past conversations, and updates a profile of hypotheses and learning goals.\nHere\u0026rsquo;s what the reflection system actually does:\nReviews the session — What patterns appeared in my behavior? What did I ask about? How did I respond to KenLM\u0026rsquo;s answers?\nSearches for evidence — The reflection agent can call a search_episodes(query) tool to find supporting patterns across all past conversations. This prevents overfitting to a single session.\nUpdates hypotheses — For each insight, it calls upsert_hypothesis() with a confidence score. Crucially, confidence only increases when patterns repeat across multiple episodes.\nMaintains learning goals — If the model notices something it doesn\u0026rsquo;t understand about me yet, it creates a learning goal—a question to explore in future interactions.\nHere\u0026rsquo;s what my actual profile looks like after some use:\n{ \u0026#34;hypotheses\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_007\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;accuracy\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth is detail-oriented and expects accurate and precise information.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.6, \u0026#34;evidence_episode_ids\u0026#34;: [ 90, 91, 89, 92 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_008\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;transparency\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth values transparency and honesty in communication.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 89 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_009\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;task_compliance\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth expects tasks to be completed accurately and without unnecessary steps.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 154, 155, 156, 157, 158 ] } ], \u0026#34;learning_goals\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;learning_goal_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;How can we ensure that Kenneth\u0026#39;s instructions are followed without unnecessary repetition or recapitulation?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;closed\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-11-30T15:10:55.704097\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;lg_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;What are Kenneth\u0026#39;s specific expectations for the precision and detail in project updates?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-12-02T15:51:56.904564\u0026#34; } ] } The reflection prompt is designed to be conservative. I explicitly instruct the model to start hypotheses with low confidence (0.2–0.4) and only raise confidence when it sees repeated evidence. This should hopefully prevent the kind of overconfident conclusions that can come from a single unusual interaction.\nLayer 3: The Forgetting System A pretty important piece of learning is actually the process by which we decide which pieces of information aren\u0026rsquo;t relevant. If we were to allow our episodic vector database to grow unbounded with every user interaction, it would quickly become slow and stop being useful during inference or reflection. Moreover, humans also don\u0026rsquo;t retain every bit of information that we are exposed to—we have complex systems that decide what information is worth retaining based on various factors.\nMy algorithm is a bit more rudimentary than the process that the human brain has, but it seems to be working fine so far. It essentially calculates an \u0026ldquo;importance\u0026rdquo; factor for every memory, which is just a weighted sum of a couple factors:\nimportance = w1 * log(1 + access_count) # Access frequency + w2 * is_protected # Protection bonus + w3 * is_evidence # Evidence bonus + w4 * exp(-age_days / tau) # Recency decay This system assigns a high score to frequently accessed memories, memories that the AI has decided it wants to \u0026ldquo;protect,\u0026rdquo; memories that are evidence for hypotheses that it has reasoned about, and memories that are recent. Memories that are old, infrequently accessed, and not useful for any of the learning goals or hypotheses the AI has created become candidates for forgetting.\nThe forgetting process is actually better understood as a summarization process. Instead of just deleting irrelevant memories, the system compacts them. It does this by summarizing \u0026ldquo;forgotten\u0026rdquo; episodes into condensed summaries and deleting the originals. This preserves knowledge while bounding storage growth—similar to how human memories become \u0026ldquo;fuzzy\u0026rdquo; over time.\nHow It Works End-to-End Let me walk through a complete cycle to show how these pieces fit together.\nStep 1: Message Received I type something like: \u0026ldquo;Can you write me a short essay about my favorite sports team?\u0026rdquo;\nStep 2: RAG Retrieval KenLM embeds my query and searches the episodic index. It finds 3 relevant past conversations about sports and essay preferences.\nStep 3: Context Assembly The system prompt is built from:\nCore identity document (who KenLM is, its purpose) Current profile (hypotheses + learning goals) RAG episodes (the relevant past conversations) Behavioral instructions (meta-prompt generated at session start) Step 4: Deliberation KenLM plans its approach: \u0026ldquo;Kenneth wants an essay about his favorite sports team. Based on my hypothesis about preferring conciseness, I should provide a short essay without excessive explanation. Based on our past conversations, his favorite sports team is the Seattle Sounders. I don\u0026rsquo;t need any external tools for this.\u0026rdquo;\nStep 5: Response Generation KenLM generates a response, informed by the plan and all the context. The response is concise and focused—because that\u0026rsquo;s what it\u0026rsquo;s learned that I prefer.\nStep 6: Episode Logging The conversation is saved as a new episode and indexed in the vector store.\nStep 7: Session End → Reflection When I exit the chat, the reflection agent wakes up. It reviews the session, searches for patterns, and decides whether to update any hypotheses or learning goals.\nStep 8: Next Session → Meta-Prompting The next time I start a chat, KenLM reads the (potentially updated) profile and generates fresh behavioral instructions. Then, the cycle continues.\nResults and Observations After using this system for a little bit, here are some of the results and observations I\u0026rsquo;ve observed:\nWhat\u0026rsquo;s working The conciseness hypothesis stuck. Early on, I found myself repeatedly asking KenLM to \u0026ldquo;be more concise\u0026rdquo; and to stop repeating itself. The reflection system noticed this pattern, formed a hypothesis, and now the model defaults to shorter responses. I rarely have to ask anymore.\nRAG retrieval is surprisingly useful. When I ask about something I discussed in a past conversation, relevant context often appears in the model\u0026rsquo;s awareness. It doesn\u0026rsquo;t always explicitly reference the past conversation, but you can tell it\u0026rsquo;s informed by it. I can refer to things like \u0026ldquo;my favorite sports team\u0026rdquo; or \u0026ldquo;where I live\u0026rdquo; and the model remembers what those things are as they relate to me.\nMeta-prompting creates coherent sessions. Each session feels more like what I\u0026rsquo;m looking for from a conversation with an LLM. It\u0026rsquo;s dropped the overly sycophantic tendencies, phrases like \u0026ldquo;Certainly!\u0026rdquo; or \u0026ldquo;You\u0026rsquo;re absolutely right!\u0026rdquo;, and other LLM-isms that have annoyed me. The behavioral instructions provide a stable \u0026ldquo;personality\u0026rdquo; for that session, and that personality is consistently adapting to my preferences.\nWhat\u0026rsquo;s Challenging Certain habits are hard to break. The model really loves markdown. It really loves outputting markdown text. No matter how many times I tell it it\u0026rsquo;s a command line tool and markdown is not helpful, it still really loves outputting neat markdown responses. It\u0026rsquo;s gotten a little better, but I anticipate its preference for markdown is baked into the model weights itself, and not so easy to override through prompting. This is one of the clearest examples I\u0026rsquo;ve found so far of the limitations of this learning system.\nThe 7B model has limitations. My base model is a fine-tuned Qwen 2.5 7B. It\u0026rsquo;s good, but it occasionally misunderstands, hallucinates, or generates malformed tool calls. A larger model would likely perform better, but I wanted something that runs locally on my MacBook.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e\n\u003cp\u003eHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As \u003ca href=\"https://abehrouz.github.io/files/NL.pdf\"\u003ethis paper\u003c/a\u003e from Google points out, LLMs lack \u003cstrong\u003eneuroplasticity\u003c/strong\u003e—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes \u003cem\u003efrozen\u003c/em\u003e. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes we\u0026rsquo;re trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights). All the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions). Each meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals). A distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass passes information between these components, using its working memory (context) as the vehicle. Let me break down how it works piece by piece.\nLayer 1: Episodic Memory This piece of the architecture is the most straightforward. To enable the model to remember specific experiences, every interaction is logged as an episode. Every episode captures the user prompt, agent response, and some metadata for memory management. It looks something like this:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-01T14:30:00\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;Can you help me write a README for this project?\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;Sure. Here\u0026#39;s a concise template...\u0026#34;, \u0026#34;access_count\u0026#34;: 3, \u0026#34;last_accessed\u0026#34;: \u0026#34;2025-12-03T10:15:00\u0026#34;, \u0026#34;is_protected\u0026#34;: false } To enable the model to retrieve relevant episodes at inference time, I utilize a technique known as retrieval-augmented generation (RAG). Simply put, I store the episode data in a vector store called ChromaDB. Embeddings are generated by sentence-transformers after every conversation. Every time I send a message, KenLM embeds my query and searches for the top-k most semantically similar episodes in its memory. This allows it to reference episodes related to my query, so if I ask about sports, it will pull up discussions where we talked about sports. If I ask about documentation, it will remember episodes where we talked about writing documentation.\nThis gives the model something resembling episodic recall: the ability to remember relevant experiences and bring them into working memory (the context window) when they\u0026rsquo;re useful.\nLayer 2: Reflective Memory Episodic memory alone would essentially be just a fancy search engine. Something that humans do that enables more complex learning is reflect on past episodes and draw generalized conclusions based on them. Taking inspiration from this process, I built an agentic reflection process into KenLM.\nWhen I exit a chat session, KenLM runs a reflection loop. A KenLM \u0026ldquo;reflection agent\u0026rdquo; analyzes the session\u0026rsquo;s episodes, searches for related patterns in past conversations, and updates a profile of hypotheses and learning goals.\nHere\u0026rsquo;s what the reflection system actually does:\nReviews the session — What patterns appeared in my behavior? What did I ask about? How did I respond to KenLM\u0026rsquo;s answers?\nSearches for evidence — The reflection agent can call a search_episodes(query) tool to find supporting patterns across all past conversations. This prevents overfitting to a single session.\nUpdates hypotheses — For each insight, it calls upsert_hypothesis() with a confidence score. Crucially, confidence only increases when patterns repeat across multiple episodes.\nMaintains learning goals — If the model notices something it doesn\u0026rsquo;t understand about me yet, it creates a learning goal—a question to explore in future interactions.\nHere\u0026rsquo;s what my actual profile looks like after some use:\n{ \u0026#34;hypotheses\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_007\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;accuracy\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth is detail-oriented and expects accurate and precise information.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.6, \u0026#34;evidence_episode_ids\u0026#34;: [ 90, 91, 89, 92 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_008\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;transparency\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth values transparency and honesty in communication.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 89 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_009\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;task_compliance\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth expects tasks to be completed accurately and without unnecessary steps.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 154, 155, 156, 157, 158 ] } ], \u0026#34;learning_goals\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;learning_goal_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;How can we ensure that Kenneth\u0026#39;s instructions are followed without unnecessary repetition or recapitulation?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;closed\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-11-30T15:10:55.704097\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;lg_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;What are Kenneth\u0026#39;s specific expectations for the precision and detail in project updates?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-12-02T15:51:56.904564\u0026#34; } ] } The reflection prompt is designed to be conservative. I explicitly instruct the model to start hypotheses with low confidence (0.2–0.4) and only raise confidence when it sees repeated evidence. This should hopefully prevent the kind of overconfident conclusions that can come from a single unusual interaction.\nLayer 3: The Forgetting System A pretty important piece of learning is actually the process by which we decide which pieces of information aren\u0026rsquo;t relevant. If we were to allow our episodic vector database to grow unbounded with every user interaction, it would quickly become slow and stop being useful during inference or reflection. Moreover, humans also don\u0026rsquo;t retain every bit of information that we are exposed to—we have complex systems that decide what information is worth retaining based on various factors.\nMy algorithm is a bit more rudimentary than the process that the human brain has, but it seems to be working fine so far. It essentially calculates an \u0026ldquo;importance\u0026rdquo; factor for every memory, which is just a weighted sum of a couple factors:\nimportance = w1 * log(1 + access_count) # Access frequency + w2 * is_protected # Protection bonus + w3 * is_evidence # Evidence bonus + w4 * exp(-age_days / tau) # Recency decay This system assigns a high score to frequently accessed memories, memories that the AI has decided it wants to \u0026ldquo;protect,\u0026rdquo; memories that are evidence for hypotheses that it has reasoned about, and memories that are recent. Memories that are old, infrequently accessed, and not useful for any of the learning goals or hypotheses the AI has created become candidates for forgetting.\nThe forgetting process is actually better understood as a summarization process. Instead of just deleting irrelevant memories, the system compacts them. It does this by summarizing \u0026ldquo;forgotten\u0026rdquo; episodes into condensed summaries and deleting the originals. This preserves knowledge while bounding storage growth—similar to how human memories become \u0026ldquo;fuzzy\u0026rdquo; over time.\nHow It Works End-to-End Let me walk through a complete cycle to show how these pieces fit together.\nStep 1: Message Received I type something like: \u0026ldquo;Can you write me a short essay about my favorite sports team?\u0026rdquo;\nStep 2: RAG Retrieval KenLM embeds my query and searches the episodic index. It finds 3 relevant past conversations about sports and essay preferences.\nStep 3: Context Assembly The system prompt is built from:\nCore identity document (who KenLM is, its purpose) Current profile (hypotheses + learning goals) RAG episodes (the relevant past conversations) Behavioral instructions (meta-prompt generated at session start) Step 4: Deliberation KenLM plans its approach: \u0026ldquo;Kenneth wants an essay about his favorite sports team. Based on my hypothesis about preferring conciseness, I should provide a short essay without excessive explanation. Based on our past conversations, his favorite sports team is the Seattle Sounders. I don\u0026rsquo;t need any external tools for this.\u0026rdquo;\nStep 5: Response Generation KenLM generates a response, informed by the plan and all the context. The response is concise and focused—because that\u0026rsquo;s what it\u0026rsquo;s learned that I prefer.\nStep 6: Episode Logging The conversation is saved as a new episode and indexed in the vector store.\nStep 7: Session End → Reflection When I exit the chat, the reflection agent wakes up. It reviews the session, searches for patterns, and decides whether to update any hypotheses or learning goals.\nStep 8: Next Session → Meta-Prompting The next time I start a chat, KenLM reads the (potentially updated) profile and generates fresh behavioral instructions. Then, the cycle continues.\nResults and Observations After using this system for a little bit, here are some of the results and observations I\u0026rsquo;ve observed:\nWhat\u0026rsquo;s working The conciseness hypothesis stuck. Early on, I found myself repeatedly asking KenLM to \u0026ldquo;be more concise\u0026rdquo; and to stop repeating itself. The reflection system noticed this pattern, formed a hypothesis, and now the model defaults to shorter responses. I rarely have to give feedback about the length of its responses anymore.\nRAG retrieval is surprisingly useful. When I ask about something I discussed in a past conversation, relevant context often appears in the model\u0026rsquo;s awareness. It doesn\u0026rsquo;t always explicitly reference the past conversation, but you can tell it\u0026rsquo;s informed by it. I can refer to things like \u0026ldquo;my favorite sports team\u0026rdquo; or \u0026ldquo;where I live\u0026rdquo; and the model remembers what those things are as they relate to me.\nMeta-prompting effectively influences personality. Each session feels more like what I\u0026rsquo;m looking for from a conversation with an LLM. It\u0026rsquo;s dropped the overly sycophantic tendencies, phrases like \u0026ldquo;Certainly!\u0026rdquo; or \u0026ldquo;You\u0026rsquo;re absolutely right!\u0026rdquo;, and other LLM-isms that have annoyed me. The behavioral instructions provide a stable \u0026ldquo;personality\u0026rdquo; for that session, and that personality is consistently adapting to my preferences.\nWhat\u0026rsquo;s Challenging Certain habits are hard to break. The model really loves markdown. I mean, it really loves outputting markdown formatting. No matter how many times I tell it it\u0026rsquo;s a command line tool and markdown is not helpful, it still really loves outputting neat markdown responses. Now, it HAS gotten a little better after much feedback, but I anticipate its preference for markdown is baked into the model weights itself, and not so easy to override through prompting. This is one of the clearest examples I\u0026rsquo;ve found so far of the limitations of this learning system.\nThe 7B model has limitations. My base model is a fine-tuned Qwen 2.5 7B. It\u0026rsquo;s good, but it occasionally misunderstands, hallucinates, or generates malformed tool calls. A larger model would likely perform better, but I wanted something that runs locally on my MacBook.\nWhat this means ","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e\n\u003cp\u003eHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As \u003ca href=\"https://abehrouz.github.io/files/NL.pdf\"\u003ethis paper\u003c/a\u003e from Google points out, LLMs lack \u003cstrong\u003eneuroplasticity\u003c/strong\u003e—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes \u003cem\u003efrozen\u003c/em\u003e. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes we\u0026rsquo;re trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights). All the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions). Each meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals). A distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass passes information between these components, using its working memory (context) as the vehicle. Let me break down how it works piece by piece.\nLayer 1: Episodic Memory This piece of the architecture is the most straightforward. To enable the model to remember specific experiences, every interaction is logged as an episode. Every episode captures the user prompt, agent response, and some metadata for memory management. It looks something like this:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-01T14:30:00\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;Can you help me write a README for this project?\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;Sure. Here\u0026#39;s a concise template...\u0026#34;, \u0026#34;access_count\u0026#34;: 3, \u0026#34;last_accessed\u0026#34;: \u0026#34;2025-12-03T10:15:00\u0026#34;, \u0026#34;is_protected\u0026#34;: false } To enable the model to retrieve relevant episodes at inference time, I utilize a technique known as retrieval-augmented generation (RAG). Simply put, I store the episode data in a vector store called ChromaDB. Embeddings are generated by sentence-transformers after every conversation. Every time I send a message, KenLM embeds my query and searches for the top-k most semantically similar episodes in its memory. This allows it to reference episodes related to my query, so if I ask about sports, it will pull up discussions where we talked about sports. If I ask about documentation, it will remember episodes where we talked about writing documentation.\nThis gives the model something resembling episodic recall: the ability to remember relevant experiences and bring them into working memory (the context window) when they\u0026rsquo;re useful.\nLayer 2: Reflective Memory Episodic memory alone would essentially be just a fancy search engine. Something that humans do that enables more complex learning is reflect on past episodes and draw generalized conclusions based on them. Taking inspiration from this process, I built an agentic reflection process into KenLM.\nWhen I exit a chat session, KenLM runs a reflection loop. A KenLM \u0026ldquo;reflection agent\u0026rdquo; analyzes the session\u0026rsquo;s episodes, searches for related patterns in past conversations, and updates a profile of hypotheses and learning goals.\nHere\u0026rsquo;s what the reflection system actually does:\nReviews the session — What patterns appeared in my behavior? What did I ask about? How did I respond to KenLM\u0026rsquo;s answers?\nSearches for evidence — The reflection agent can call a search_episodes(query) tool to find supporting patterns across all past conversations. This prevents overfitting to a single session.\nUpdates hypotheses — For each insight, it calls upsert_hypothesis() with a confidence score. Crucially, confidence only increases when patterns repeat across multiple episodes.\nMaintains learning goals — If the model notices something it doesn\u0026rsquo;t understand about me yet, it creates a learning goal—a question to explore in future interactions.\nHere\u0026rsquo;s what my actual profile looks like after some use:\n{ \u0026#34;hypotheses\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_007\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;accuracy\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth is detail-oriented and expects accurate and precise information.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.6, \u0026#34;evidence_episode_ids\u0026#34;: [ 90, 91, 89, 92 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_008\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;transparency\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth values transparency and honesty in communication.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 89 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_009\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;task_compliance\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth expects tasks to be completed accurately and without unnecessary steps.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 154, 155, 156, 157, 158 ] } ], \u0026#34;learning_goals\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;learning_goal_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;How can we ensure that Kenneth\u0026#39;s instructions are followed without unnecessary repetition or recapitulation?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;closed\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-11-30T15:10:55.704097\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;lg_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;What are Kenneth\u0026#39;s specific expectations for the precision and detail in project updates?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-12-02T15:51:56.904564\u0026#34; } ] } The reflection prompt is designed to be conservative. I explicitly instruct the model to start hypotheses with low confidence (0.2–0.4) and only raise confidence when it sees repeated evidence. This should hopefully prevent the kind of overconfident conclusions that can come from a single unusual interaction.\nLayer 3: The Forgetting System A pretty important piece of learning is actually the process by which we decide which pieces of information aren\u0026rsquo;t relevant. If we were to allow our episodic vector database to grow unbounded with every user interaction, it would quickly become slow and stop being useful during inference or reflection. Moreover, humans also don\u0026rsquo;t retain every bit of information that we are exposed to—we have complex systems that decide what information is worth retaining based on various factors.\nMy algorithm is a bit more rudimentary than the process that the human brain has, but it seems to be working fine so far. It essentially calculates an \u0026ldquo;importance\u0026rdquo; factor for every memory, which is just a weighted sum of a couple factors:\nimportance = w1 * log(1 + access_count) # Access frequency + w2 * is_protected # Protection bonus + w3 * is_evidence # Evidence bonus + w4 * exp(-age_days / tau) # Recency decay This system assigns a high score to frequently accessed memories, memories that the AI has decided it wants to \u0026ldquo;protect,\u0026rdquo; memories that are evidence for hypotheses that it has reasoned about, and memories that are recent. Memories that are old, infrequently accessed, and not useful for any of the learning goals or hypotheses the AI has created become candidates for forgetting.\nThe forgetting process is actually better understood as a summarization process. Instead of just deleting irrelevant memories, the system compacts them. It does this by summarizing \u0026ldquo;forgotten\u0026rdquo; episodes into condensed summaries and deleting the originals. This preserves knowledge while bounding storage growth—similar to how human memories become \u0026ldquo;fuzzy\u0026rdquo; over time.\nHow It Works End-to-End Let me walk through a complete cycle to show how these pieces fit together.\nStep 1: Message Received I type something like: \u0026ldquo;Can you write me a short essay about my favorite sports team?\u0026rdquo;\nStep 2: RAG Retrieval KenLM embeds my query and searches the episodic index. It finds 3 relevant past conversations about sports and essay preferences.\nStep 3: Context Assembly The system prompt is built from:\nCore identity document (who KenLM is, its purpose) Current profile (hypotheses + learning goals) RAG episodes (the relevant past conversations) Behavioral instructions (meta-prompt generated at session start) Step 4: Deliberation KenLM plans its approach: \u0026ldquo;Kenneth wants an essay about his favorite sports team. Based on my hypothesis about preferring conciseness, I should provide a short essay without excessive explanation. Based on our past conversations, his favorite sports team is the Seattle Sounders. I don\u0026rsquo;t need any external tools for this.\u0026rdquo;\nStep 5: Response Generation KenLM generates a response, informed by the plan and all the context. The response is concise and focused—because that\u0026rsquo;s what it\u0026rsquo;s learned that I prefer.\nStep 6: Episode Logging The conversation is saved as a new episode and indexed in the vector store.\nStep 7: Session End → Reflection When I exit the chat, the reflection agent wakes up. It reviews the session, searches for patterns, and decides whether to update any hypotheses or learning goals.\nStep 8: Next Session → Meta-Prompting The next time I start a chat, KenLM reads the (potentially updated) profile and generates fresh behavioral instructions. Then, the cycle continues.\nResults and Observations After using this system for a little bit, here are some of the results and observations I\u0026rsquo;ve observed:\nWhat\u0026rsquo;s working The conciseness hypothesis stuck. Early on, I found myself repeatedly asking KenLM to \u0026ldquo;be more concise\u0026rdquo; and to stop repeating itself. The reflection system noticed this pattern, formed a hypothesis, and now the model defaults to shorter responses. I rarely have to give feedback about the length of its responses anymore.\nRAG retrieval is surprisingly useful. When I ask about something I discussed in a past conversation, relevant context often appears in the model\u0026rsquo;s awareness. It doesn\u0026rsquo;t always explicitly reference the past conversation, but you can tell it\u0026rsquo;s informed by it. I can refer to things like \u0026ldquo;my favorite sports team\u0026rdquo; or \u0026ldquo;where I live\u0026rdquo; and the model remembers what those things are as they relate to me.\nMeta-prompting effectively influences personality. Each session feels more like what I\u0026rsquo;m looking for from a conversation with an LLM. It\u0026rsquo;s dropped the overly sycophantic tendencies, phrases like \u0026ldquo;Certainly!\u0026rdquo; or \u0026ldquo;You\u0026rsquo;re absolutely right!\u0026rdquo;, and other LLM-isms that have annoyed me. The behavioral instructions provide a stable \u0026ldquo;personality\u0026rdquo; for that session, and that personality is consistently adapting to my preferences.\nWhat\u0026rsquo;s Challenging Certain habits are hard to break. The model really loves markdown. I mean, it really loves outputting markdown formatting. No matter how many times I tell it it\u0026rsquo;s a command line tool and markdown is not helpful, it still really loves outputting neat markdown responses. Now, it HAS gotten a little better after much feedback, but I anticipate its preference for markdown is baked into the model weights itself, and not so easy to override through prompting. This is one of the clearest examples I\u0026rsquo;ve found so far of the limitations of this learning system.\nThe 7B model has limitations. My base model is a fine-tuned Qwen 2.5 7B. It\u0026rsquo;s good, but it occasionally misunderstands, hallucinates, or generates malformed tool calls. A larger model would likely perform better, but I wanted something that runs locally on my MacBook.\nThis process adds significant latency to the chat process. The RAG retrieval, meta-prompting, and reflection processes all take time. Using a model that\u0026rsquo;s running on my laptop means it can feel painfully slow at times.\nWhat this means This was a fun experiment, but what does this mean? Is there any use for systems like these\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e\n\u003cp\u003eHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As \u003ca href=\"https://abehrouz.github.io/files/NL.pdf\"\u003ethis paper\u003c/a\u003e from Google points out, LLMs lack \u003cstrong\u003eneuroplasticity\u003c/strong\u003e—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes \u003cem\u003efrozen\u003c/em\u003e. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes we\u0026rsquo;re trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights). All the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions). Each meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals). A distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass passes information between these components, using its working memory (context) as the vehicle. Let me break down how it works piece by piece.\nLayer 1: Episodic Memory This piece of the architecture is the most straightforward. To enable the model to remember specific experiences, every interaction is logged as an episode. Every episode captures the user prompt, agent response, and some metadata for memory management. It looks something like this:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-01T14:30:00\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;Can you help me write a README for this project?\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;Sure. Here\u0026#39;s a concise template...\u0026#34;, \u0026#34;access_count\u0026#34;: 3, \u0026#34;last_accessed\u0026#34;: \u0026#34;2025-12-03T10:15:00\u0026#34;, \u0026#34;is_protected\u0026#34;: false } To enable the model to retrieve relevant episodes at inference time, I utilize a technique known as retrieval-augmented generation (RAG). Simply put, I store the episode data in a vector store called ChromaDB. Embeddings are generated by sentence-transformers after every conversation. Every time I send a message, KenLM embeds my query and searches for the top-k most semantically similar episodes in its memory. This allows it to reference episodes related to my query, so if I ask about sports, it will pull up discussions where we talked about sports. If I ask about documentation, it will remember episodes where we talked about writing documentation.\nThis gives the model something resembling episodic recall: the ability to remember relevant experiences and bring them into working memory (the context window) when they\u0026rsquo;re useful.\nLayer 2: Reflective Memory Episodic memory alone would essentially be just a fancy search engine. Something that humans do that enables more complex learning is reflect on past episodes and draw generalized conclusions based on them. Taking inspiration from this process, I built an agentic reflection process into KenLM.\nWhen I exit a chat session, KenLM runs a reflection loop. A KenLM \u0026ldquo;reflection agent\u0026rdquo; analyzes the session\u0026rsquo;s episodes, searches for related patterns in past conversations, and updates a profile of hypotheses and learning goals.\nHere\u0026rsquo;s what the reflection system actually does:\nReviews the session — What patterns appeared in my behavior? What did I ask about? How did I respond to KenLM\u0026rsquo;s answers?\nSearches for evidence — The reflection agent can call a search_episodes(query) tool to find supporting patterns across all past conversations. This prevents overfitting to a single session.\nUpdates hypotheses — For each insight, it calls upsert_hypothesis() with a confidence score. Crucially, confidence only increases when patterns repeat across multiple episodes.\nMaintains learning goals — If the model notices something it doesn\u0026rsquo;t understand about me yet, it creates a learning goal—a question to explore in future interactions.\nHere\u0026rsquo;s what my actual profile looks like after some use:\n{ \u0026#34;hypotheses\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_007\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;accuracy\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth is detail-oriented and expects accurate and precise information.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.6, \u0026#34;evidence_episode_ids\u0026#34;: [ 90, 91, 89, 92 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_008\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;transparency\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth values transparency and honesty in communication.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 89 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_009\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;task_compliance\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth expects tasks to be completed accurately and without unnecessary steps.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 154, 155, 156, 157, 158 ] } ], \u0026#34;learning_goals\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;learning_goal_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;How can we ensure that Kenneth\u0026#39;s instructions are followed without unnecessary repetition or recapitulation?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;closed\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-11-30T15:10:55.704097\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;lg_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;What are Kenneth\u0026#39;s specific expectations for the precision and detail in project updates?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-12-02T15:51:56.904564\u0026#34; } ] } The reflection prompt is designed to be conservative. I explicitly instruct the model to start hypotheses with low confidence (0.2–0.4) and only raise confidence when it sees repeated evidence. This should hopefully prevent the kind of overconfident conclusions that can come from a single unusual interaction.\nLayer 3: The Forgetting System A pretty important piece of learning is actually the process by which we decide which pieces of information aren\u0026rsquo;t relevant. If we were to allow our episodic vector database to grow unbounded with every user interaction, it would quickly become slow and stop being useful during inference or reflection. Moreover, humans also don\u0026rsquo;t retain every bit of information that we are exposed to—we have complex systems that decide what information is worth retaining based on various factors.\nMy algorithm is a bit more rudimentary than the process that the human brain has, but it seems to be working fine so far. It essentially calculates an \u0026ldquo;importance\u0026rdquo; factor for every memory, which is just a weighted sum of a couple factors:\nimportance = w1 * log(1 + access_count) # Access frequency + w2 * is_protected # Protection bonus + w3 * is_evidence # Evidence bonus + w4 * exp(-age_days / tau) # Recency decay This system assigns a high score to frequently accessed memories, memories that the AI has decided it wants to \u0026ldquo;protect,\u0026rdquo; memories that are evidence for hypotheses that it has reasoned about, and memories that are recent. Memories that are old, infrequently accessed, and not useful for any of the learning goals or hypotheses the AI has created become candidates for forgetting.\nThe forgetting process is actually better understood as a summarization process. Instead of just deleting irrelevant memories, the system compacts them. It does this by summarizing \u0026ldquo;forgotten\u0026rdquo; episodes into condensed summaries and deleting the originals. This preserves knowledge while bounding storage growth—similar to how human memories become \u0026ldquo;fuzzy\u0026rdquo; over time.\nHow It Works End-to-End Let me walk through a complete cycle to show how these pieces fit together.\nStep 1: Message Received I type something like: \u0026ldquo;Can you write me a short essay about my favorite sports team?\u0026rdquo;\nStep 2: RAG Retrieval KenLM embeds my query and searches the episodic index. It finds 3 relevant past conversations about sports and essay preferences.\nStep 3: Context Assembly The system prompt is built from:\nCore identity document (who KenLM is, its purpose) Current profile (hypotheses + learning goals) RAG episodes (the relevant past conversations) Behavioral instructions (meta-prompt generated at session start) Step 4: Deliberation KenLM plans its approach: \u0026ldquo;Kenneth wants an essay about his favorite sports team. Based on my hypothesis about preferring conciseness, I should provide a short essay without excessive explanation. Based on our past conversations, his favorite sports team is the Seattle Sounders. I don\u0026rsquo;t need any external tools for this.\u0026rdquo;\nStep 5: Response Generation KenLM generates a response, informed by the plan and all the context. The response is concise and focused—because that\u0026rsquo;s what it\u0026rsquo;s learned that I prefer.\nStep 6: Episode Logging The conversation is saved as a new episode and indexed in the vector store.\nStep 7: Session End → Reflection When I exit the chat, the reflection agent wakes up. It reviews the session, searches for patterns, and decides whether to update any hypotheses or learning goals.\nStep 8: Next Session → Meta-Prompting The next time I start a chat, KenLM reads the (potentially updated) profile and generates fresh behavioral instructions. Then, the cycle continues.\nResults and Observations After using this system for a little bit, here are some of the results and observations I\u0026rsquo;ve observed:\nWhat\u0026rsquo;s working The conciseness hypothesis stuck. Early on, I found myself repeatedly asking KenLM to \u0026ldquo;be more concise\u0026rdquo; and to stop repeating itself. The reflection system noticed this pattern, formed a hypothesis, and now the model defaults to shorter responses. I rarely have to give feedback about the length of its responses anymore.\nRAG retrieval is surprisingly useful. When I ask about something I discussed in a past conversation, relevant context often appears in the model\u0026rsquo;s awareness. It doesn\u0026rsquo;t always explicitly reference the past conversation, but you can tell it\u0026rsquo;s informed by it. I can refer to things like \u0026ldquo;my favorite sports team\u0026rdquo; or \u0026ldquo;where I live\u0026rdquo; and the model remembers what those things are as they relate to me.\nMeta-prompting effectively influences personality. Each session feels more like what I\u0026rsquo;m looking for from a conversation with an LLM. It\u0026rsquo;s dropped the overly sycophantic tendencies, phrases like \u0026ldquo;Certainly!\u0026rdquo; or \u0026ldquo;You\u0026rsquo;re absolutely right!\u0026rdquo;, and other LLM-isms that have annoyed me. The behavioral instructions provide a stable \u0026ldquo;personality\u0026rdquo; for that session, and that personality is consistently adapting to my preferences.\nWhat\u0026rsquo;s Challenging Certain habits are hard to break. The model really loves markdown. I mean, it really loves outputting markdown formatting. No matter how many times I tell it it\u0026rsquo;s a command line tool and markdown is not helpful, it still really loves outputting neat markdown responses. Now, it HAS gotten a little better after much feedback, but I anticipate its preference for markdown is baked into the model weights itself, and not so easy to override through prompting. This is one of the clearest examples I\u0026rsquo;ve found so far of the limitations of this learning system.\nThe 7B model has limitations. My base model is a fine-tuned Qwen 2.5 7B. It\u0026rsquo;s good, but it occasionally misunderstands, hallucinates, or generates malformed tool calls. A larger model would likely perform better, but I wanted something that runs locally on my MacBook.\nThis process adds significant latency to the chat process. The RAG retrieval, meta-prompting, and reflection processes all take time. Using a model that\u0026rsquo;s running on my laptop means it can feel painfully slow at times.\nWhat this means This was a fun experiment, but what does this mean? Is there any use for systems like these in industry?\nI\u0026rsquo;m far from an AI expert, but off the top of my head I can think of a couple potential applications of systems like these to develop real AI systems that solve real-world problems. Giving an LLM the ability to learn from experience opens the door to all kinds of systems we don’t really have today.\nThe obvious application is personalization: assistants that remember your preferences and workflows, which is what KenLM is. But the implications go beyond “your AI knows your favorite sports team.” Imagine a model that initially struggles with chain-of-thought reasoning or tool usage—say something that consistently runs into the same error when trying to write python code that runs in a sandbox. Currently, AI systems have no way to notice these things and improve. Human engineers need to monitor and step in, adjusting prompts, doing reinforcement learning, or collecting data with which to fine tune in order to eke out a little bit better performance.\nA memory-equipped model changes this dynamic. For anything it\u0026rsquo;s trying to optimize for, be that reasoning or tool use, it can:\nnotice failed patterns by reasoning about them and remembering past experiences see successful patterns internalize the better method and adjust for future tasks Instead of updating billions of weights, the model updates a small set of learned strategies derived from lived experience. Over time, the model becomes a better problem solver not because it grew larger, but because it learned.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e\n\u003cp\u003eHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As \u003ca href=\"https://abehrouz.github.io/files/NL.pdf\"\u003ethis paper\u003c/a\u003e from Google points out, LLMs lack \u003cstrong\u003eneuroplasticity\u003c/strong\u003e—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes \u003cem\u003efrozen\u003c/em\u003e. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes we\u0026rsquo;re trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights). All the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions). Each meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals). A distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass passes information between these components, using its working memory (context) as the vehicle. Let me break down how it works piece by piece.\nLayer 1: Episodic Memory This piece of the architecture is the most straightforward. To enable the model to remember specific experiences, every interaction is logged as an episode. Every episode captures the user prompt, agent response, and some metadata for memory management. It looks something like this:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-01T14:30:00\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;Can you help me write a README for this project?\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;Sure. Here\u0026#39;s a concise template...\u0026#34;, \u0026#34;access_count\u0026#34;: 3, \u0026#34;last_accessed\u0026#34;: \u0026#34;2025-12-03T10:15:00\u0026#34;, \u0026#34;is_protected\u0026#34;: false } To enable the model to retrieve relevant episodes at inference time, I utilize a technique known as retrieval-augmented generation (RAG). Simply put, I store the episode data in a vector store called ChromaDB. Embeddings are generated by sentence-transformers after every conversation. Every time I send a message, KenLM embeds my query and searches for the top-k most semantically similar episodes in its memory. This allows it to reference episodes related to my query, so if I ask about sports, it will pull up discussions where we talked about sports. If I ask about documentation, it will remember episodes where we talked about writing documentation.\nThis gives the model something resembling episodic recall: the ability to remember relevant experiences and bring them into working memory (the context window) when they\u0026rsquo;re useful.\nLayer 2: Reflective Memory Episodic memory alone would essentially be just a fancy search engine. Something that humans do that enables more complex learning is reflect on past episodes and draw generalized conclusions based on them. Taking inspiration from this process, I built an agentic reflection process into KenLM.\nWhen I exit a chat session, KenLM runs a reflection loop. A KenLM \u0026ldquo;reflection agent\u0026rdquo; analyzes the session\u0026rsquo;s episodes, searches for related patterns in past conversations, and updates a profile of hypotheses and learning goals.\nHere\u0026rsquo;s what the reflection system actually does:\nReviews the session — What patterns appeared in my behavior? What did I ask about? How did I respond to KenLM\u0026rsquo;s answers?\nSearches for evidence — The reflection agent can call a search_episodes(query) tool to find supporting patterns across all past conversations. This prevents overfitting to a single session.\nUpdates hypotheses — For each insight, it calls upsert_hypothesis() with a confidence score. Crucially, confidence only increases when patterns repeat across multiple episodes.\nMaintains learning goals — If the model notices something it doesn\u0026rsquo;t understand about me yet, it creates a learning goal—a question to explore in future interactions.\nHere\u0026rsquo;s what my actual profile looks like after some use:\n{ \u0026#34;hypotheses\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_007\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;accuracy\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth is detail-oriented and expects accurate and precise information.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.6, \u0026#34;evidence_episode_ids\u0026#34;: [ 90, 91, 89, 92 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_008\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;transparency\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth values transparency and honesty in communication.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 89 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_009\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;task_compliance\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth expects tasks to be completed accurately and without unnecessary steps.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 154, 155, 156, 157, 158 ] } ], \u0026#34;learning_goals\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;learning_goal_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;How can we ensure that Kenneth\u0026#39;s instructions are followed without unnecessary repetition or recapitulation?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;closed\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-11-30T15:10:55.704097\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;lg_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;What are Kenneth\u0026#39;s specific expectations for the precision and detail in project updates?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-12-02T15:51:56.904564\u0026#34; } ] } The reflection prompt is designed to be conservative. I explicitly instruct the model to start hypotheses with low confidence (0.2–0.4) and only raise confidence when it sees repeated evidence. This should hopefully prevent the kind of overconfident conclusions that can come from a single unusual interaction.\nLayer 3: The Forgetting System A pretty important piece of learning is actually the process by which we decide which pieces of information aren\u0026rsquo;t relevant. If we were to allow our episodic vector database to grow unbounded with every user interaction, it would quickly become slow and stop being useful during inference or reflection. Moreover, humans also don\u0026rsquo;t retain every bit of information that we are exposed to—we have complex systems that decide what information is worth retaining based on various factors.\nMy algorithm is a bit more rudimentary than the process that the human brain has, but it seems to be working fine so far. It essentially calculates an \u0026ldquo;importance\u0026rdquo; factor for every memory, which is just a weighted sum of a couple factors:\nimportance = w1 * log(1 + access_count) # Access frequency + w2 * is_protected # Protection bonus + w3 * is_evidence # Evidence bonus + w4 * exp(-age_days / tau) # Recency decay This system assigns a high score to frequently accessed memories, memories that the AI has decided it wants to \u0026ldquo;protect,\u0026rdquo; memories that are evidence for hypotheses that it has reasoned about, and memories that are recent. Memories that are old, infrequently accessed, and not useful for any of the learning goals or hypotheses the AI has created become candidates for forgetting.\nThe forgetting process is actually better understood as a summarization process. Instead of just deleting irrelevant memories, the system compacts them. It does this by summarizing \u0026ldquo;forgotten\u0026rdquo; episodes into condensed summaries and deleting the originals. This preserves knowledge while bounding storage growth—similar to how human memories become \u0026ldquo;fuzzy\u0026rdquo; over time.\nHow It Works End-to-End Let me walk through a complete cycle to show how these pieces fit together.\nStep 1: Message Received I type something like: \u0026ldquo;Can you write me a short essay about my favorite sports team?\u0026rdquo;\nStep 2: RAG Retrieval KenLM embeds my query and searches the episodic index. It finds 3 relevant past conversations about sports and essay preferences.\nStep 3: Context Assembly The system prompt is built from:\nCore identity document (who KenLM is, its purpose) Current profile (hypotheses + learning goals) RAG episodes (the relevant past conversations) Behavioral instructions (meta-prompt generated at session start) Step 4: Deliberation KenLM plans its approach: \u0026ldquo;Kenneth wants an essay about his favorite sports team. Based on my hypothesis about preferring conciseness, I should provide a short essay without excessive explanation. Based on our past conversations, his favorite sports team is the Seattle Sounders. I don\u0026rsquo;t need any external tools for this.\u0026rdquo;\nStep 5: Response Generation KenLM generates a response, informed by the plan and all the context. The response is concise and focused—because that\u0026rsquo;s what it\u0026rsquo;s learned that I prefer.\nStep 6: Episode Logging The conversation is saved as a new episode and indexed in the vector store.\nStep 7: Session End → Reflection When I exit the chat, the reflection agent wakes up. It reviews the session, searches for patterns, and decides whether to update any hypotheses or learning goals.\nStep 8: Next Session → Meta-Prompting The next time I start a chat, KenLM reads the (potentially updated) profile and generates fresh behavioral instructions. Then, the cycle continues.\nResults and Observations After using this system for a little bit, here are some of the results and observations I\u0026rsquo;ve observed:\nWhat\u0026rsquo;s working The conciseness hypothesis stuck. Early on, I found myself repeatedly asking KenLM to \u0026ldquo;be more concise\u0026rdquo; and to stop repeating itself. The reflection system noticed this pattern, formed a hypothesis, and now the model defaults to shorter responses. I rarely have to give feedback about the length of its responses anymore.\nRAG retrieval is surprisingly useful. When I ask about something I discussed in a past conversation, relevant context often appears in the model\u0026rsquo;s awareness. It doesn\u0026rsquo;t always explicitly reference the past conversation, but you can tell it\u0026rsquo;s informed by it. I can refer to things like \u0026ldquo;my favorite sports team\u0026rdquo; or \u0026ldquo;where I live\u0026rdquo; and the model remembers what those things are as they relate to me.\nMeta-prompting effectively influences personality. Each session feels more like what I\u0026rsquo;m looking for from a conversation with an LLM. It\u0026rsquo;s dropped the overly sycophantic tendencies, phrases like \u0026ldquo;Certainly!\u0026rdquo; or \u0026ldquo;You\u0026rsquo;re absolutely right!\u0026rdquo;, and other LLM-isms that have annoyed me. The behavioral instructions provide a stable \u0026ldquo;personality\u0026rdquo; for that session, and that personality is consistently adapting to my preferences.\nWhat\u0026rsquo;s Challenging Certain habits are hard to break. The model really loves markdown. I mean, it really loves outputting markdown formatting. No matter how many times I tell it it\u0026rsquo;s a command line tool and markdown is not helpful, it still really loves outputting neat markdown responses. Now, it HAS gotten a little better after much feedback, but I anticipate its preference for markdown is baked into the model weights itself, and not so easy to override through prompting. This is one of the clearest examples I\u0026rsquo;ve found so far of the limitations of this learning system.\nThe 7B model has limitations. My base model is a fine-tuned Qwen 2.5 7B. It\u0026rsquo;s good, but it occasionally misunderstands, hallucinates, or generates malformed tool calls. A larger model would likely perform better, but I wanted something that runs locally on my MacBook.\nThis process adds significant latency to the chat process. The RAG retrieval, meta-prompting, and reflection processes all take time. Using a model that\u0026rsquo;s running on my laptop means it can feel painfully slow at times.\nWhat this means This was a fun experiment, but what does this mean? Is there any use for systems like these in industry?\nI\u0026rsquo;m far from an AI expert, but off the top of my head I can think of a couple potential applications of systems like these to develop real AI systems that solve real-world problems. Giving an LLM the ability to learn from experience opens the door to all kinds of systems we don’t really have today.\nThe obvious application is personalization: assistants that remember your preferences and workflows, which is what KenLM is. But the implications go beyond “your AI knows your favorite sports team.” Imagine an educational model that can learn how you learn and what you know, remember your strong and weak areas, and adapt its explanations to your thinking style. This model could act as a more helpful learning companion than a model with no such memory capabilities.\nAnother application is models that improve themselves by optimizing for factors outside of human interaction. Imagine a model that initially struggles with chain-of-thought reasoning or tool usage—say something that consistently runs into the same error when trying to write python code that runs in a sandbox. Currently, AI systems have no way to notice these things and improve. Human engineers need to monitor and step in, adjusting prompts, doing reinforcement learning, or collecting data with which to fine-tune in order to eke out a little bit better performance.\nA memory-equipped model changes this dynamic. For anything it\u0026rsquo;s trying to optimize for, be that reasoning or tool use, it can:\nnotice failed patterns by reasoning about them and remembering past experiences see successful patterns internalize the better method and adjust for future tasks Instead of updating billions of weights, the model updates a small set of learned strategies derived from lived experience. Over time, the model becomes a better problem solver not because it grew larger, but because it learned.\nThis all can potentially allow for smaller, more efficient models to rapidly adapt to specific use cases, reducing the industry\u0026rsquo;s reliance on frontier, compute-heavy models and allowing for more innovation and experimentation at the edge. Instead of treating model improvement as something that can only happen upstream, by massive labs with huge quantities of (often stolen) data, you get a world where models improve downstream, close to the user, close to the task, and close to the environment they operate in.\nIn that world, you don’t need a 400B-parameter model for everything. You need a reasonably capable base model with the right scaffolding: a memory system, a reflection system, and a mechanism for gradually internalizing the lessons it learns. Given enough experience, my intuition is that a modest model can outperform a much larger one on the tasks it specializes for, because it has context, history, and self-derived strategies the bigger model simply doesn’t have.\nIn the end, this really was just a weekend experiment, nothing more than an excuse to poke at a problem that’s been stuck in my head. But now that it’s working, even in this early form, I think there are probably real ideas worth exploring here. I don’t know exactly where it will go yet, but I’m excited to keep experimenting and see how far a small model can go when you let it learn from its own experiences.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e\n\u003cp\u003eHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As \u003ca href=\"https://abehrouz.github.io/files/NL.pdf\"\u003ethis paper\u003c/a\u003e from Google points out, LLMs lack \u003cstrong\u003eneuroplasticity\u003c/strong\u003e—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes \u003cem\u003efrozen\u003c/em\u003e. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes we\u0026rsquo;re trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights). All the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions). Each meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals). A distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass passes information between these components, using its working memory (context) as the vehicle. Let me break down how it works piece by piece.\nLayer 1: Episodic Memory This piece of the architecture is the most straightforward. To enable the model to remember specific experiences, every interaction is logged as an episode. Every episode captures the user prompt, agent response, and some metadata for memory management. It looks something like this:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-01T14:30:00\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;Can you help me write a README for this project?\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;Sure. Here\u0026#39;s a concise template...\u0026#34;, \u0026#34;access_count\u0026#34;: 3, \u0026#34;last_accessed\u0026#34;: \u0026#34;2025-12-03T10:15:00\u0026#34;, \u0026#34;is_protected\u0026#34;: false } To enable the model to retrieve relevant episodes at inference time, I utilize a technique known as retrieval-augmented generation (RAG). Simply put, I store the episode data in a vector store called ChromaDB. Embeddings are generated by sentence-transformers after every conversation. Every time I send a message, KenLM embeds my query and searches for the top-k most semantically similar episodes in its memory. This allows it to reference episodes related to my query, so if I ask about sports, it will pull up discussions where we talked about sports. If I ask about documentation, it will remember episodes where we talked about writing documentation.\nThis gives the model something resembling episodic recall: the ability to remember relevant experiences and bring them into working memory (the context window) when they\u0026rsquo;re useful.\nLayer 2: Reflective Memory Episodic memory alone would essentially be just a fancy search engine. Something that humans do that enables more complex learning is reflect on past episodes and draw generalized conclusions based on them. Taking inspiration from this process, I built an agentic reflection process into KenLM.\nWhen I exit a chat session, KenLM runs a reflection loop. A KenLM \u0026ldquo;reflection agent\u0026rdquo; analyzes the session\u0026rsquo;s episodes, searches for related patterns in past conversations, and updates a profile of hypotheses and learning goals.\nHere\u0026rsquo;s what the reflection system actually does:\nReviews the session — What patterns appeared in my behavior? What did I ask about? How did I respond to KenLM\u0026rsquo;s answers?\nSearches for evidence — The reflection agent can call a search_episodes(query) tool to find supporting patterns across all past conversations. This prevents overfitting to a single session.\nUpdates hypotheses — For each insight, it calls upsert_hypothesis() with a confidence score. Crucially, confidence only increases when patterns repeat across multiple episodes.\nMaintains learning goals — If the model notices something it doesn\u0026rsquo;t understand about me yet, it creates a learning goal—a question to explore in future interactions.\nHere\u0026rsquo;s what my actual profile looks like after some use:\n{ \u0026#34;hypotheses\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_007\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;accuracy\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth is detail-oriented and expects accurate and precise information.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.6, \u0026#34;evidence_episode_ids\u0026#34;: [ 90, 91, 89, 92 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_008\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;transparency\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth values transparency and honesty in communication.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 89 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_009\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;task_compliance\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth expects tasks to be completed accurately and without unnecessary steps.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 154, 155, 156, 157, 158 ] } ], \u0026#34;learning_goals\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;learning_goal_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;How can we ensure that Kenneth\u0026#39;s instructions are followed without unnecessary repetition or recapitulation?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;closed\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-11-30T15:10:55.704097\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;lg_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;What are Kenneth\u0026#39;s specific expectations for the precision and detail in project updates?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-12-02T15:51:56.904564\u0026#34; } ] } The reflection prompt is designed to be conservative. I explicitly instruct the model to start hypotheses with low confidence (0.2–0.4) and only raise confidence when it sees repeated evidence. This should hopefully prevent the kind of overconfident conclusions that can come from a single unusual interaction.\nLayer 3: The Forgetting System A pretty important piece of learning is actually the process by which we decide which pieces of information aren\u0026rsquo;t relevant. If we were to allow our episodic vector database to grow unbounded with every user interaction, it would quickly become slow and stop being useful during inference or reflection. Moreover, humans also don\u0026rsquo;t retain every bit of information that we are exposed to—we have complex systems that decide what information is worth retaining based on various factors.\nMy algorithm is a bit more rudimentary than the process that the human brain has, but it seems to be working fine so far. It essentially calculates an \u0026ldquo;importance\u0026rdquo; factor for every memory, which is just a weighted sum of a couple factors:\nimportance = w1 * log(1 + access_count) # Access frequency + w2 * is_protected # Protection bonus + w3 * is_evidence # Evidence bonus + w4 * exp(-age_days / tau) # Recency decay This system assigns a high score to frequently accessed memories, memories that the AI has decided it wants to \u0026ldquo;protect,\u0026rdquo; memories that are evidence for hypotheses that it has reasoned about, and memories that are recent. Memories that are old, infrequently accessed, and not useful for any of the learning goals or hypotheses the AI has created become candidates for forgetting.\nThe forgetting process is actually better understood as a summarization process. Instead of just deleting irrelevant memories, the system compacts them. It does this by summarizing \u0026ldquo;forgotten\u0026rdquo; episodes into condensed summaries and deleting the originals. This preserves knowledge while bounding storage growth—similar to how human memories become \u0026ldquo;fuzzy\u0026rdquo; over time.\nHow It Works End-to-End Let me walk through a complete cycle to show how these pieces fit together.\nStep 1: Message Received I type something like: \u0026ldquo;Can you write me a short essay about my favorite sports team?\u0026rdquo;\nStep 2: RAG Retrieval KenLM embeds my query and searches the episodic index. It finds 3 relevant past conversations about sports and essay preferences.\nStep 3: Context Assembly The system prompt is built from:\nCore identity document (who KenLM is, its purpose) Current profile (hypotheses + learning goals) RAG episodes (the relevant past conversations) Behavioral instructions (meta-prompt generated at session start) Step 4: Deliberation KenLM plans its approach: \u0026ldquo;Kenneth wants an essay about his favorite sports team. Based on my hypothesis about preferring conciseness, I should provide a short essay without excessive explanation. Based on our past conversations, his favorite sports team is the Seattle Sounders. I don\u0026rsquo;t need any external tools for this.\u0026rdquo;\nStep 5: Response Generation KenLM generates a response, informed by the plan and all the context. The response is concise and focused—because that\u0026rsquo;s what it\u0026rsquo;s learned that I prefer.\nStep 6: Episode Logging The conversation is saved as a new episode and indexed in the vector store.\nStep 7: Session End → Reflection When I exit the chat, the reflection agent wakes up. It reviews the session, searches for patterns, and decides whether to update any hypotheses or learning goals.\nStep 8: Next Session → Meta-Prompting The next time I start a chat, KenLM reads the (potentially updated) profile and generates fresh behavioral instructions. Then, the cycle continues.\nResults and Observations After using this system for a little bit, here are some of the results and observations I\u0026rsquo;ve observed:\nWhat\u0026rsquo;s working The conciseness hypothesis stuck. Early on, I found myself repeatedly asking KenLM to \u0026ldquo;be more concise\u0026rdquo; and to stop repeating itself. The reflection system noticed this pattern, formed a hypothesis, and now the model defaults to shorter responses. I rarely have to give feedback about the length of its responses anymore.\nRAG retrieval is surprisingly useful. When I ask about something I discussed in a past conversation, relevant context often appears in the model\u0026rsquo;s awareness. It doesn\u0026rsquo;t always explicitly reference the past conversation, but you can tell it\u0026rsquo;s informed by it. I can refer to things like \u0026ldquo;my favorite sports team\u0026rdquo; or \u0026ldquo;where I live\u0026rdquo; and the model remembers what those things are as they relate to me.\nMeta-prompting effectively influences personality. Each session feels more like what I\u0026rsquo;m looking for from a conversation with an LLM. It\u0026rsquo;s dropped the overly sycophantic tendencies, phrases like \u0026ldquo;Certainly!\u0026rdquo; or \u0026ldquo;You\u0026rsquo;re absolutely right!\u0026rdquo;, and other LLM-isms that have annoyed me. The behavioral instructions provide a stable \u0026ldquo;personality\u0026rdquo; for that session, and that personality is consistently adapting to my preferences.\nWhat\u0026rsquo;s Challenging Certain habits are hard to break. The model really loves markdown. I mean, it really loves outputting markdown formatting. No matter how many times I tell it it\u0026rsquo;s a command line tool and markdown is not helpful, it still really loves outputting neat markdown responses. Now, it HAS gotten a little better after much feedback, but I anticipate its preference for markdown is baked into the model weights itself, and not so easy to override through prompting. This is one of the clearest examples I\u0026rsquo;ve found so far of the limitations of this learning system.\nThe 7B model has limitations. My base model is a fine-tuned Qwen 2.5 7B. It\u0026rsquo;s good, but it occasionally misunderstands, hallucinates, or generates malformed tool calls. A larger model would likely perform better, but I wanted something that runs locally on my MacBook.\nThis process adds significant latency to the chat process. The RAG retrieval, meta-prompting, and reflection processes all take time. Using a model that\u0026rsquo;s running on my laptop means it can feel painfully slow at times.\nWhat this means This was a fun experiment, but what does this mean? Is there any use for systems like these in industry?\nI\u0026rsquo;m far from an AI expert, but off the top of my head I can think of a couple potential applications of systems like these to develop real AI systems that solve real-world problems. Giving an LLM the ability to learn from experience opens the door to all kinds of systems we don’t really have today.\nThe obvious application is personalization: assistants that remember your preferences and workflows, which is what KenLM is. But the implications go beyond “your AI knows your favorite sports team.” Imagine an educational model that can learn how you learn and what you know, remember your strong and weak areas, and adapt its explanations to your thinking style. This model could act as a more helpful learning companion than a model with no such memory capabilities.\nAnother application is models that improve themselves by optimizing for factors outside of human interaction. Imagine a model that initially struggles with chain-of-thought reasoning or tool usage—say something that consistently runs into the same error when trying to write python code that runs in a sandbox. Currently, AI systems have no way to notice these things and improve. Human engineers need to monitor and step in, adjusting prompts, doing reinforcement learning, or collecting data with which to fine-tune in order to eke out a little bit better performance.\nA memory-equipped model changes this dynamic. For anything it\u0026rsquo;s trying to optimize for, be that reasoning or tool use, it can:\nnotice failed patterns by reasoning about them and remembering past experiences see successful patterns internalize the better method and adjust for future tasks Instead of updating billions of weights, the model updates a small set of learned strategies derived from lived experience. Over time, the model becomes a better problem solver not because it grew larger, but because it learned.\nThis all can potentially allow for smaller, more efficient models to rapidly adapt to specific use cases, reducing the industry\u0026rsquo;s reliance on frontier, compute-heavy models and allowing for more innovation and experimentation at the edge. Instead of treating model improvement as something that can only happen upstream, by massive labs with huge quantities of (often stolen) data, you get a world where models improve downstream, close to the user, close to the task, and close to the environment they operate in.\nIn that world, you don’t need a 400B-parameter model for everything. You need a reasonably capable base model with the right scaffolding: a memory system, a reflection system, and a mechanism for gradually internalizing the lessons it learns. Given enough experience, my intuition is that a modest model can outperform a much larger one on the tasks it specializes for, because it has context, history, and self-derived strategies the bigger model simply doesn’t have.\nIn the end, this really was just a weekend experiment, nothing more than an excuse to poke at a problem that’s been stuck in my head. But now that it’s working, even in this early form, I think there are probably real ideas worth exploring here. I don’t know exactly where it will go yet, but I’m excited to keep experimenting and see how far a small model can go when you let it learn from its own experiences.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e\n\u003cp\u003eHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As \u003ca href=\"https://abehrouz.github.io/files/NL.pdf\"\u003ethis paper\u003c/a\u003e from Google points out, LLMs lack \u003cstrong\u003eneuroplasticity\u003c/strong\u003e—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes \u003cem\u003efrozen\u003c/em\u003e. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes we\u0026rsquo;re trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights). All the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions). Each meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals). A distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass passes information between these components, using its working memory (context) as the vehicle. Let me break down how it works piece by piece.\nLayer 1: Episodic Memory This piece of the architecture is the most straightforward. To enable the model to remember specific experiences, every interaction is logged as an episode. Every episode captures the user prompt, agent response, and some metadata for memory management. It looks something like this:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-01T14:30:00\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;Can you help me write a README for this project?\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;Sure. Here\u0026#39;s a concise template...\u0026#34;, \u0026#34;access_count\u0026#34;: 3, \u0026#34;last_accessed\u0026#34;: \u0026#34;2025-12-03T10:15:00\u0026#34;, \u0026#34;is_protected\u0026#34;: false } To enable the model to retrieve relevant episodes at inference time, I utilize a technique known as retrieval-augmented generation (RAG). Simply put, I store the episode data in a vector store called ChromaDB. Embeddings are generated by sentence-transformers after every conversation. Every time I send a message, KenLM embeds my query and searches for the top-k most semantically similar episodes in its memory. This allows it to reference episodes related to my query, so if I ask about sports, it will pull up discussions where we talked about sports. If I ask about documentation, it will remember episodes where we talked about writing documentation.\nThis gives the model something resembling episodic recall: the ability to remember relevant experiences and bring them into working memory (the context window) when they\u0026rsquo;re useful.\nLayer 2: Reflective Memory Episodic memory alone would essentially be just a fancy search engine. Something that humans do that enables more complex learning is reflect on past episodes and draw generalized conclusions based on them. Taking inspiration from this process, I built an agentic reflection process into KenLM.\nWhen I exit a chat session, KenLM runs a reflection loop. A KenLM \u0026ldquo;reflection agent\u0026rdquo; analyzes the session\u0026rsquo;s episodes, searches for related patterns in past conversations, and updates a profile of hypotheses and learning goals.\nHere\u0026rsquo;s what the reflection system actually does:\nReviews the session — What patterns appeared in my behavior? What did I ask about? How did I respond to KenLM\u0026rsquo;s answers?\nSearches for evidence — The reflection agent can call a search_episodes(query) tool to find supporting patterns across all past conversations. This prevents overfitting to a single session.\nUpdates hypotheses — For each insight, it calls upsert_hypothesis() with a confidence score. Crucially, confidence only increases when patterns repeat across multiple episodes.\nMaintains learning goals — If the model notices something it doesn\u0026rsquo;t understand about me yet, it creates a learning goal—a question to explore in future interactions.\nHere\u0026rsquo;s what my actual profile looks like after some use:\n{ \u0026#34;hypotheses\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_007\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;accuracy\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth is detail-oriented and expects accurate and precise information.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.6, \u0026#34;evidence_episode_ids\u0026#34;: [ 90, 91, 89, 92 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_008\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;transparency\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth values transparency and honesty in communication.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 89 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_009\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;task_compliance\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth expects tasks to be completed accurately and without unnecessary steps.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 154, 155, 156, 157, 158 ] } ], \u0026#34;learning_goals\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;learning_goal_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;How can we ensure that Kenneth\u0026#39;s instructions are followed without unnecessary repetition or recapitulation?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;closed\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-11-30T15:10:55.704097\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;lg_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;What are Kenneth\u0026#39;s specific expectations for the precision and detail in project updates?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-12-02T15:51:56.904564\u0026#34; } ] } The reflection prompt is designed to be conservative. I explicitly instruct the model to start hypotheses with low confidence (0.2–0.4) and only raise confidence when it sees repeated evidence. This should hopefully prevent the kind of overconfident conclusions that can come from a single unusual interaction.\nLayer 3: The Forgetting System A pretty important piece of learning is actually the process by which we decide which pieces of information aren\u0026rsquo;t relevant. If we were to allow our episodic vector database to grow unbounded with every user interaction, it would quickly become slow and stop being useful during inference or reflection. Moreover, humans also don\u0026rsquo;t retain every bit of information that we are exposed to—we have complex systems that decide what information is worth retaining based on various factors.\nMy algorithm is a bit more rudimentary than the process that the human brain has, but it seems to be working fine so far. It essentially calculates an \u0026ldquo;importance\u0026rdquo; factor for every memory, which is just a weighted sum of a couple factors:\nimportance = w1 * log(1 + access_count) # Access frequency + w2 * is_protected # Protection bonus + w3 * is_evidence # Evidence bonus + w4 * exp(-age_days / tau) # Recency decay This system assigns a high score to frequently accessed memories, memories that the AI has decided it wants to \u0026ldquo;protect,\u0026rdquo; memories that are evidence for hypotheses that it has reasoned about, and memories that are recent. Memories that are old, infrequently accessed, and not useful for any of the learning goals or hypotheses the AI has created become candidates for forgetting.\nThe forgetting process is actually better understood as a summarization process. Instead of just deleting irrelevant memories, the system compacts them. It does this by summarizing \u0026ldquo;forgotten\u0026rdquo; episodes into condensed summaries and deleting the originals. This preserves knowledge while bounding storage growth—similar to how human memories become \u0026ldquo;fuzzy\u0026rdquo; over time.\nHow It Works End-to-End Let me walk through a complete cycle to show how these pieces fit together.\nStep 1: Message Received I type something like: \u0026ldquo;Can you write me a short essay about my favorite sports team?\u0026rdquo;\nStep 2: RAG Retrieval KenLM embeds my query and searches the episodic index. It finds 3 relevant past conversations about sports and essay preferences.\nStep 3: Context Assembly The system prompt is built from:\nCore identity document (who KenLM is, its purpose) Current profile (hypotheses + learning goals) RAG episodes (the relevant past conversations) Behavioral instructions (meta-prompt generated at session start) Step 4: Deliberation KenLM plans its approach: \u0026ldquo;Kenneth wants an essay about his favorite sports team. Based on my hypothesis about preferring conciseness, I should provide a short essay without excessive explanation. Based on our past conversations, his favorite sports team is the Seattle Sounders. I don\u0026rsquo;t need any external tools for this.\u0026rdquo;\nStep 5: Response Generation KenLM generates a response, informed by the plan and all the context. The response is concise and focused—because that\u0026rsquo;s what it\u0026rsquo;s learned that I prefer.\nStep 6: Episode Logging The conversation is saved as a new episode and indexed in the vector store.\nStep 7: Session End → Reflection When I exit the chat, the reflection agent wakes up. It reviews the session, searches for patterns, and decides whether to update any hypotheses or learning goals.\nStep 8: Next Session → Meta-Prompting The next time I start a chat, KenLM reads the (potentially updated) profile and generates fresh behavioral instructions. Then, the cycle continues.\nResults and Observations After using this system for a little bit, here are some of the results and observations I\u0026rsquo;ve observed:\nWhat\u0026rsquo;s working The conciseness hypothesis stuck. Early on, I found myself repeatedly asking KenLM to \u0026ldquo;be more concise\u0026rdquo; and to stop repeating itself. The reflection system noticed this pattern, formed a hypothesis, and now the model defaults to shorter responses. I rarely have to give feedback about the length of its responses anymore.\nRAG retrieval is surprisingly useful. When I ask about something I discussed in a past conversation, relevant context often appears in the model\u0026rsquo;s awareness. It doesn\u0026rsquo;t always explicitly reference the past conversation, but you can tell it\u0026rsquo;s informed by it. I can refer to things like \u0026ldquo;my favorite sports team\u0026rdquo; or \u0026ldquo;where I live\u0026rdquo; and the model remembers what those things are as they relate to me.\nMeta-prompting effectively influences personality. Each session feels more like what I\u0026rsquo;m looking for from a conversation with an LLM. It\u0026rsquo;s dropped the overly sycophantic tendencies, phrases like \u0026ldquo;Certainly!\u0026rdquo; or \u0026ldquo;You\u0026rsquo;re absolutely right!\u0026rdquo;, and other LLM-isms that have annoyed me. The behavioral instructions provide a stable \u0026ldquo;personality\u0026rdquo; for that session, and that personality is consistently adapting to my preferences.\nWhat\u0026rsquo;s Challenging Certain habits are hard to break. The model really loves markdown. I mean, it really loves outputting markdown formatting. No matter how many times I tell it it\u0026rsquo;s a command line tool and markdown is not helpful, it still really loves outputting neat markdown responses. Now, it HAS gotten a little better after much feedback, but I anticipate its preference for markdown is baked into the model weights itself, and not so easy to override through prompting. This is one of the clearest examples I\u0026rsquo;ve found so far of the limitations of this learning system.\nThe 7B model has limitations. My base model is a fine-tuned Qwen 2.5 7B. It\u0026rsquo;s good, but it occasionally misunderstands, hallucinates, or generates malformed tool calls. A larger model would likely perform better, but I wanted something that runs locally on my MacBook.\nThis process adds significant latency to the chat process. The RAG retrieval, meta-prompting, and reflection processes all take time. Using a model that\u0026rsquo;s running on my laptop means it can feel painfully slow at times.\nWhat this means This was a fun experiment, but what does this mean? Is there any use for systems like these in industry?\nI\u0026rsquo;m far from an AI expert, but off the top of my head I can think of a couple potential applications of systems like these to develop real AI systems that solve real-world problems. Giving an LLM the ability to learn from experience opens the door to all kinds of systems we don’t really have today.\nThe obvious application is personalization: assistants that remember your preferences and workflows, which is what KenLM is. However, I think the implications go beyond “your AI knows your favorite sports team.” Imagine an educational model that can learn how you learn and what you know, remember your strong and weak areas, and adapt its explanations to your thinking style. This model could act as a more helpful learning companion than a model with no such memory capabilities.\nAnother application is models that improve themselves by optimizing for factors outside of human interaction. Imagine a model that initially struggles with chain-of-thought reasoning or tool usage—say something that consistently runs into the same error when trying to write python code that runs in a sandbox. Currently, AI systems have no way to notice these things and improve. Human engineers need to monitor and step in, adjusting prompts, doing reinforcement learning, or collecting data with which to fine-tune in order to eke out a little bit better performance.\nA memory-equipped model changes this dynamic. For anything it\u0026rsquo;s trying to optimize for, be that reasoning or tool use, it can:\nnotice failed patterns by reasoning about them and remembering past experiences see successful patterns internalize the better method and adjust for future tasks Instead of updating billions of weights, the model updates a small set of learned strategies derived from lived experience. Over time, the model becomes a better problem solver not because it grew larger, but because it learned.\nThis all can potentially allow for smaller, more efficient models to rapidly adapt to specific use cases, reducing the industry\u0026rsquo;s reliance on frontier, compute-heavy models and allowing for more innovation and experimentation at the edge. Instead of treating model improvement as something that can only happen upstream, by massive labs with huge quantities of (often stolen) data, you get a world where models improve downstream, close to the user, close to the task, and close to the environment they operate in.\nIn that world, you don’t need a 400B-parameter model for everything. You need a reasonably capable base model with the right scaffolding: a memory system, a reflection system, and a mechanism for gradually internalizing the lessons it learns. Given enough experience, my intuition is that a modest model can outperform a much larger one on the tasks it specializes for, because it has context, history, and self-derived strategies the bigger model simply doesn’t have.\nIn the end, this really was just a weekend experiment, nothing more than an excuse to poke at a problem that’s been stuck in my head. But now that it’s working, even in this early form, I think there are probably real ideas worth exploring here. I don’t know exactly where it will go yet, but I’m excited to keep experimenting and see how far a small model can go when you let it learn from its own experiences.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e\n\u003cp\u003eHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As \u003ca href=\"https://abehrouz.github.io/files/NL.pdf\"\u003ethis paper\u003c/a\u003e from Google points out, LLMs lack \u003cstrong\u003eneuroplasticity\u003c/strong\u003e—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes \u003cem\u003efrozen\u003c/em\u003e. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes we\u0026rsquo;re trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights). All the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions). Each meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals). A distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass passes information between these components, using its working memory (context) as the vehicle. Let me break down how it works piece by piece.\nLayer 1: Episodic Memory This piece of the architecture is the most straightforward. To enable the model to remember specific experiences, every interaction is logged as an episode. Every episode captures the user prompt, agent response, and some metadata for memory management. It looks something like this:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-01T14:30:00\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;Can you help me write a README for this project?\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;Sure. Here\u0026#39;s a concise template...\u0026#34;, \u0026#34;access_count\u0026#34;: 3, \u0026#34;last_accessed\u0026#34;: \u0026#34;2025-12-03T10:15:00\u0026#34;, \u0026#34;is_protected\u0026#34;: false } To enable the model to retrieve relevant episodes at inference time, I utilize a technique known as retrieval-augmented generation (RAG). Simply put, I store the episode data in a vector store called ChromaDB. Embeddings are generated by sentence-transformers after every conversation. Every time I send a message, KenLM embeds my query and searches for the top-k most semantically similar episodes in its memory. This allows it to reference episodes related to my query, so if I ask about sports, it will pull up discussions where we talked about sports. If I ask about documentation, it will remember episodes where we talked about writing documentation.\nThis gives the model something resembling episodic recall: the ability to remember relevant experiences and bring them into working memory (the context window) when they\u0026rsquo;re useful.\nLayer 2: Reflective Memory Episodic memory alone would essentially be just a fancy search engine. Something that humans do that enables more complex learning is reflect on past episodes and draw generalized conclusions based on them. Taking inspiration from this process, I built an agentic reflection process into KenLM.\nWhen I exit a chat session, KenLM runs a reflection loop. A KenLM \u0026ldquo;reflection agent\u0026rdquo; analyzes the session\u0026rsquo;s episodes, searches for related patterns in past conversations, and updates a profile of hypotheses and learning goals.\nHere\u0026rsquo;s what the reflection system actually does:\nReviews the session — What patterns appeared in my behavior? What did I ask about? How did I respond to KenLM\u0026rsquo;s answers?\nSearches for evidence — The reflection agent can call a search_episodes(query) tool to find supporting patterns across all past conversations. This prevents overfitting to a single session.\nUpdates hypotheses — For each insight, it calls upsert_hypothesis() with a confidence score. Crucially, confidence only increases when patterns repeat across multiple episodes.\nMaintains learning goals — If the model notices something it doesn\u0026rsquo;t understand about me yet, it creates a learning goal—a question to explore in future interactions.\nHere\u0026rsquo;s what my actual profile looks like after some use:\n{ \u0026#34;hypotheses\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_007\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;accuracy\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth is detail-oriented and expects accurate and precise information.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.6, \u0026#34;evidence_episode_ids\u0026#34;: [ 90, 91, 89, 92 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_008\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;transparency\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth values transparency and honesty in communication.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 89 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_009\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;task_compliance\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth expects tasks to be completed accurately and without unnecessary steps.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 154, 155, 156, 157, 158 ] } ], \u0026#34;learning_goals\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;learning_goal_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;How can we ensure that Kenneth\u0026#39;s instructions are followed without unnecessary repetition or recapitulation?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;closed\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-11-30T15:10:55.704097\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;lg_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;What are Kenneth\u0026#39;s specific expectations for the precision and detail in project updates?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-12-02T15:51:56.904564\u0026#34; } ] } The reflection prompt is designed to be conservative. I explicitly instruct the model to start hypotheses with low confidence (0.2–0.4) and only raise confidence when it sees repeated evidence. This should hopefully prevent the kind of overconfident conclusions that can come from a single unusual interaction.\nLayer 3: The Forgetting System A pretty important piece of learning is actually the process by which we decide which pieces of information aren\u0026rsquo;t relevant. If we were to allow our episodic vector database to grow unbounded with every user interaction, it would quickly become slow and stop being useful during inference or reflection. Moreover, humans also don\u0026rsquo;t retain every bit of information that we are exposed to—we have complex systems that decide what information is worth retaining based on various factors.\nMy algorithm is a bit more rudimentary than the process that the human brain has, but it seems to be working fine so far. It essentially calculates an \u0026ldquo;importance\u0026rdquo; factor for every memory, which is just a weighted sum of a couple factors:\nimportance = w1 * log(1 + access_count) # Access frequency + w2 * is_protected # Protection bonus + w3 * is_evidence # Evidence bonus + w4 * exp(-age_days / tau) # Recency decay This system assigns a high score to frequently accessed memories, memories that the AI has decided it wants to \u0026ldquo;protect,\u0026rdquo; memories that are evidence for hypotheses that it has reasoned about, and memories that are recent. Memories that are old, infrequently accessed, and not useful for any of the learning goals or hypotheses the AI has created become candidates for forgetting.\nThe forgetting process is actually better understood as a summarization process. Instead of just deleting irrelevant memories, the system compacts them. It does this by summarizing \u0026ldquo;forgotten\u0026rdquo; episodes into condensed summaries and deleting the originals. This preserves knowledge while bounding storage growth—similar to how human memories become \u0026ldquo;fuzzy\u0026rdquo; over time.\nHow It Works End-to-End Let me walk through a complete cycle to show how these pieces fit together.\nStep 1: Message Received I type something like: \u0026ldquo;Can you write me a short essay about my favorite sports team?\u0026rdquo;\nStep 2: RAG Retrieval KenLM embeds my query and searches the episodic index. It finds 3 relevant past conversations about sports and essay preferences.\nStep 3: Context Assembly The system prompt is built from:\nCore identity document (who KenLM is, its purpose) Current profile (hypotheses + learning goals) RAG episodes (the relevant past conversations) Behavioral instructions (meta-prompt generated at session start) Step 4: Deliberation KenLM plans its approach: \u0026ldquo;Kenneth wants an essay about his favorite sports team. Based on my hypothesis about preferring conciseness, I should provide a short essay without excessive explanation. Based on our past conversations, his favorite sports team is the Seattle Sounders. I don\u0026rsquo;t need any external tools for this.\u0026rdquo;\nStep 5: Response Generation KenLM generates a response, informed by the plan and all the context. The response is concise and focused—because that\u0026rsquo;s what it\u0026rsquo;s learned that I prefer.\nStep 6: Episode Logging The conversation is saved as a new episode and indexed in the vector store.\nStep 7: Session End → Reflection When I exit the chat, the reflection agent wakes up. It reviews the session, searches for patterns, and decides whether to update any hypotheses or learning goals.\nStep 8: Next Session → Meta-Prompting The next time I start a chat, KenLM reads the (potentially updated) profile and generates fresh behavioral instructions. Then, the cycle continues.\nResults and Observations After using this system for a little bit, here are some of the results and observations I\u0026rsquo;ve observed:\nWhat\u0026rsquo;s working The conciseness hypothesis stuck. Early on, I found myself repeatedly asking KenLM to \u0026ldquo;be more concise\u0026rdquo; and to stop repeating itself. The reflection system noticed this pattern, formed a hypothesis, and now the model defaults to shorter responses. I rarely have to give feedback about the length of its responses anymore.\nRAG retrieval is surprisingly useful. When I ask about something I discussed in a past conversation, relevant context often appears in the model\u0026rsquo;s awareness. It doesn\u0026rsquo;t always explicitly reference the past conversation, but you can tell it\u0026rsquo;s informed by it. I can refer to things like \u0026ldquo;my favorite sports team\u0026rdquo; or \u0026ldquo;where I live\u0026rdquo; and the model remembers what those things are as they relate to me.\nMeta-prompting effectively influences personality. Each session feels more like what I\u0026rsquo;m looking for from a conversation with an LLM. It\u0026rsquo;s dropped the overly sycophantic tendencies, phrases like \u0026ldquo;Certainly!\u0026rdquo; or \u0026ldquo;You\u0026rsquo;re absolutely right!\u0026rdquo;, and other LLM-isms that have annoyed me. The behavioral instructions provide a stable \u0026ldquo;personality\u0026rdquo; for that session, and that personality is consistently adapting to my preferences.\nWhat\u0026rsquo;s Challenging Certain habits are hard to break. The model really loves markdown. I mean, it really loves outputting markdown formatting. No matter how many times I tell it it\u0026rsquo;s a command line tool and markdown is not helpful, it still really loves outputting neat markdown responses. Now, it HAS gotten a little better after much feedback, but I anticipate its preference for markdown is baked into the model weights itself, and not so easy to override through prompting. This is one of the clearest examples I\u0026rsquo;ve found so far of the limitations of this learning system.\nThe 7B model has limitations. My base model is a fine-tuned Qwen 2.5 7B. It\u0026rsquo;s good, but it occasionally misunderstands, hallucinates, or generates malformed tool calls. A larger model would likely perform better, but I wanted something that runs locally on my MacBook.\nThis process adds significant latency to the chat process. The RAG retrieval, meta-prompting, and reflection processes all take time. Using a model that\u0026rsquo;s running on my laptop means it can feel painfully slow at times.\nWhat this means This was a fun experiment, but what does this mean? Is there any use for systems like these in industry?\nI\u0026rsquo;m far from an AI expert, but off the top of my head I can think of a couple potential applications of systems like these to develop real AI systems that solve real-world problems. Giving an LLM the ability to learn from experience opens the door to all kinds of systems we don’t really have today.\nThe obvious application is personalization: assistants that remember your preferences and workflows, which is what KenLM is. However, I think the implications go beyond “your AI knows your favorite sports team.” For example, imagine an educational model that can learn how you learn and what you know, remember your strong and weak areas, and adapt its explanations to your thinking style. This model could act as a more helpful learning companion than a model with no such memory capabilities.\nAnother application is models that improve themselves by optimizing for factors outside of human interaction. Imagine a model that initially struggles with chain-of-thought reasoning or tool usage—say something that consistently runs into the same error when trying to write python code that runs in a sandbox. Currently, AI systems have no way to notice these things and improve. Human engineers need to monitor and step in, adjusting prompts, doing reinforcement learning, or collecting data with which to fine-tune in order to eke out a little bit better performance.\nA memory-equipped model changes this dynamic. For anything it\u0026rsquo;s trying to optimize for, be that reasoning or tool use, it can:\nnotice failed patterns by reasoning about them and remembering past experiences see successful patterns internalize the better method and adjust for future tasks Instead of updating billions of weights, the model updates a small set of learned strategies derived from lived experience. Over time, the model becomes a better problem solver not because it grew larger, but because it learned.\nThis all can potentially allow for smaller, more efficient models to rapidly adapt to specific use cases, reducing the industry\u0026rsquo;s reliance on frontier, compute-heavy models and allowing for more innovation and experimentation at the edge. Instead of treating model improvement as something that can only happen upstream, by massive labs with huge quantities of (often stolen) data, you get a world where models improve downstream, close to the user, close to the task, and close to the environment they operate in.\nIn that world, you don’t need a 400B-parameter model for everything. You need a reasonably capable base model with the right scaffolding: a memory system, a reflection system, and a mechanism for gradually internalizing the lessons it learns. Given enough experience, my intuition is that a modest model can outperform a much larger one on the tasks it specializes for, because it has context, history, and self-derived strategies the bigger model simply doesn’t have.\nIn the end, this really was just a weekend experiment, nothing more than an excuse to poke at a problem that’s been stuck in my head. But now that it’s working, even in this early form, I think there are probably real ideas worth exploring here. I don’t know exactly where it will go yet, but I’m excited to keep experimenting and see how far a small model can go when you let it learn from its own experiences.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e\n\u003cp\u003eHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As \u003ca href=\"https://abehrouz.github.io/files/NL.pdf\"\u003ethis paper\u003c/a\u003e from Google points out, LLMs lack \u003cstrong\u003eneuroplasticity\u003c/strong\u003e—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes \u003cem\u003efrozen\u003c/em\u003e. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes we\u0026rsquo;re trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights). All the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions). Each meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals). A distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass passes information between these components, using its working memory (context) as the vehicle. Let me break down how it works piece by piece.\nLayer 1: Episodic Memory This piece of the architecture is the most straightforward. To enable the model to remember specific experiences, every interaction is logged as an episode. Every episode captures the user prompt, agent response, and some metadata for memory management. It looks something like this:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-01T14:30:00\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;Can you help me write a README for this project?\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;Sure. Here\u0026#39;s a concise template...\u0026#34;, \u0026#34;access_count\u0026#34;: 3, \u0026#34;last_accessed\u0026#34;: \u0026#34;2025-12-03T10:15:00\u0026#34;, \u0026#34;is_protected\u0026#34;: false } To enable the model to retrieve relevant episodes at inference time, I utilize a technique known as retrieval-augmented generation (RAG). Simply put, I store the episode data in a vector store called ChromaDB. Embeddings are generated by sentence-transformers after every conversation. Every time I send a message, KenLM embeds my query and searches for the top-k most semantically similar episodes in its memory. This allows it to reference episodes related to my query, so if I ask about sports, it will pull up discussions where we talked about sports. If I ask about documentation, it will remember episodes where we talked about writing documentation.\nThis gives the model something resembling episodic recall: the ability to remember relevant experiences and bring them into working memory (the context window) when they\u0026rsquo;re useful.\nLayer 2: Reflective Memory Episodic memory alone would essentially be just a fancy search engine. Something that humans do that enables more complex learning is reflect on past episodes and draw generalized conclusions based on them. Taking inspiration from this process, I built an agentic reflection process into KenLM.\nWhen I exit a chat session, KenLM runs a reflection loop. A KenLM \u0026ldquo;reflection agent\u0026rdquo; analyzes the session\u0026rsquo;s episodes, searches for related patterns in past conversations, and updates a profile of hypotheses and learning goals.\nHere\u0026rsquo;s what the reflection system actually does:\nReviews the session — What patterns appeared in my behavior? What did I ask about? How did I respond to KenLM\u0026rsquo;s answers?\nSearches for evidence — The reflection agent can call a search_episodes(query) tool to find supporting patterns across all past conversations. This prevents overfitting to a single session.\nUpdates hypotheses — For each insight, it calls upsert_hypothesis() with a confidence score. Crucially, confidence only increases when patterns repeat across multiple episodes.\nMaintains learning goals — If the model notices something it doesn\u0026rsquo;t understand about me yet, it creates a learning goal—a question to explore in future interactions.\nHere\u0026rsquo;s what my actual profile looks like after some use:\n{ \u0026#34;hypotheses\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_007\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;accuracy\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth is detail-oriented and expects accurate and precise information.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.6, \u0026#34;evidence_episode_ids\u0026#34;: [ 90, 91, 89, 92 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_008\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;transparency\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth values transparency and honesty in communication.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 89 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_009\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;task_compliance\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth expects tasks to be completed accurately and without unnecessary steps.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 154, 155, 156, 157, 158 ] } ], \u0026#34;learning_goals\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;learning_goal_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;How can we ensure that Kenneth\u0026#39;s instructions are followed without unnecessary repetition or recapitulation?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;closed\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-11-30T15:10:55.704097\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;lg_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;What are Kenneth\u0026#39;s specific expectations for the precision and detail in project updates?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-12-02T15:51:56.904564\u0026#34; } ] } The reflection prompt is designed to be conservative. I explicitly instruct the model to start hypotheses with low confidence (0.2–0.4) and only raise confidence when it sees repeated evidence. This should hopefully prevent the kind of overconfident conclusions that can come from a single unusual interaction.\nLayer 3: The Forgetting System A pretty important piece of learning is actually the process by which we decide which pieces of information aren\u0026rsquo;t relevant. If we were to allow our episodic vector database to grow unbounded with every user interaction, it would quickly become slow and stop being useful during inference or reflection. Moreover, humans also don\u0026rsquo;t retain every bit of information that we are exposed to—we have complex systems that decide what information is worth retaining based on various factors.\nMy algorithm is a bit more rudimentary than the process that the human brain has, but it seems to be working fine so far. It essentially calculates an \u0026ldquo;importance\u0026rdquo; factor for every memory, which is just a weighted sum of a couple factors:\nimportance = w1 * log(1 + access_count) # Access frequency + w2 * is_protected # Protection bonus + w3 * is_evidence # Evidence bonus + w4 * exp(-age_days / tau) # Recency decay This system assigns a high score to frequently accessed memories, memories that the AI has decided it wants to \u0026ldquo;protect,\u0026rdquo; memories that are evidence for hypotheses that it has reasoned about, and memories that are recent. Memories that are old, infrequently accessed, and not useful for any of the learning goals or hypotheses the AI has created become candidates for forgetting.\nThe forgetting process is actually better understood as a summarization process. Instead of just deleting irrelevant memories, the system compacts them. It does this by summarizing \u0026ldquo;forgotten\u0026rdquo; episodes into condensed summaries and deleting the originals. This preserves knowledge while bounding storage growth—similar to how human memories become \u0026ldquo;fuzzy\u0026rdquo; over time.\nHow It Works End-to-End Let me walk through a complete cycle to show how these pieces fit together.\nStep 1: Message Received I type something like: \u0026ldquo;Can you write me a short essay about my favorite sports team?\u0026rdquo;\nStep 2: RAG Retrieval KenLM embeds my query and searches the episodic index. It finds 3 relevant past conversations about sports and essay preferences.\nStep 3: Context Assembly The system prompt is built from:\nCore identity document (who KenLM is, its purpose) Current profile (hypotheses + learning goals) RAG episodes (the relevant past conversations) Behavioral instructions (meta-prompt generated at session start) Step 4: Deliberation KenLM plans its approach: \u0026ldquo;Kenneth wants an essay about his favorite sports team. Based on my hypothesis about preferring conciseness, I should provide a short essay without excessive explanation. Based on our past conversations, his favorite sports team is the Seattle Sounders. I don\u0026rsquo;t need any external tools for this.\u0026rdquo;\nStep 5: Response Generation KenLM generates a response, informed by the plan and all the context. The response is concise and focused—because that\u0026rsquo;s what it\u0026rsquo;s learned that I prefer.\nStep 6: Episode Logging The conversation is saved as a new episode and indexed in the vector store.\nStep 7: Session End → Reflection When I exit the chat, the reflection agent wakes up. It reviews the session, searches for patterns, and decides whether to update any hypotheses or learning goals.\nStep 8: Next Session → Meta-Prompting The next time I start a chat, KenLM reads the (potentially updated) profile and generates fresh behavioral instructions. Then, the cycle continues.\nResults and Observations After using this system for a little bit, here are some of the results and observations I\u0026rsquo;ve observed:\nWhat\u0026rsquo;s working The conciseness hypothesis stuck. Early on, I found myself repeatedly asking KenLM to \u0026ldquo;be more concise\u0026rdquo; and to stop repeating itself. The reflection system noticed this pattern, formed a hypothesis, and now the model defaults to shorter responses. I rarely have to give feedback about the length of its responses anymore.\nRAG retrieval is surprisingly useful. When I ask about something I discussed in a past conversation, relevant context often appears in the model\u0026rsquo;s awareness. It doesn\u0026rsquo;t always explicitly reference the past conversation, but you can tell it\u0026rsquo;s informed by it. I can refer to things like \u0026ldquo;my favorite sports team\u0026rdquo; or \u0026ldquo;where I live\u0026rdquo; and the model remembers what those things are as they relate to me.\nMeta-prompting effectively influences personality. Each session feels more like what I\u0026rsquo;m looking for from a conversation with an LLM. It\u0026rsquo;s dropped the overly sycophantic tendencies, phrases like \u0026ldquo;Certainly!\u0026rdquo; or \u0026ldquo;You\u0026rsquo;re absolutely right!\u0026rdquo;, and other LLM-isms that have annoyed me. The behavioral instructions provide a stable \u0026ldquo;personality\u0026rdquo; for that session, and that personality is consistently adapting to my preferences.\nWhat\u0026rsquo;s Challenging Certain habits are hard to break. The model really loves markdown. I mean, it really loves outputting markdown formatting. No matter how many times I tell it it\u0026rsquo;s a command line tool and markdown is not helpful, it still really loves outputting neat markdown responses. Now, it HAS gotten a little better after much feedback, but I anticipate its preference for markdown is baked into the model weights itself, and not so easy to override through prompting. This is one of the clearest examples I\u0026rsquo;ve found so far of the limitations of this learning system.\nThe 7B model has limitations. My base model is a fine-tuned Qwen 2.5 7B. It\u0026rsquo;s good, but it occasionally misunderstands, hallucinates, or generates malformed tool calls. A larger model would likely perform better, but I wanted something that runs locally on my MacBook.\nThis process adds significant latency to the chat process. The RAG retrieval, meta-prompting, and reflection processes all take time. Using a model that\u0026rsquo;s running on my laptop means it can feel painfully slow at times.\nWhat this means This was a fun experiment, but what does this mean? Is there any use for systems like these in industry?\nI\u0026rsquo;m far from an AI expert, but off the top of my head I can think of a couple potential applications of systems like these to develop real AI systems that solve real-world problems. Giving an LLM the ability to learn from experience opens the door to all kinds of systems we don’t really have today.\nThe obvious application is personalization: assistants that remember your preferences and workflows, which is what KenLM is. However, I think the implications go beyond “your AI knows your favorite sports team.” For example, imagine an educational model that can learn how you learn and what you know, remember your strong and weak areas, and adapt its explanations to your thinking style. This model could act as a more helpful learning companion than a model with no such memory capabilities.\nAnother application I can think of is creating models that improve themselves by optimizing for factors outside of human interaction. Imagine a model that initially struggles with chain-of-thought reasoning or tool usage—say something that consistently runs into the same error when trying to write python code that runs in a sandbox. Currently, AI systems have no way to notice these things and improve. Human engineers need to monitor and step in, adjusting prompts, doing reinforcement learning, or collecting data with which to fine-tune in order to eke out a little bit better performance.\nA memory-equipped model changes this dynamic. For anything it\u0026rsquo;s trying to optimize for, be that reasoning or tool use, it can:\nnotice failed patterns by reasoning about them and remembering past experiences see successful patterns internalize the better method and adjust for future tasks Instead of updating billions of weights, the model updates a small set of learned strategies derived from lived experience. Over time, the model becomes a better problem solver not because it grew larger, but because it learned.\nThis all can potentially allow for smaller, more efficient models to rapidly adapt to specific use cases, reducing the industry\u0026rsquo;s reliance on frontier, compute-heavy models and allowing for more innovation and experimentation at the edge. Instead of treating model improvement as something that can only happen upstream, by massive labs with huge quantities of (often stolen) data, you get a world where models improve downstream, close to the user, close to the task, and close to the environment they operate in.\nIn that world, you don’t need a 400B-parameter model for everything. You need a reasonably capable base model with the right scaffolding: a memory system, a reflection system, and a mechanism for gradually internalizing the lessons it learns. Given enough experience, my intuition is that a modest model can outperform a much larger one on the tasks it specializes for, because it has context, history, and self-derived strategies the bigger model simply doesn’t have.\nIn the end, this really was just a weekend experiment, nothing more than an excuse to poke at a problem that’s been stuck in my head. But now that it’s working, even in this early form, I think there are probably real ideas worth exploring here. I don’t know exactly where it will go yet, but I’m excited to keep experimenting and see how far a small model can go when you let it learn from its own experiences.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e\n\u003cp\u003eHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As \u003ca href=\"https://abehrouz.github.io/files/NL.pdf\"\u003ethis paper\u003c/a\u003e from Google points out, LLMs lack \u003cstrong\u003eneuroplasticity\u003c/strong\u003e—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes \u003cem\u003efrozen\u003c/em\u003e. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes we\u0026rsquo;re trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights). All the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions). Each meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals). A distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass passes information between these components, using its working memory (context) as the vehicle. Let me break down how it works piece by piece.\nLayer 1: Episodic Memory This piece of the architecture is the most straightforward. To enable the model to remember specific experiences, every interaction is logged as an episode. Every episode captures the user prompt, agent response, and some metadata for memory management. It looks something like this:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-01T14:30:00\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;Can you help me write a README for this project?\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;Sure. Here\u0026#39;s a concise template...\u0026#34;, \u0026#34;access_count\u0026#34;: 3, \u0026#34;last_accessed\u0026#34;: \u0026#34;2025-12-03T10:15:00\u0026#34;, \u0026#34;is_protected\u0026#34;: false } To enable the model to retrieve relevant episodes at inference time, I utilize a technique known as retrieval-augmented generation (RAG). Simply put, I store the episode data in a vector store called ChromaDB. Embeddings are generated by sentence-transformers after every conversation. Every time I send a message, KenLM embeds my query and searches for the top-k most semantically similar episodes in its memory. This allows it to reference episodes related to my query, so if I ask about sports, it will pull up discussions where we talked about sports. If I ask about documentation, it will remember episodes where we talked about writing documentation.\nThis gives the model something resembling episodic recall: the ability to remember relevant experiences and bring them into working memory (the context window) when they\u0026rsquo;re useful.\nLayer 2: Reflective Memory Episodic memory alone would essentially be just a fancy search engine. Something that humans do that enables more complex learning is reflect on past episodes and draw generalized conclusions based on them. Taking inspiration from this process, I built an agentic reflection process into KenLM.\nWhen I exit a chat session, KenLM runs a reflection loop. A KenLM \u0026ldquo;reflection agent\u0026rdquo; analyzes the session\u0026rsquo;s episodes, searches for related patterns in past conversations, and updates a profile of hypotheses and learning goals.\nHere\u0026rsquo;s what the reflection system actually does:\nReviews the session — What patterns appeared in my behavior? What did I ask about? How did I respond to KenLM\u0026rsquo;s answers?\nSearches for evidence — The reflection agent can call a search_episodes(query) tool to find supporting patterns across all past conversations. This prevents overfitting to a single session.\nUpdates hypotheses — For each insight, it calls upsert_hypothesis() with a confidence score. Crucially, confidence only increases when patterns repeat across multiple episodes.\nMaintains learning goals — If the model notices something it doesn\u0026rsquo;t understand about me yet, it creates a learning goal—a question to explore in future interactions.\nHere\u0026rsquo;s what my actual profile looks like after some use:\n{ \u0026#34;hypotheses\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_007\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;accuracy\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth is detail-oriented and expects accurate and precise information.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.6, \u0026#34;evidence_episode_ids\u0026#34;: [ 90, 91, 89, 92 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_008\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;transparency\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth values transparency and honesty in communication.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 89 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_009\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;task_compliance\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth expects tasks to be completed accurately and without unnecessary steps.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 154, 155, 156, 157, 158 ] } ], \u0026#34;learning_goals\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;learning_goal_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;How can we ensure that Kenneth\u0026#39;s instructions are followed without unnecessary repetition or recapitulation?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;closed\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-11-30T15:10:55.704097\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;lg_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;What are Kenneth\u0026#39;s specific expectations for the precision and detail in project updates?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-12-02T15:51:56.904564\u0026#34; } ] } The reflection prompt is designed to be conservative. I explicitly instruct the model to start hypotheses with low confidence (0.2–0.4) and only raise confidence when it sees repeated evidence. This should hopefully prevent the kind of overconfident conclusions that can come from a single unusual interaction.\nLayer 3: The Forgetting System A pretty important piece of learning is actually the process by which we decide which pieces of information aren\u0026rsquo;t relevant. If we were to allow our episodic vector database to grow unbounded with every user interaction, it would quickly become slow and stop being useful during inference or reflection. Moreover, humans also don\u0026rsquo;t retain every bit of information that we are exposed to—we have complex systems that decide what information is worth retaining based on various factors.\nMy algorithm is a bit more rudimentary than the process that the human brain has, but it seems to be working fine so far. It essentially calculates an \u0026ldquo;importance\u0026rdquo; factor for every memory, which is just a weighted sum of a couple factors:\nimportance = w1 * log(1 + access_count) # Access frequency + w2 * is_protected # Protection bonus + w3 * is_evidence # Evidence bonus + w4 * exp(-age_days / tau) # Recency decay This system assigns a high score to frequently accessed memories, memories that the AI has decided it wants to \u0026ldquo;protect,\u0026rdquo; memories that are evidence for hypotheses that it has reasoned about, and memories that are recent. Memories that are old, infrequently accessed, and not useful for any of the learning goals or hypotheses the AI has created become candidates for forgetting.\nThe forgetting process is actually better understood as a summarization process. Instead of just deleting irrelevant memories, the system compacts them. It does this by summarizing \u0026ldquo;forgotten\u0026rdquo; episodes into condensed summaries and deleting the originals. This preserves knowledge while bounding storage growth—similar to how human memories become \u0026ldquo;fuzzy\u0026rdquo; over time.\nHow It Works End-to-End Let me walk through a complete cycle to show how these pieces fit together.\nStep 1: Message Received I type something like: \u0026ldquo;Can you write me a short essay about my favorite sports team?\u0026rdquo;\nStep 2: RAG Retrieval KenLM embeds my query and searches the episodic index. It finds 3 relevant past conversations about sports and essay preferences.\nStep 3: Context Assembly The system prompt is built from:\nCore identity document (who KenLM is, its purpose) Current profile (hypotheses + learning goals) RAG episodes (the relevant past conversations) Behavioral instructions (meta-prompt generated at session start) Step 4: Deliberation KenLM plans its approach: \u0026ldquo;Kenneth wants an essay about his favorite sports team. Based on my hypothesis about preferring conciseness, I should provide a short essay without excessive explanation. Based on our past conversations, his favorite sports team is the Seattle Sounders. I don\u0026rsquo;t need any external tools for this.\u0026rdquo;\nStep 5: Response Generation KenLM generates a response, informed by the plan and all the context. The response is concise and focused—because that\u0026rsquo;s what it\u0026rsquo;s learned that I prefer.\nStep 6: Episode Logging The conversation is saved as a new episode and indexed in the vector store.\nStep 7: Session End → Reflection When I exit the chat, the reflection agent wakes up. It reviews the session, searches for patterns, and decides whether to update any hypotheses or learning goals.\nStep 8: Next Session → Meta-Prompting The next time I start a chat, KenLM reads the (potentially updated) profile and generates fresh behavioral instructions. Then, the cycle continues.\nResults and Observations After using this system for a little bit, here are some of the results and observations I\u0026rsquo;ve observed:\nWhat\u0026rsquo;s working The conciseness hypothesis stuck. Early on, I found myself repeatedly asking KenLM to \u0026ldquo;be more concise\u0026rdquo; and to stop repeating itself. The reflection system noticed this pattern, formed a hypothesis, and now the model defaults to shorter responses. I rarely have to give feedback about the length of its responses anymore.\nRAG retrieval is surprisingly useful. When I ask about something I discussed in a past conversation, relevant context often appears in the model\u0026rsquo;s awareness. It doesn\u0026rsquo;t always explicitly reference the past conversation, but you can tell it\u0026rsquo;s informed by it. I can refer to things like \u0026ldquo;my favorite sports team\u0026rdquo; or \u0026ldquo;where I live\u0026rdquo; and the model remembers what those things are as they relate to me.\nMeta-prompting effectively influences personality. Each session feels more like what I\u0026rsquo;m looking for from a conversation with an LLM. It\u0026rsquo;s dropped the overly sycophantic tendencies, phrases like \u0026ldquo;Certainly!\u0026rdquo; or \u0026ldquo;You\u0026rsquo;re absolutely right!\u0026rdquo;, and other LLM-isms that have annoyed me. The behavioral instructions provide a stable \u0026ldquo;personality\u0026rdquo; for that session, and that personality is consistently adapting to my preferences.\nWhat\u0026rsquo;s Challenging Certain habits are hard to break. The model really loves markdown. I mean, it really loves outputting markdown formatting. No matter how many times I tell it it\u0026rsquo;s a command line tool and markdown is not helpful, it still really loves outputting neat markdown responses. Now, it HAS gotten a little better after much feedback, but I anticipate its preference for markdown is baked into the model weights itself, and not so easy to override through prompting. This is one of the clearest examples I\u0026rsquo;ve found so far of the limitations of this learning system.\nThe 7B model has limitations. My base model is a fine-tuned Qwen 2.5 7B. It\u0026rsquo;s good, but it occasionally misunderstands, hallucinates, or generates malformed tool calls. A larger model would likely perform better, but I wanted something that runs locally on my MacBook.\nThis process adds significant latency to the chat process. The RAG retrieval, meta-prompting, and reflection processes all take time. Using a model that\u0026rsquo;s running on my laptop means it can feel painfully slow at times.\nWhat this means This was a fun experiment, but what does this mean? Is there any use for systems like these in industry?\nI\u0026rsquo;m far from an AI expert, but off the top of my head I can think of a couple potential applications of systems like these to develop real AI systems that solve real-world problems. Giving an LLM the ability to learn from experience opens the door to all kinds of systems we don’t really have today.\nThe obvious application is personalization: assistants that remember your preferences and workflows, which is what KenLM is. However, I think the implications go beyond “your AI knows your favorite sports team.” For example, imagine an educational model that can learn how you learn and what you know, remember your strong and weak areas, and adapt its explanations to your thinking style. This model could act as a more helpful learning companion than a model with no such memory capabilities.\nAnother application I can think of is creating models that improve themselves by optimizing for factors outside of human interaction. Imagine a model that initially struggles with chain-of-thought reasoning or tool usage—say something that consistently runs into the same error when trying to write python code or execute a web search. Currently, AI systems have no way to notice these things and improve. Human engineers need to monitor and step in, adjusting prompts, doing reinforcement learning, or collecting data with which to fine-tune in order to eke out a little bit better performance.\nA memory-equipped model changes this dynamic. For anything it\u0026rsquo;s trying to optimize for, be that reasoning or tool use, it can:\nnotice failed patterns by reasoning about them and remembering past experiences see successful patterns internalize the better method and adjust for future tasks Instead of updating billions of weights, the model updates a small set of learned strategies derived from lived experience. Over time, the model becomes a better problem solver not because it grew larger, but because it learned.\nThis all can potentially allow for smaller, more efficient models to rapidly adapt to specific use cases, reducing the industry\u0026rsquo;s reliance on frontier, compute-heavy models and allowing for more innovation and experimentation at the edge. Instead of treating model improvement as something that can only happen upstream, by massive labs with huge quantities of (often stolen) data, you get a world where models improve downstream, close to the user, close to the task, and close to the environment they operate in.\nIn that world, you don’t need a 400B-parameter model for everything. You need a reasonably capable base model with the right scaffolding: a memory system, a reflection system, and a mechanism for gradually internalizing the lessons it learns. Given enough experience, my intuition is that a modest model can outperform a much larger one on the tasks it specializes for, because it has context, history, and self-derived strategies the bigger model simply doesn’t have.\nIn the end, this really was just a weekend experiment, nothing more than an excuse to poke at a problem that’s been stuck in my head. But now that it’s working, even in this early form, I think there are probably real ideas worth exploring here. I don’t know exactly where it will go yet, but I’m excited to keep experimenting and see how far a small model can go when you let it learn from its own experiences.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e\n\u003cp\u003eHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As \u003ca href=\"https://abehrouz.github.io/files/NL.pdf\"\u003ethis paper\u003c/a\u003e from Google points out, LLMs lack \u003cstrong\u003eneuroplasticity\u003c/strong\u003e—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes \u003cem\u003efrozen\u003c/em\u003e. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes we\u0026rsquo;re trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge between the two, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights). All the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions). Each meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals). A distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass passes information between these components, using its working memory (context) as the vehicle. Let me break down how it works piece by piece.\nLayer 1: Episodic Memory This piece of the architecture is the most straightforward. To enable the model to remember specific experiences, every interaction is logged as an episode. Every episode captures the user prompt, agent response, and some metadata for memory management. It looks something like this:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-01T14:30:00\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;Can you help me write a README for this project?\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;Sure. Here\u0026#39;s a concise template...\u0026#34;, \u0026#34;access_count\u0026#34;: 3, \u0026#34;last_accessed\u0026#34;: \u0026#34;2025-12-03T10:15:00\u0026#34;, \u0026#34;is_protected\u0026#34;: false } To enable the model to retrieve relevant episodes at inference time, I utilize a technique known as retrieval-augmented generation (RAG). Simply put, I store the episode data in a vector store called ChromaDB. Embeddings are generated by sentence-transformers after every conversation. Every time I send a message, KenLM embeds my query and searches for the top-k most semantically similar episodes in its memory. This allows it to reference episodes related to my query, so if I ask about sports, it will pull up discussions where we talked about sports. If I ask about documentation, it will remember episodes where we talked about writing documentation.\nThis gives the model something resembling episodic recall: the ability to remember relevant experiences and bring them into working memory (the context window) when they\u0026rsquo;re useful.\nLayer 2: Reflective Memory Episodic memory alone would essentially be just a fancy search engine. Something that humans do that enables more complex learning is reflect on past episodes and draw generalized conclusions based on them. Taking inspiration from this process, I built an agentic reflection process into KenLM.\nWhen I exit a chat session, KenLM runs a reflection loop. A KenLM \u0026ldquo;reflection agent\u0026rdquo; analyzes the session\u0026rsquo;s episodes, searches for related patterns in past conversations, and updates a profile of hypotheses and learning goals.\nHere\u0026rsquo;s what the reflection system actually does:\nReviews the session — What patterns appeared in my behavior? What did I ask about? How did I respond to KenLM\u0026rsquo;s answers?\nSearches for evidence — The reflection agent can call a search_episodes(query) tool to find supporting patterns across all past conversations. This prevents overfitting to a single session.\nUpdates hypotheses — For each insight, it calls upsert_hypothesis() with a confidence score. Crucially, confidence only increases when patterns repeat across multiple episodes.\nMaintains learning goals — If the model notices something it doesn\u0026rsquo;t understand about me yet, it creates a learning goal—a question to explore in future interactions.\nHere\u0026rsquo;s what my actual profile looks like after some use:\n{ \u0026#34;hypotheses\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_007\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;accuracy\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth is detail-oriented and expects accurate and precise information.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.6, \u0026#34;evidence_episode_ids\u0026#34;: [ 90, 91, 89, 92 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_008\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;transparency\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth values transparency and honesty in communication.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 89 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_009\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;task_compliance\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth expects tasks to be completed accurately and without unnecessary steps.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 154, 155, 156, 157, 158 ] } ], \u0026#34;learning_goals\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;learning_goal_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;How can we ensure that Kenneth\u0026#39;s instructions are followed without unnecessary repetition or recapitulation?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;closed\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-11-30T15:10:55.704097\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;lg_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;What are Kenneth\u0026#39;s specific expectations for the precision and detail in project updates?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-12-02T15:51:56.904564\u0026#34; } ] } The reflection prompt is designed to be conservative. I explicitly instruct the model to start hypotheses with low confidence (0.2–0.4) and only raise confidence when it sees repeated evidence. This should hopefully prevent the kind of overconfident conclusions that can come from a single unusual interaction.\nLayer 3: The Forgetting System A pretty important piece of learning is actually the process by which we decide which pieces of information aren\u0026rsquo;t relevant. If we were to allow our episodic vector database to grow unbounded with every user interaction, it would quickly become slow and stop being useful during inference or reflection. Moreover, humans also don\u0026rsquo;t retain every bit of information that we are exposed to—we have complex systems that decide what information is worth retaining based on various factors.\nMy algorithm is a bit more rudimentary than the process that the human brain has, but it seems to be working fine so far. It essentially calculates an \u0026ldquo;importance\u0026rdquo; factor for every memory, which is just a weighted sum of a couple factors:\nimportance = w1 * log(1 + access_count) # Access frequency + w2 * is_protected # Protection bonus + w3 * is_evidence # Evidence bonus + w4 * exp(-age_days / tau) # Recency decay This system assigns a high score to frequently accessed memories, memories that the AI has decided it wants to \u0026ldquo;protect,\u0026rdquo; memories that are evidence for hypotheses that it has reasoned about, and memories that are recent. Memories that are old, infrequently accessed, and not useful for any of the learning goals or hypotheses the AI has created become candidates for forgetting.\nThe forgetting process is actually better understood as a summarization process. Instead of just deleting irrelevant memories, the system compacts them. It does this by summarizing \u0026ldquo;forgotten\u0026rdquo; episodes into condensed summaries and deleting the originals. This preserves knowledge while bounding storage growth—similar to how human memories become \u0026ldquo;fuzzy\u0026rdquo; over time.\nHow It Works End-to-End Let me walk through a complete cycle to show how these pieces fit together.\nStep 1: Message Received I type something like: \u0026ldquo;Can you write me a short essay about my favorite sports team?\u0026rdquo;\nStep 2: RAG Retrieval KenLM embeds my query and searches the episodic index. It finds 3 relevant past conversations about sports and essay preferences.\nStep 3: Context Assembly The system prompt is built from:\nCore identity document (who KenLM is, its purpose) Current profile (hypotheses + learning goals) RAG episodes (the relevant past conversations) Behavioral instructions (meta-prompt generated at session start) Step 4: Deliberation KenLM plans its approach: \u0026ldquo;Kenneth wants an essay about his favorite sports team. Based on my hypothesis about preferring conciseness, I should provide a short essay without excessive explanation. Based on our past conversations, his favorite sports team is the Seattle Sounders. I don\u0026rsquo;t need any external tools for this.\u0026rdquo;\nStep 5: Response Generation KenLM generates a response, informed by the plan and all the context. The response is concise and focused—because that\u0026rsquo;s what it\u0026rsquo;s learned that I prefer.\nStep 6: Episode Logging The conversation is saved as a new episode and indexed in the vector store.\nStep 7: Session End → Reflection When I exit the chat, the reflection agent wakes up. It reviews the session, searches for patterns, and decides whether to update any hypotheses or learning goals.\nStep 8: Next Session → Meta-Prompting The next time I start a chat, KenLM reads the (potentially updated) profile and generates fresh behavioral instructions. Then, the cycle continues.\nResults and Observations After using this system for a little bit, here are some of the results and observations I\u0026rsquo;ve observed:\nWhat\u0026rsquo;s working The conciseness hypothesis stuck. Early on, I found myself repeatedly asking KenLM to \u0026ldquo;be more concise\u0026rdquo; and to stop repeating itself. The reflection system noticed this pattern, formed a hypothesis, and now the model defaults to shorter responses. I rarely have to give feedback about the length of its responses anymore.\nRAG retrieval is surprisingly useful. When I ask about something I discussed in a past conversation, relevant context often appears in the model\u0026rsquo;s awareness. It doesn\u0026rsquo;t always explicitly reference the past conversation, but you can tell it\u0026rsquo;s informed by it. I can refer to things like \u0026ldquo;my favorite sports team\u0026rdquo; or \u0026ldquo;where I live\u0026rdquo; and the model remembers what those things are as they relate to me.\nMeta-prompting effectively influences personality. Each session feels more like what I\u0026rsquo;m looking for from a conversation with an LLM. It\u0026rsquo;s dropped the overly sycophantic tendencies, phrases like \u0026ldquo;Certainly!\u0026rdquo; or \u0026ldquo;You\u0026rsquo;re absolutely right!\u0026rdquo;, and other LLM-isms that have annoyed me. The behavioral instructions provide a stable \u0026ldquo;personality\u0026rdquo; for that session, and that personality is consistently adapting to my preferences.\nWhat\u0026rsquo;s Challenging Certain habits are hard to break. The model really loves markdown. I mean, it really loves outputting markdown formatting. No matter how many times I tell it it\u0026rsquo;s a command line tool and markdown is not helpful, it still really loves outputting neat markdown responses. Now, it HAS gotten a little better after much feedback, but I anticipate its preference for markdown is baked into the model weights itself, and not so easy to override through prompting. This is one of the clearest examples I\u0026rsquo;ve found so far of the limitations of this learning system.\nThe 7B model has limitations. My base model is a fine-tuned Qwen 2.5 7B. It\u0026rsquo;s good, but it occasionally misunderstands, hallucinates, or generates malformed tool calls. A larger model would likely perform better, but I wanted something that runs locally on my MacBook.\nThis process adds significant latency to the chat process. The RAG retrieval, meta-prompting, and reflection processes all take time. Using a model that\u0026rsquo;s running on my laptop means it can feel painfully slow at times.\nWhat this means This was a fun experiment, but what does this mean? Is there any use for systems like these in industry?\nI\u0026rsquo;m far from an AI expert, but off the top of my head I can think of a couple potential applications of systems like these to develop real AI systems that solve real-world problems. Giving an LLM the ability to learn from experience opens the door to all kinds of systems we don’t really have today.\nThe obvious application is personalization: assistants that remember your preferences and workflows, which is what KenLM is. However, I think the implications go beyond “your AI knows your favorite sports team.” For example, imagine an educational model that can learn how you learn and what you know, remember your strong and weak areas, and adapt its explanations to your thinking style. This model could act as a more helpful learning companion than a model with no such memory capabilities.\nAnother application I can think of is creating models that improve themselves by optimizing for factors outside of human interaction. Imagine a model that initially struggles with chain-of-thought reasoning or tool usage—say something that consistently runs into the same error when trying to write python code or execute a web search. Currently, AI systems have no way to notice these things and improve. Human engineers need to monitor and step in, adjusting prompts, doing reinforcement learning, or collecting data with which to fine-tune in order to eke out a little bit better performance.\nA memory-equipped model changes this dynamic. For anything it\u0026rsquo;s trying to optimize for, be that reasoning or tool use, it can:\nnotice failed patterns by reasoning about them and remembering past experiences see successful patterns internalize the better method and adjust for future tasks Instead of updating billions of weights, the model updates a small set of learned strategies derived from lived experience. Over time, the model becomes a better problem solver not because it grew larger, but because it learned.\nThis all can potentially allow for smaller, more efficient models to rapidly adapt to specific use cases, reducing the industry\u0026rsquo;s reliance on frontier, compute-heavy models and allowing for more innovation and experimentation at the edge. Instead of treating model improvement as something that can only happen upstream, by massive labs with huge quantities of (often stolen) data, you get a world where models improve downstream, close to the user, close to the task, and close to the environment they operate in.\nIn that world, you don’t need a 400B-parameter model for everything. You need a reasonably capable base model with the right scaffolding: a memory system, a reflection system, and a mechanism for gradually internalizing the lessons it learns. Given enough experience, my intuition is that a modest model can outperform a much larger one on the tasks it specializes for, because it has context, history, and self-derived strategies the bigger model simply doesn’t have.\nIn the end, this really was just a weekend experiment, nothing more than an excuse to poke at a problem that’s been stuck in my head. I do, however, think that there are probably real ideas worth exploring here. I don’t know exactly where it will go yet, but I’m excited to keep experimenting and see how far a small model can go when you let it learn from its own experiences.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e\n\u003cp\u003eHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As \u003ca href=\"https://abehrouz.github.io/files/NL.pdf\"\u003ethis paper\u003c/a\u003e from Google points out, LLMs lack \u003cstrong\u003eneuroplasticity\u003c/strong\u003e—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes \u003cem\u003efrozen\u003c/em\u003e. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes we\u0026rsquo;re trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge from short-term to long-term storage, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights). All the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions). Each meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals). A distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass passes information between these components, using its working memory (context) as the vehicle. Let me break down how it works piece by piece.\nLayer 1: Episodic Memory This piece of the architecture is the most straightforward. To enable the model to remember specific experiences, every interaction is logged as an episode. Every episode captures the user prompt, agent response, and some metadata for memory management. It looks something like this:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-01T14:30:00\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;Can you help me write a README for this project?\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;Sure. Here\u0026#39;s a concise template...\u0026#34;, \u0026#34;access_count\u0026#34;: 3, \u0026#34;last_accessed\u0026#34;: \u0026#34;2025-12-03T10:15:00\u0026#34;, \u0026#34;is_protected\u0026#34;: false } To enable the model to retrieve relevant episodes at inference time, I utilize a technique known as retrieval-augmented generation (RAG). Simply put, I store the episode data in a vector store called ChromaDB. Embeddings are generated by sentence-transformers after every conversation. Every time I send a message, KenLM embeds my query and searches for the top-k most semantically similar episodes in its memory. This allows it to reference episodes related to my query, so if I ask about sports, it will pull up discussions where we talked about sports. If I ask about documentation, it will remember episodes where we talked about writing documentation.\nThis gives the model something resembling episodic recall: the ability to remember relevant experiences and bring them into working memory (the context window) when they\u0026rsquo;re useful.\nLayer 2: Reflective Memory Episodic memory alone would essentially be just a fancy search engine. Something that humans do that enables more complex learning is reflect on past episodes and draw generalized conclusions based on them. Taking inspiration from this process, I built an agentic reflection process into KenLM.\nWhen I exit a chat session, KenLM runs a reflection loop. A KenLM \u0026ldquo;reflection agent\u0026rdquo; analyzes the session\u0026rsquo;s episodes, searches for related patterns in past conversations, and updates a profile of hypotheses and learning goals.\nHere\u0026rsquo;s what the reflection system actually does:\nReviews the session — What patterns appeared in my behavior? What did I ask about? How did I respond to KenLM\u0026rsquo;s answers?\nSearches for evidence — The reflection agent can call a search_episodes(query) tool to find supporting patterns across all past conversations. This prevents overfitting to a single session.\nUpdates hypotheses — For each insight, it calls upsert_hypothesis() with a confidence score. Crucially, confidence only increases when patterns repeat across multiple episodes.\nMaintains learning goals — If the model notices something it doesn\u0026rsquo;t understand about me yet, it creates a learning goal—a question to explore in future interactions.\nHere\u0026rsquo;s what my actual profile looks like after some use:\n{ \u0026#34;hypotheses\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_007\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;accuracy\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth is detail-oriented and expects accurate and precise information.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.6, \u0026#34;evidence_episode_ids\u0026#34;: [ 90, 91, 89, 92 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_008\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;transparency\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth values transparency and honesty in communication.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 89 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_009\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;task_compliance\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth expects tasks to be completed accurately and without unnecessary steps.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 154, 155, 156, 157, 158 ] } ], \u0026#34;learning_goals\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;learning_goal_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;How can we ensure that Kenneth\u0026#39;s instructions are followed without unnecessary repetition or recapitulation?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;closed\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-11-30T15:10:55.704097\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;lg_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;What are Kenneth\u0026#39;s specific expectations for the precision and detail in project updates?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-12-02T15:51:56.904564\u0026#34; } ] } The reflection prompt is designed to be conservative. I explicitly instruct the model to start hypotheses with low confidence (0.2–0.4) and only raise confidence when it sees repeated evidence. This should hopefully prevent the kind of overconfident conclusions that can come from a single unusual interaction.\nLayer 3: The Forgetting System A pretty important piece of learning is actually the process by which we decide which pieces of information aren\u0026rsquo;t relevant. If we were to allow our episodic vector database to grow unbounded with every user interaction, it would quickly become slow and stop being useful during inference or reflection. Moreover, humans also don\u0026rsquo;t retain every bit of information that we are exposed to—we have complex systems that decide what information is worth retaining based on various factors.\nMy algorithm is a bit more rudimentary than the process that the human brain has, but it seems to be working fine so far. It essentially calculates an \u0026ldquo;importance\u0026rdquo; factor for every memory, which is just a weighted sum of a couple factors:\nimportance = w1 * log(1 + access_count) # Access frequency + w2 * is_protected # Protection bonus + w3 * is_evidence # Evidence bonus + w4 * exp(-age_days / tau) # Recency decay This system assigns a high score to frequently accessed memories, memories that the AI has decided it wants to \u0026ldquo;protect,\u0026rdquo; memories that are evidence for hypotheses that it has reasoned about, and memories that are recent. Memories that are old, infrequently accessed, and not useful for any of the learning goals or hypotheses the AI has created become candidates for forgetting.\nThe forgetting process is actually better understood as a summarization process. Instead of just deleting irrelevant memories, the system compacts them. It does this by summarizing \u0026ldquo;forgotten\u0026rdquo; episodes into condensed summaries and deleting the originals. This preserves knowledge while bounding storage growth—similar to how human memories become \u0026ldquo;fuzzy\u0026rdquo; over time.\nHow It Works End-to-End Let me walk through a complete cycle to show how these pieces fit together.\nStep 1: Message Received I type something like: \u0026ldquo;Can you write me a short essay about my favorite sports team?\u0026rdquo;\nStep 2: RAG Retrieval KenLM embeds my query and searches the episodic index. It finds 3 relevant past conversations about sports and essay preferences.\nStep 3: Context Assembly The system prompt is built from:\nCore identity document (who KenLM is, its purpose) Current profile (hypotheses + learning goals) RAG episodes (the relevant past conversations) Behavioral instructions (meta-prompt generated at session start) Step 4: Deliberation KenLM plans its approach: \u0026ldquo;Kenneth wants an essay about his favorite sports team. Based on my hypothesis about preferring conciseness, I should provide a short essay without excessive explanation. Based on our past conversations, his favorite sports team is the Seattle Sounders. I don\u0026rsquo;t need any external tools for this.\u0026rdquo;\nStep 5: Response Generation KenLM generates a response, informed by the plan and all the context. The response is concise and focused—because that\u0026rsquo;s what it\u0026rsquo;s learned that I prefer.\nStep 6: Episode Logging The conversation is saved as a new episode and indexed in the vector store.\nStep 7: Session End → Reflection When I exit the chat, the reflection agent wakes up. It reviews the session, searches for patterns, and decides whether to update any hypotheses or learning goals.\nStep 8: Next Session → Meta-Prompting The next time I start a chat, KenLM reads the (potentially updated) profile and generates fresh behavioral instructions. Then, the cycle continues.\nResults and Observations After using this system for a little bit, here are some of the results and observations I\u0026rsquo;ve observed:\nWhat\u0026rsquo;s working The conciseness hypothesis stuck. Early on, I found myself repeatedly asking KenLM to \u0026ldquo;be more concise\u0026rdquo; and to stop repeating itself. The reflection system noticed this pattern, formed a hypothesis, and now the model defaults to shorter responses. I rarely have to give feedback about the length of its responses anymore.\nRAG retrieval is surprisingly useful. When I ask about something I discussed in a past conversation, relevant context often appears in the model\u0026rsquo;s awareness. It doesn\u0026rsquo;t always explicitly reference the past conversation, but you can tell it\u0026rsquo;s informed by it. I can refer to things like \u0026ldquo;my favorite sports team\u0026rdquo; or \u0026ldquo;where I live\u0026rdquo; and the model remembers what those things are as they relate to me.\nMeta-prompting effectively influences personality. Each session feels more like what I\u0026rsquo;m looking for from a conversation with an LLM. It\u0026rsquo;s dropped the overly sycophantic tendencies, phrases like \u0026ldquo;Certainly!\u0026rdquo; or \u0026ldquo;You\u0026rsquo;re absolutely right!\u0026rdquo;, and other LLM-isms that have annoyed me. The behavioral instructions provide a stable \u0026ldquo;personality\u0026rdquo; for that session, and that personality is consistently adapting to my preferences.\nWhat\u0026rsquo;s Challenging Certain habits are hard to break. The model really loves markdown. I mean, it really loves outputting markdown formatting. No matter how many times I tell it it\u0026rsquo;s a command line tool and markdown is not helpful, it still really loves outputting neat markdown responses. Now, it HAS gotten a little better after much feedback, but I anticipate its preference for markdown is baked into the model weights itself, and not so easy to override through prompting. This is one of the clearest examples I\u0026rsquo;ve found so far of the limitations of this learning system.\nThe 7B model has limitations. My base model is a fine-tuned Qwen 2.5 7B. It\u0026rsquo;s good, but it occasionally misunderstands, hallucinates, or generates malformed tool calls. A larger model would likely perform better, but I wanted something that runs locally on my MacBook.\nThis process adds significant latency to the chat process. The RAG retrieval, meta-prompting, and reflection processes all take time. Using a model that\u0026rsquo;s running on my laptop means it can feel painfully slow at times.\nWhat this means This was a fun experiment, but what does this mean? Is there any use for systems like these in industry?\nI\u0026rsquo;m far from an AI expert, but off the top of my head I can think of a couple potential applications of systems like these to develop real AI systems that solve real-world problems. Giving an LLM the ability to learn from experience opens the door to all kinds of systems we don’t really have today.\nThe obvious application is personalization: assistants that remember your preferences and workflows, which is what KenLM is. However, I think the implications go beyond “your AI knows your favorite sports team.” For example, imagine an educational model that can learn how you learn and what you know, remember your strong and weak areas, and adapt its explanations to your thinking style. This model could act as a more helpful learning companion than a model with no such memory capabilities.\nAnother application I can think of is creating models that improve themselves by optimizing for factors outside of human interaction. Imagine a model that initially struggles with chain-of-thought reasoning or tool usage—say something that consistently runs into the same error when trying to write python code or execute a web search. Currently, AI systems have no way to notice these things and improve. Human engineers need to monitor and step in, adjusting prompts, doing reinforcement learning, or collecting data with which to fine-tune in order to eke out a little bit better performance.\nA memory-equipped model changes this dynamic. For anything it\u0026rsquo;s trying to optimize for, be that reasoning or tool use, it can:\nnotice failed patterns by reasoning about them and remembering past experiences see successful patterns internalize the better method and adjust for future tasks Instead of updating billions of weights, the model updates a small set of learned strategies derived from lived experience. Over time, the model becomes a better problem solver not because it grew larger, but because it learned.\nThis all can potentially allow for smaller, more efficient models to rapidly adapt to specific use cases, reducing the industry\u0026rsquo;s reliance on frontier, compute-heavy models and allowing for more innovation and experimentation at the edge. Instead of treating model improvement as something that can only happen upstream, by massive labs with huge quantities of (often stolen) data, you get a world where models improve downstream, close to the user, close to the task, and close to the environment they operate in.\nIn that world, you don’t need a 400B-parameter model for everything. You need a reasonably capable base model with the right scaffolding: a memory system, a reflection system, and a mechanism for gradually internalizing the lessons it learns. Given enough experience, my intuition is that a modest model can outperform a much larger one on the tasks it specializes for, because it has context, history, and self-derived strategies the bigger model simply doesn’t have.\nIn the end, this really was just a weekend experiment, nothing more than an excuse to poke at a problem that’s been stuck in my head. I do, however, think that there are probably real ideas worth exploring here. I don’t know exactly where it will go yet, but I’m excited to keep experimenting and see how far a small model can go when you let it learn from its own experiences.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e\n\u003cp\u003eHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As \u003ca href=\"https://abehrouz.github.io/files/NL.pdf\"\u003ethis paper\u003c/a\u003e from Google points out, LLMs lack \u003cstrong\u003eneuroplasticity\u003c/strong\u003e—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes \u003cem\u003efrozen\u003c/em\u003e. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes we\u0026rsquo;re trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge from short-term to long-term storage, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights). All the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions). Each meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals). A distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass passes information between these components, using its working memory (context) as the vehicle. Let me break down how it works piece by piece.\nLayer 1: Episodic Memory This piece of the architecture is the most straightforward. To enable the model to remember specific experiences, every interaction is logged as an episode. Every episode captures the user prompt, agent response, and some metadata for memory management. It looks something like this:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-01T14:30:00\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;Can you help me write a README for this project?\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;Sure. Here\u0026#39;s a concise template...\u0026#34;, \u0026#34;access_count\u0026#34;: 3, \u0026#34;last_accessed\u0026#34;: \u0026#34;2025-12-03T10:15:00\u0026#34;, \u0026#34;is_protected\u0026#34;: false } To enable the model to retrieve relevant episodes at inference time, I utilize a technique known as retrieval-augmented generation (RAG). Simply put, I store the episode data in a vector store called ChromaDB. Embeddings are generated by sentence-transformers after every conversation. Every time I send a message, KenLM embeds my query and searches for the top-k most semantically similar episodes in its memory. This allows it to reference episodes related to my query, so if I ask about sports, it will pull up discussions where we talked about sports. If I ask about documentation, it will remember episodes where we talked about writing documentation.\nThis gives the model something resembling episodic recall: the ability to remember relevant experiences and bring them into working memory (the context window) when they\u0026rsquo;re useful.\nLayer 2: Reflective Memory Episodic memory alone would essentially be just a fancy search engine. Something that humans do that enables more complex learning is reflect on past episodes and draw generalized conclusions based on them. Taking inspiration from this process, I built an agentic reflection process into KenLM.\nWhen I exit a chat session, KenLM runs a reflection loop. A KenLM \u0026ldquo;reflection agent\u0026rdquo; analyzes the session\u0026rsquo;s episodes, searches for related patterns in past conversations, and updates a profile of hypotheses and learning goals.\nHere\u0026rsquo;s what the reflection system actually does:\nReviews the session — What patterns appeared in my behavior? What did I ask about? How did I respond to KenLM\u0026rsquo;s answers?\nSearches for evidence — The reflection agent can call a search_episodes(query) tool to find supporting patterns across all past conversations. This prevents overfitting to a single session.\nUpdates hypotheses — For each insight, it calls upsert_hypothesis() with a confidence score. Crucially, confidence only increases when patterns repeat across multiple episodes.\nMaintains learning goals — If the model notices something it doesn\u0026rsquo;t understand about me yet, it creates a learning goal—a question to explore in future interactions.\nHere\u0026rsquo;s what my actual profile looks like after some use:\n{ \u0026#34;hypotheses\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_007\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;accuracy\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth is detail-oriented and expects accurate and precise information.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.6, \u0026#34;evidence_episode_ids\u0026#34;: [ 90, 91, 89, 92 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_008\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;transparency\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth values transparency and honesty in communication.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 89 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_009\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;task_compliance\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth expects tasks to be completed accurately and without unnecessary steps.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 154, 155, 156, 157, 158 ] } ], \u0026#34;learning_goals\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;learning_goal_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;How can we ensure that Kenneth\u0026#39;s instructions are followed without unnecessary repetition or recapitulation?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;closed\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-11-30T15:10:55.704097\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;lg_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;What are Kenneth\u0026#39;s specific expectations for the precision and detail in project updates?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-12-02T15:51:56.904564\u0026#34; } ] } The reflection prompt is designed to be conservative. I explicitly instruct the model to start hypotheses with low confidence (0.2–0.4) and only raise confidence when it sees repeated evidence. This should hopefully prevent the kind of overconfident conclusions that can come from a single unusual interaction.\nLayer 3: The Forgetting System A pretty important piece of learning is actually the process by which we decide which pieces of information aren\u0026rsquo;t relevant. If we were to allow our episodic vector database to grow unbounded with every user interaction, it would quickly become slow and stop being useful during inference or reflection. Moreover, humans also don\u0026rsquo;t retain every bit of information that we are exposed to—we have complex systems that decide what information is worth retaining based on various factors.\nMy algorithm is a bit more rudimentary than the process that the human brain has, but it seems to be working fine so far. It essentially calculates an \u0026ldquo;importance\u0026rdquo; score for every memory, which is just a weighted sum of a couple factors:\nimportance = w1 * log(1 + access_count) # Access frequency + w2 * is_protected # Protection bonus + w3 * is_evidence # Evidence bonus + w4 * exp(-age_days / tau) # Recency decay This system assigns a high score to frequently accessed memories, memories that the AI has decided it wants to \u0026ldquo;protect,\u0026rdquo; memories that are evidence for hypotheses that it has reasoned about, and memories that are recent. Memories that are old, infrequently accessed, and not useful for any of the learning goals or hypotheses the AI has created become candidates for forgetting.\nThe forgetting process is actually better understood as a summarization process. Instead of just deleting irrelevant memories, the system compacts them. It does this by summarizing \u0026ldquo;forgotten\u0026rdquo; episodes into condensed summaries and deleting the originals. This preserves knowledge while bounding storage growth—similar to how human memories become \u0026ldquo;fuzzy\u0026rdquo; over time.\nHow It Works End-to-End Let me walk through a complete cycle to show how these pieces fit together.\nStep 1: Message Received I type something like: \u0026ldquo;Can you write me a short essay about my favorite sports team?\u0026rdquo;\nStep 2: RAG Retrieval KenLM embeds my query and searches the episodic index. It finds 3 relevant past conversations about sports and essay preferences.\nStep 3: Context Assembly The system prompt is built from:\nCore identity document (who KenLM is, its purpose) Current profile (hypotheses + learning goals) RAG episodes (the relevant past conversations) Behavioral instructions (meta-prompt generated at session start) Step 4: Deliberation KenLM plans its approach: \u0026ldquo;Kenneth wants an essay about his favorite sports team. Based on my hypothesis about preferring conciseness, I should provide a short essay without excessive explanation. Based on our past conversations, his favorite sports team is the Seattle Sounders. I don\u0026rsquo;t need any external tools for this.\u0026rdquo;\nStep 5: Response Generation KenLM generates a response, informed by the plan and all the context. The response is concise and focused—because that\u0026rsquo;s what it\u0026rsquo;s learned that I prefer.\nStep 6: Episode Logging The conversation is saved as a new episode and indexed in the vector store.\nStep 7: Session End → Reflection When I exit the chat, the reflection agent wakes up. It reviews the session, searches for patterns, and decides whether to update any hypotheses or learning goals.\nStep 8: Next Session → Meta-Prompting The next time I start a chat, KenLM reads the (potentially updated) profile and generates fresh behavioral instructions. Then, the cycle continues.\nResults and Observations After using this system for a little bit, here are some of the results and observations I\u0026rsquo;ve observed:\nWhat\u0026rsquo;s working The conciseness hypothesis stuck. Early on, I found myself repeatedly asking KenLM to \u0026ldquo;be more concise\u0026rdquo; and to stop repeating itself. The reflection system noticed this pattern, formed a hypothesis, and now the model defaults to shorter responses. I rarely have to give feedback about the length of its responses anymore.\nRAG retrieval is surprisingly useful. When I ask about something I discussed in a past conversation, relevant context often appears in the model\u0026rsquo;s awareness. It doesn\u0026rsquo;t always explicitly reference the past conversation, but you can tell it\u0026rsquo;s informed by it. I can refer to things like \u0026ldquo;my favorite sports team\u0026rdquo; or \u0026ldquo;where I live\u0026rdquo; and the model remembers what those things are as they relate to me.\nMeta-prompting effectively influences personality. Each session feels more like what I\u0026rsquo;m looking for from a conversation with an LLM. It\u0026rsquo;s dropped the overly sycophantic tendencies, phrases like \u0026ldquo;Certainly!\u0026rdquo; or \u0026ldquo;You\u0026rsquo;re absolutely right!\u0026rdquo;, and other LLM-isms that have annoyed me. The behavioral instructions provide a stable \u0026ldquo;personality\u0026rdquo; for that session, and that personality is consistently adapting to my preferences.\nWhat\u0026rsquo;s Challenging Certain habits are hard to break. The model really loves markdown. I mean, it really loves outputting markdown formatting. No matter how many times I tell it it\u0026rsquo;s a command line tool and markdown is not helpful, it still really loves outputting neat markdown responses. Now, it HAS gotten a little better after much feedback, but I anticipate its preference for markdown is baked into the model weights itself, and not so easy to override through prompting. This is one of the clearest examples I\u0026rsquo;ve found so far of the limitations of this learning system.\nThe 7B model has limitations. My base model is a fine-tuned Qwen 2.5 7B. It\u0026rsquo;s good, but it occasionally misunderstands, hallucinates, or generates malformed tool calls. A larger model would likely perform better, but I wanted something that runs locally on my MacBook.\nThis process adds significant latency to the chat process. The RAG retrieval, meta-prompting, and reflection processes all take time. Using a model that\u0026rsquo;s running on my laptop means it can feel painfully slow at times.\nWhat this means This was a fun experiment, but what does this mean? Is there any use for systems like these in industry?\nI\u0026rsquo;m far from an AI expert, but off the top of my head I can think of a couple potential applications of systems like these to develop real AI systems that solve real-world problems. Giving an LLM the ability to learn from experience opens the door to all kinds of systems we don’t really have today.\nThe obvious application is personalization: assistants that remember your preferences and workflows, which is what KenLM is. However, I think the implications go beyond “your AI knows your favorite sports team.” For example, imagine an educational model that can learn how you learn and what you know, remember your strong and weak areas, and adapt its explanations to your thinking style. This model could act as a more helpful learning companion than a model with no such memory capabilities.\nAnother application I can think of is creating models that improve themselves by optimizing for factors outside of human interaction. Imagine a model that initially struggles with chain-of-thought reasoning or tool usage—say something that consistently runs into the same error when trying to write python code or execute a web search. Currently, AI systems have no way to notice these things and improve. Human engineers need to monitor and step in, adjusting prompts, doing reinforcement learning, or collecting data with which to fine-tune in order to eke out a little bit better performance.\nA memory-equipped model changes this dynamic. For anything it\u0026rsquo;s trying to optimize for, be that reasoning or tool use, it can:\nnotice failed patterns by reasoning about them and remembering past experiences see successful patterns internalize the better method and adjust for future tasks Instead of updating billions of weights, the model updates a small set of learned strategies derived from lived experience. Over time, the model becomes a better problem solver not because it grew larger, but because it learned.\nThis all can potentially allow for smaller, more efficient models to rapidly adapt to specific use cases, reducing the industry\u0026rsquo;s reliance on frontier, compute-heavy models and allowing for more innovation and experimentation at the edge. Instead of treating model improvement as something that can only happen upstream, by massive labs with huge quantities of (often stolen) data, you get a world where models improve downstream, close to the user, close to the task, and close to the environment they operate in.\nIn that world, you don’t need a 400B-parameter model for everything. You need a reasonably capable base model with the right scaffolding: a memory system, a reflection system, and a mechanism for gradually internalizing the lessons it learns. Given enough experience, my intuition is that a modest model can outperform a much larger one on the tasks it specializes for, because it has context, history, and self-derived strategies the bigger model simply doesn’t have.\nIn the end, this really was just a weekend experiment, nothing more than an excuse to poke at a problem that’s been stuck in my head. I do, however, think that there are probably real ideas worth exploring here. I don’t know exactly where it will go yet, but I’m excited to keep experimenting and see how far a small model can go when you let it learn from its own experiences.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e\n\u003cp\u003eHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As \u003ca href=\"https://abehrouz.github.io/files/NL.pdf\"\u003ethis paper\u003c/a\u003e from Google points out, LLMs lack \u003cstrong\u003eneuroplasticity\u003c/strong\u003e—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes \u003cem\u003efrozen\u003c/em\u003e. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes we\u0026rsquo;re trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge from short-term to long-term storage, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights). All the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions). Each meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals). A distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass passes information between these components, using its working memory (context) as the vehicle. Let me break down how it works piece by piece.\nLayer 1: Episodic Memory This piece of the architecture is the most straightforward. To enable the model to remember specific experiences, every interaction is logged as an episode. Every episode captures the user prompt, agent response, and some metadata for memory management. It looks something like this:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-01T14:30:00\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;Can you help me write a README for this project?\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;Sure. Here\u0026#39;s a concise template...\u0026#34;, \u0026#34;access_count\u0026#34;: 3, \u0026#34;last_accessed\u0026#34;: \u0026#34;2025-12-03T10:15:00\u0026#34;, \u0026#34;is_protected\u0026#34;: false } To enable the model to retrieve relevant episodes at inference time, I utilize a technique known as retrieval-augmented generation (RAG). Simply put, I store the episode data in a vector store called ChromaDB. Embeddings are generated by sentence-transformers after every conversation. Every time I send a message, KenLM embeds my query and searches for the top-k most semantically similar episodes in its memory. This allows it to reference episodes related to my query, so if I ask about sports, it will pull up discussions where we talked about sports. If I ask about documentation, it will remember episodes where we talked about writing documentation.\nThis gives the model something resembling episodic recall: the ability to remember relevant experiences and bring them into working memory (the context window) when they\u0026rsquo;re useful.\nLayer 2: Reflective Memory Episodic memory alone would essentially be just a fancy search engine. Something that humans do that enables more complex learning is reflect on past episodes and draw generalized conclusions based on them. Taking inspiration from this process, I built an agentic reflection process into KenLM.\nWhen I exit a chat session, KenLM runs a reflection loop. A KenLM \u0026ldquo;reflection agent\u0026rdquo; analyzes the session\u0026rsquo;s episodes, searches for related patterns in past conversations, and updates a profile of hypotheses and learning goals.\nHere\u0026rsquo;s what the reflection system actually does:\nReviews the session — What patterns appeared in my behavior? What did I ask about? How did I respond to KenLM\u0026rsquo;s answers?\nSearches for evidence — The reflection agent can call a search_episodes(query) tool to find supporting patterns across all past conversations. This prevents overfitting to a single session.\nUpdates hypotheses — For each insight, it calls upsert_hypothesis() with a confidence score. Crucially, confidence only increases when patterns repeat across multiple episodes.\nMaintains learning goals — If the model notices something it doesn\u0026rsquo;t understand about me yet, it creates a learning goal—a question to explore in future interactions.\nHere\u0026rsquo;s what my actual profile looks like after some use:\n{ \u0026#34;hypotheses\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_007\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;accuracy\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth is detail-oriented and expects accurate and precise information.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.6, \u0026#34;evidence_episode_ids\u0026#34;: [ 90, 91, 89, 92 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_008\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;transparency\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth values transparency and honesty in communication.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 89 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_009\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;task_compliance\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth expects tasks to be completed accurately and without unnecessary steps.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 154, 155, 156, 157, 158 ] } ], \u0026#34;learning_goals\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;learning_goal_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;How can we ensure that Kenneth\u0026#39;s instructions are followed without unnecessary repetition or recapitulation?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;closed\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-11-30T15:10:55.704097\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;lg_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;What are Kenneth\u0026#39;s specific expectations for the precision and detail in project updates?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-12-02T15:51:56.904564\u0026#34; } ] } The reflection prompt is designed to be conservative. I explicitly instruct the model to start hypotheses with low confidence (0.2–0.4) and only raise confidence when it sees repeated evidence. This should hopefully prevent the kind of overconfident conclusions that can come from a single unusual interaction.\nLayer 3: The Forgetting System A pretty important piece of learning is actually the process by which we decide which pieces of information aren\u0026rsquo;t relevant. If we were to allow our episodic vector database to grow unbounded with every user interaction, it would quickly become slow and stop being useful during inference or reflection. Moreover, humans also don\u0026rsquo;t retain every bit of information that we are exposed to—we have complex systems that decide what information is worth retaining based on various factors.\nMy algorithm is a bit more rudimentary than the process that the human brain has, but it seems to be working fine so far. It essentially calculates an \u0026ldquo;importance\u0026rdquo; score for every memory, which is just a weighted sum of a couple factors:\nimportance = w1 * log(1 + access_count) # Access frequency + w2 * is_protected # Protection bonus + w3 * is_evidence # Evidence bonus + w4 * exp(-age_days / tau) # Recency decay This system assigns a high score to frequently accessed memories, memories that the AI has decided it wants to \u0026ldquo;protect,\u0026rdquo; memories that are evidence for hypotheses that it has reasoned about, and memories that are recent. Memories that are old, infrequently accessed, and not useful for any of the learning goals or hypotheses the AI has created become candidates for forgetting.\nThe forgetting process is actually better understood as a summarization process. Instead of just deleting irrelevant memories, the system compacts them. It does this by summarizing \u0026ldquo;forgotten\u0026rdquo; episodes into condensed summaries and deleting the originals. This preserves knowledge while bounding storage growth—similar to how human memories become \u0026ldquo;fuzzy\u0026rdquo; over time.\nHow It Works End-to-End Let me walk through a complete cycle to show how these pieces fit together.\nStep 1: Message Received: I type something like: \u0026ldquo;Can you write me a short essay about my favorite sports team?\u0026rdquo;\nStep 2: RAG Retrieval: KenLM embeds my query and searches the episodic index. It finds 3 relevant past conversations about sports and essay preferences.\nStep 3: Context Assembly: The system prompt is built from:\nCore identity document (who KenLM is, its purpose) Current profile (hypotheses + learning goals) RAG episodes (the relevant past conversations) Behavioral instructions (meta-prompt generated at session start) Step 4: Deliberation: KenLM plans its approach: \u0026ldquo;Kenneth wants an essay about his favorite sports team. Based on my hypothesis about preferring conciseness, I should provide a short essay without excessive explanation. Based on our past conversations, his favorite sports team is the Seattle Sounders. I don\u0026rsquo;t need any external tools for this.\u0026rdquo;\nStep 5: Response Generation: KenLM generates a response, informed by the plan and all the context. The response is concise and focused—because that\u0026rsquo;s what it\u0026rsquo;s learned that I prefer.\nStep 6: Episode Logging: The conversation is saved as a new episode and indexed in the vector store.\nStep 7: Session End → Reflection: When I exit the chat, the reflection agent wakes up. It reviews the session, searches for patterns, and decides whether to update any hypotheses or learning goals.\nStep 8: Next Session → Meta-Prompting: The next time I start a chat, KenLM reads the (potentially updated) profile and generates fresh behavioral instructions. Then, the cycle continues.\nResults and Observations After using this system for a little bit, here are some of the results and observations I\u0026rsquo;ve observed:\nWhat\u0026rsquo;s working The conciseness hypothesis stuck. Early on, I found myself repeatedly asking KenLM to \u0026ldquo;be more concise\u0026rdquo; and to stop repeating itself. The reflection system noticed this pattern, formed a hypothesis, and now the model defaults to shorter responses. I rarely have to give feedback about the length of its responses anymore.\nRAG retrieval is surprisingly useful. When I ask about something I discussed in a past conversation, relevant context often appears in the model\u0026rsquo;s awareness. It doesn\u0026rsquo;t always explicitly reference the past conversation, but you can tell it\u0026rsquo;s informed by it. I can refer to things like \u0026ldquo;my favorite sports team\u0026rdquo; or \u0026ldquo;where I live\u0026rdquo; and the model remembers what those things are as they relate to me.\nMeta-prompting effectively influences personality. Each session feels more like what I\u0026rsquo;m looking for from a conversation with an LLM. It\u0026rsquo;s dropped the overly sycophantic tendencies, phrases like \u0026ldquo;Certainly!\u0026rdquo; or \u0026ldquo;You\u0026rsquo;re absolutely right!\u0026rdquo;, and other LLM-isms that have annoyed me. The behavioral instructions provide a stable \u0026ldquo;personality\u0026rdquo; for that session, and that personality is consistently adapting to my preferences.\nWhat\u0026rsquo;s Challenging Certain habits are hard to break. The model really loves markdown. I mean, it really loves outputting markdown formatting. No matter how many times I tell it it\u0026rsquo;s a command line tool and markdown is not helpful, it still really loves outputting neat markdown responses. Now, it HAS gotten a little better after much feedback, but I anticipate its preference for markdown is baked into the model weights itself, and not so easy to override through prompting. This is one of the clearest examples I\u0026rsquo;ve found so far of the limitations of this learning system.\nThe 7B model has limitations. My base model is a fine-tuned Qwen 2.5 7B. It\u0026rsquo;s good, but it occasionally misunderstands, hallucinates, or generates malformed tool calls. A larger model would likely perform better, but I wanted something that runs locally on my MacBook.\nThis process adds significant latency to the chat process. The RAG retrieval, meta-prompting, and reflection processes all take time. Using a model that\u0026rsquo;s running on my laptop means it can feel painfully slow at times.\nWhat this means This was a fun experiment, but what does this mean? Is there any use for systems like these in industry?\nI\u0026rsquo;m far from an AI expert, but off the top of my head I can think of a couple potential applications of systems like these to develop real AI systems that solve real-world problems. Giving an LLM the ability to learn from experience opens the door to all kinds of systems we don’t really have today.\nThe obvious application is personalization: assistants that remember your preferences and workflows, which is what KenLM is. However, I think the implications go beyond “your AI knows your favorite sports team.” For example, imagine an educational model that can learn how you learn and what you know, remember your strong and weak areas, and adapt its explanations to your thinking style. This model could act as a more helpful learning companion than a model with no such memory capabilities.\nAnother application I can think of is creating models that improve themselves by optimizing for factors outside of human interaction. Imagine a model that initially struggles with chain-of-thought reasoning or tool usage—say something that consistently runs into the same error when trying to write python code or execute a web search. Currently, AI systems have no way to notice these things and improve. Human engineers need to monitor and step in, adjusting prompts, doing reinforcement learning, or collecting data with which to fine-tune in order to eke out a little bit better performance.\nA memory-equipped model changes this dynamic. For anything it\u0026rsquo;s trying to optimize for, be that reasoning or tool use, it can:\nnotice failed patterns by reasoning about them and remembering past experiences see successful patterns internalize the better method and adjust for future tasks Instead of updating billions of weights, the model updates a small set of learned strategies derived from lived experience. Over time, the model becomes a better problem solver not because it grew larger, but because it learned.\nThis all can potentially allow for smaller, more efficient models to rapidly adapt to specific use cases, reducing the industry\u0026rsquo;s reliance on frontier, compute-heavy models and allowing for more innovation and experimentation at the edge. Instead of treating model improvement as something that can only happen upstream, by massive labs with huge quantities of (often stolen) data, you get a world where models improve downstream, close to the user, close to the task, and close to the environment they operate in.\nIn that world, you don’t need a 400B-parameter model for everything. You need a reasonably capable base model with the right scaffolding: a memory system, a reflection system, and a mechanism for gradually internalizing the lessons it learns. Given enough experience, my intuition is that a modest model can outperform a much larger one on the tasks it specializes for, because it has context, history, and self-derived strategies the bigger model simply doesn’t have.\nIn the end, this really was just a weekend experiment, nothing more than an excuse to poke at a problem that’s been stuck in my head. I do, however, think that there are probably real ideas worth exploring here. I don’t know exactly where it will go yet, but I’m excited to keep experimenting and see how far a small model can go when you let it learn from its own experiences.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e\n\u003cp\u003eHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As \u003ca href=\"https://abehrouz.github.io/files/NL.pdf\"\u003ethis paper\u003c/a\u003e from Google points out, LLMs lack \u003cstrong\u003eneuroplasticity\u003c/strong\u003e—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes \u003cem\u003efrozen\u003c/em\u003e. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes we\u0026rsquo;re trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge from short-term to long-term storage, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights). All the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions). Each meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals). A distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass passes information between these components, using its working memory (context) as the vehicle. Let me break down how it works piece by piece.\nLayer 1: Episodic Memory This piece of the architecture is the most straightforward. To enable the model to remember specific experiences, every interaction is logged as an episode. Every episode captures the user prompt, agent response, and some metadata for memory management. It looks something like this:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-01T14:30:00\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;Can you help me write a README for this project?\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;Sure. Here\u0026#39;s a concise template...\u0026#34;, \u0026#34;access_count\u0026#34;: 3, \u0026#34;last_accessed\u0026#34;: \u0026#34;2025-12-03T10:15:00\u0026#34;, \u0026#34;is_protected\u0026#34;: false } To enable the model to retrieve relevant episodes at inference time, I utilize a technique known as retrieval-augmented generation (RAG). Simply put, I store the episode data in a vector store called ChromaDB. Embeddings are generated by sentence-transformers after every conversation. Every time I send a message, KenLM embeds my query and searches for the top-k most semantically similar episodes in its memory. This allows it to reference episodes related to my query, so if I ask about sports, it will pull up discussions where we talked about sports. If I ask about documentation, it will remember episodes where we talked about writing documentation.\nThis gives the model something resembling episodic recall: the ability to remember relevant experiences and bring them into working memory (the context window) when they\u0026rsquo;re useful.\nLayer 2: Reflective Memory Episodic memory alone would essentially be just a fancy search engine. Something that humans do that enables more complex learning is reflect on past episodes and draw generalized conclusions based on them. Taking inspiration from this process, I built an agentic reflection process into KenLM.\nWhen I exit a chat session, KenLM runs a reflection loop. A KenLM \u0026ldquo;reflection agent\u0026rdquo; analyzes the session\u0026rsquo;s episodes, searches for related patterns in past conversations, and updates a profile of hypotheses and learning goals.\nHere\u0026rsquo;s what the reflection system actually does:\nReviews the session — What patterns appeared in my behavior? What did I ask about? How did I respond to KenLM\u0026rsquo;s answers?\nSearches for evidence — The reflection agent can call a search_episodes(query) tool to find supporting patterns across all past conversations. This prevents overfitting to a single session.\nUpdates hypotheses — For each insight, it calls upsert_hypothesis() with a confidence score. Crucially, confidence only increases when patterns repeat across multiple episodes.\nMaintains learning goals — If the model notices something it doesn\u0026rsquo;t understand about me yet, it creates a learning goal—a question to explore in future interactions.\nHere\u0026rsquo;s what my actual profile looks like after some use:\n{ \u0026#34;hypotheses\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_007\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;accuracy\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth is detail-oriented and expects accurate and precise information.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.6, \u0026#34;evidence_episode_ids\u0026#34;: [ 90, 91, 89, 92 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_008\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;transparency\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth values transparency and honesty in communication.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 89 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_009\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;task_compliance\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth expects tasks to be completed accurately and without unnecessary steps.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 154, 155, 156, 157, 158 ] } ], \u0026#34;learning_goals\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;learning_goal_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;How can we ensure that Kenneth\u0026#39;s instructions are followed without unnecessary repetition or recapitulation?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;closed\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-11-30T15:10:55.704097\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;lg_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;What are Kenneth\u0026#39;s specific expectations for the precision and detail in project updates?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-12-02T15:51:56.904564\u0026#34; } ] } The reflection prompt is designed to be conservative. I explicitly instruct the model to start hypotheses with low confidence (0.2–0.4) and only raise confidence when it sees repeated evidence. This should hopefully prevent the kind of overconfident conclusions that can come from a single unusual interaction.\nLayer 3: The Forgetting System A pretty important piece of learning is actually the process by which we decide which pieces of information aren\u0026rsquo;t relevant. If we were to allow our episodic vector database to grow unbounded with every user interaction, it would quickly become slow and stop being useful during inference or reflection. Moreover, humans also don\u0026rsquo;t retain every bit of information that we are exposed to—we have complex systems that decide what information is worth retaining based on various factors.\nMy algorithm is a bit more rudimentary than the process that the human brain has, but it seems to be working fine so far. It essentially calculates an \u0026ldquo;importance\u0026rdquo; score for every memory, which is just a weighted sum of a couple factors:\nimportance = w1 * log(1 + access_count) # Access frequency + w2 * is_protected # Protection bonus + w3 * is_evidence # Evidence bonus + w4 * exp(-age_days / tau) # Recency decay This system assigns a high score to frequently accessed memories, memories that the AI has decided it wants to \u0026ldquo;protect,\u0026rdquo; memories that are evidence for hypotheses that it has reasoned about, and memories that are recent. Memories that are old, infrequently accessed, and not useful for any of the learning goals or hypotheses the AI has created become candidates for forgetting.\nThe forgetting process is actually better understood as a summarization process. Instead of just deleting irrelevant memories, the system compacts them. It does this by summarizing \u0026ldquo;forgotten\u0026rdquo; episodes into condensed summaries and deleting the originals. This preserves knowledge while bounding storage growth—similar to how human memories become \u0026ldquo;fuzzy\u0026rdquo; over time.\nHow It Works End-to-End Let me walk through a complete cycle to show how these pieces fit together.\nStep 1: Message Received: I type something like: \u0026ldquo;Can you write me a short essay about my favorite sports team?\u0026rdquo;\nStep 2: RAG Retrieval: KenLM embeds my query and searches the episodic index. It finds 3 relevant past conversations about sports and essay preferences.\nStep 3: Context Assembly: The system prompt is built from:\nCore identity document (who KenLM is, its purpose) Current profile (hypotheses + learning goals) RAG episodes (the relevant past conversations) Behavioral instructions (meta-prompt generated at session start) Step 4: Deliberation: KenLM plans its approach: \u0026ldquo;Kenneth wants an essay about his favorite sports team. Based on my hypothesis about preferring conciseness, I should provide a short essay without excessive explanation. Based on our past conversations, his favorite sports team is the Seattle Sounders. I don\u0026rsquo;t need any external tools for this.\u0026rdquo;\nStep 5: Response Generation: KenLM generates a response, informed by the plan and all the context. The response is concise and focused—because that\u0026rsquo;s what it\u0026rsquo;s learned that I prefer.\nStep 6: Episode Logging: The conversation is saved as a new episode and indexed in the vector store.\nStep 7: Session End → Reflection: When I exit the chat, the reflection agent wakes up. It reviews the session, searches for patterns, and decides whether to update any hypotheses or learning goals.\nStep 8: Next Session → Meta-Prompting: The next time I start a chat, KenLM reads the (potentially updated) profile and generates fresh behavioral instructions. Then, the cycle continues.\nResults and Observations After using this system for a little bit, here are some of the results I\u0026rsquo;ve observed:\nWhat\u0026rsquo;s working The conciseness hypothesis stuck. Early on, I found myself repeatedly asking KenLM to \u0026ldquo;be more concise\u0026rdquo; and to stop repeating itself. The reflection system noticed this pattern, formed a hypothesis, and now the model defaults to shorter responses. I rarely have to give feedback about the length of its responses anymore.\nRAG retrieval is surprisingly useful. When I ask about something I discussed in a past conversation, relevant context often appears in the model\u0026rsquo;s awareness. It doesn\u0026rsquo;t always explicitly reference the past conversation, but you can tell it\u0026rsquo;s informed by it. I can refer to things like \u0026ldquo;my favorite sports team\u0026rdquo; or \u0026ldquo;where I live\u0026rdquo; and the model remembers what those things are as they relate to me.\nMeta-prompting effectively influences personality. Each session feels more like what I\u0026rsquo;m looking for from a conversation with an LLM. It\u0026rsquo;s dropped the overly sycophantic tendencies, phrases like \u0026ldquo;Certainly!\u0026rdquo; or \u0026ldquo;You\u0026rsquo;re absolutely right!\u0026rdquo;, and other LLM-isms that have annoyed me. The behavioral instructions provide a stable \u0026ldquo;personality\u0026rdquo; for that session, and that personality is consistently adapting to my preferences.\nWhat\u0026rsquo;s Challenging Certain habits are hard to break. The model really loves markdown. I mean, it really loves outputting markdown formatting. No matter how many times I tell it it\u0026rsquo;s a command line tool and markdown is not helpful, it still really loves outputting neat markdown responses. Now, it HAS gotten a little better after much feedback, but I anticipate its preference for markdown is baked into the model weights itself, and not so easy to override through prompting. This is one of the clearest examples I\u0026rsquo;ve found so far of the limitations of this learning system.\nThe 7B model has limitations. My base model is a fine-tuned Qwen 2.5 7B. It\u0026rsquo;s good, but it occasionally misunderstands, hallucinates, or generates malformed tool calls. A larger model would likely perform better, but I wanted something that runs locally on my MacBook.\nThis process adds significant latency to the chat process. The RAG retrieval, meta-prompting, and reflection processes all take time. Using a model that\u0026rsquo;s running on my laptop means it can feel painfully slow at times.\nWhat this means This was a fun experiment, but what does this mean? Is there any use for systems like these in industry?\nI\u0026rsquo;m far from an AI expert, but off the top of my head I can think of a couple potential applications of systems like these to develop real AI systems that solve real-world problems. Giving an LLM the ability to learn from experience opens the door to all kinds of systems we don’t really have today.\nThe obvious application is personalization: assistants that remember your preferences and workflows, which is what KenLM is. However, I think the implications go beyond “your AI knows your favorite sports team.” For example, imagine an educational model that can learn how you learn and what you know, remember your strong and weak areas, and adapt its explanations to your thinking style. This model could act as a more helpful learning companion than a model with no such memory capabilities.\nAnother application I can think of is creating models that improve themselves by optimizing for factors outside of human interaction. Imagine a model that initially struggles with chain-of-thought reasoning or tool usage—say something that consistently runs into the same error when trying to write python code or execute a web search. Currently, AI systems have no way to notice these things and improve. Human engineers need to monitor and step in, adjusting prompts, doing reinforcement learning, or collecting data with which to fine-tune in order to eke out a little bit better performance.\nA memory-equipped model changes this dynamic. For anything it\u0026rsquo;s trying to optimize for, be that reasoning or tool use, it can:\nnotice failed patterns by reasoning about them and remembering past experiences see successful patterns internalize the better method and adjust for future tasks Instead of updating billions of weights, the model updates a small set of learned strategies derived from lived experience. Over time, the model becomes a better problem solver not because it grew larger, but because it learned.\nThis all can potentially allow for smaller, more efficient models to rapidly adapt to specific use cases, reducing the industry\u0026rsquo;s reliance on frontier, compute-heavy models and allowing for more innovation and experimentation at the edge. Instead of treating model improvement as something that can only happen upstream, by massive labs with huge quantities of (often stolen) data, you get a world where models improve downstream, close to the user, close to the task, and close to the environment they operate in.\nIn that world, you don’t need a 400B-parameter model for everything. You need a reasonably capable base model with the right scaffolding: a memory system, a reflection system, and a mechanism for gradually internalizing the lessons it learns. Given enough experience, my intuition is that a modest model can outperform a much larger one on the tasks it specializes for, because it has context, history, and self-derived strategies the bigger model simply doesn’t have.\nIn the end, this really was just a weekend experiment, nothing more than an excuse to poke at a problem that’s been stuck in my head. I do, however, think that there are probably real ideas worth exploring here. I don’t know exactly where it will go yet, but I’m excited to keep experimenting and see how far a small model can go when you let it learn from its own experiences.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e\n\u003cp\u003eHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As \u003ca href=\"https://abehrouz.github.io/files/NL.pdf\"\u003ethis paper\u003c/a\u003e from Google points out, LLMs lack \u003cstrong\u003eneuroplasticity\u003c/strong\u003e—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes \u003cem\u003efrozen\u003c/em\u003e. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes we\u0026rsquo;re trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge from short-term to long-term storage, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights). All the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions). Each meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals). A distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass passes information between these components, using its working memory (context) as the vehicle. Let me break down how it works piece by piece.\nLayer 1: Episodic Memory This piece of the architecture is the most straightforward. To enable the model to remember specific experiences, every interaction is logged as an episode. Every episode captures the user prompt, agent response, and some metadata for memory management. It looks something like this:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-01T14:30:00\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;Can you help me write a README for this project?\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;Sure. Here\u0026#39;s a concise template...\u0026#34;, \u0026#34;access_count\u0026#34;: 3, \u0026#34;last_accessed\u0026#34;: \u0026#34;2025-12-03T10:15:00\u0026#34;, \u0026#34;is_protected\u0026#34;: false } To enable the model to retrieve relevant episodes at inference time, I utilize a technique known as retrieval-augmented generation (RAG). Simply put, I store the episode data in a vector store called ChromaDB. Embeddings are generated by sentence-transformers after every conversation. Every time I send a message, KenLM embeds my query and searches for the top-k most semantically similar episodes in its memory. This allows it to reference episodes related to my query, so if I ask about sports, it will pull up discussions where we talked about sports. If I ask about documentation, it will remember episodes where we talked about writing documentation.\nThis gives the model something resembling episodic recall: the ability to remember relevant experiences and bring them into working memory (the context window) when they\u0026rsquo;re useful.\nLayer 2: Reflective Memory Episodic memory alone would essentially be just a fancy search engine. Something that humans do that enables more complex learning is reflect on past episodes and draw generalized conclusions based on them. Taking inspiration from this process, I built an agentic reflection process into KenLM.\nWhen I exit a chat session, KenLM runs a reflection loop. A KenLM \u0026ldquo;reflection agent\u0026rdquo; analyzes the session\u0026rsquo;s episodes, searches for related patterns in past conversations, and updates a profile of hypotheses and learning goals.\nHere\u0026rsquo;s what the reflection system actually does:\nReviews the session — What patterns appeared in my behavior? What did I ask about? How did I respond to KenLM\u0026rsquo;s answers?\nSearches for evidence — The reflection agent can call a search_episodes(query) tool to find supporting patterns across all past conversations. This prevents overfitting to a single session.\nUpdates hypotheses — For each insight, it calls upsert_hypothesis() with a confidence score. Crucially, confidence only increases when patterns repeat across multiple episodes.\nMaintains learning goals — If the model notices something it doesn\u0026rsquo;t understand about me yet, it creates a learning goal—a question to explore in future interactions.\nHere\u0026rsquo;s what my actual profile looks like after some use:\n{ \u0026#34;hypotheses\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_007\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;accuracy\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth is detail-oriented and expects accurate and precise information.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.6, \u0026#34;evidence_episode_ids\u0026#34;: [ 90, 91, 89, 92 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_008\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;transparency\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth values transparency and honesty in communication.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 89 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_009\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;task_compliance\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth expects tasks to be completed accurately and without unnecessary steps.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 154, 155, 156, 157, 158 ] } ], \u0026#34;learning_goals\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;learning_goal_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;How can we ensure that Kenneth\u0026#39;s instructions are followed without unnecessary repetition or recapitulation?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;closed\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-11-30T15:10:55.704097\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;lg_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;What are Kenneth\u0026#39;s specific expectations for the precision and detail in project updates?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-12-02T15:51:56.904564\u0026#34; } ] } The reflection prompt is designed to be conservative. I explicitly instruct the model to start hypotheses with low confidence (0.2–0.4) and only raise confidence when it sees repeated evidence. This should hopefully prevent the kind of overconfident conclusions that can come from a single unusual interaction.\nLayer 3: The Forgetting System A pretty important piece of learning is actually the process by which we decide which pieces of information aren\u0026rsquo;t relevant. If we were to allow our episodic vector database to grow unbounded with every user interaction, it would quickly become slow and stop being useful during inference or reflection. Moreover, humans also don\u0026rsquo;t retain every bit of information that we are exposed to—we have complex systems that decide what information is worth retaining based on various factors.\nMy algorithm is a bit more rudimentary than the process that the human brain has, but it seems to be working fine so far. It essentially calculates an \u0026ldquo;importance\u0026rdquo; score for every memory, which is just a weighted sum of a couple factors:\nimportance = w1 * log(1 + access_count) # Access frequency + w2 * is_protected # Protection bonus + w3 * is_evidence # Evidence bonus + w4 * exp(-age_days / tau) # Recency decay This system assigns a high score to frequently accessed memories, memories that the AI has decided it wants to \u0026ldquo;protect,\u0026rdquo; memories that are evidence for hypotheses that it has reasoned about, and memories that are recent. Memories that are old, infrequently accessed, and not useful for any of the learning goals or hypotheses the AI has created become candidates for forgetting.\nThe forgetting process is actually better understood as a summarization process. Instead of just deleting irrelevant memories, the system compacts them. It does this by summarizing \u0026ldquo;forgotten\u0026rdquo; episodes into condensed summaries and deleting the originals. This preserves knowledge while bounding storage growth—similar to how human memories become \u0026ldquo;fuzzy\u0026rdquo; over time.\nHow It Works End-to-End Let me walk through a complete cycle to show how these pieces fit together.\nStep 1: Message Received: I type something like: \u0026ldquo;Can you write me a short essay about my favorite sports team?\u0026rdquo;\nStep 2: RAG Retrieval: KenLM embeds my query and searches the episodic index. It finds 3 relevant past conversations about sports and essay preferences.\nStep 3: Context Assembly: The system prompt is built from:\nCore identity document (who KenLM is, its purpose) Current profile (hypotheses + learning goals) RAG episodes (the relevant past conversations) Behavioral instructions (meta-prompt generated at session start) Step 4: Deliberation: KenLM plans its approach: \u0026ldquo;Kenneth wants an essay about his favorite sports team. Based on my hypothesis about preferring conciseness, I should provide a short essay without excessive explanation. Based on our past conversations, his favorite sports team is the Seattle Sounders. I don\u0026rsquo;t need any external tools for this.\u0026rdquo;\nStep 5: Response Generation: KenLM generates a response, informed by the plan and all the context. The response is concise and focused—because that\u0026rsquo;s what it\u0026rsquo;s learned that I prefer.\nStep 6: Episode Logging: The conversation is saved as a new episode and indexed in the vector store.\nStep 7: Session End → Reflection: When I exit the chat, the reflection agent wakes up. It reviews the session, searches for patterns, and decides whether to update any hypotheses or learning goals.\nStep 8: Next Session → Meta-Prompting: The next time I start a chat, KenLM reads the (potentially updated) profile and generates fresh behavioral instructions. Then, the cycle continues.\nResults and Observations After using this system for a little bit, here are some of the results I\u0026rsquo;ve observed:\nWhat\u0026rsquo;s working Certain hypotheses stick very well. For example, early on, I found myself repeatedly asking KenLM to \u0026ldquo;be more concise\u0026rdquo; and to stop repeating itself. The reflection system noticed this pattern, formed a hypothesis, and now the model defaults to shorter responses. I rarely have to give feedback about the length of its responses anymore.\nRAG retrieval is surprisingly useful. When I ask about something I discussed in a past conversation, relevant context often appears in the model\u0026rsquo;s awareness. It doesn\u0026rsquo;t always explicitly reference the past conversation, but you can tell it\u0026rsquo;s informed by it. I can refer to things like \u0026ldquo;my favorite sports team\u0026rdquo; or \u0026ldquo;where I live\u0026rdquo; and the model remembers what those things are as they relate to me.\nMeta-prompting effectively influences personality. Each session feels more like what I\u0026rsquo;m looking for from a conversation with an LLM. It\u0026rsquo;s dropped the overly sycophantic tendencies, phrases like \u0026ldquo;Certainly!\u0026rdquo; or \u0026ldquo;You\u0026rsquo;re absolutely right!\u0026rdquo;, and other LLM-isms that have annoyed me. The behavioral instructions provide a stable \u0026ldquo;personality\u0026rdquo; for that session, and that personality is consistently adapting to my preferences.\nWhat\u0026rsquo;s Challenging Certain habits are hard to break. The model really loves markdown. I mean, it really loves outputting markdown formatting. No matter how many times I tell it it\u0026rsquo;s a command line tool and markdown is not helpful, it still really loves outputting neat markdown responses. Now, it HAS gotten a little better after much feedback, but I anticipate its preference for markdown is baked into the model weights itself, and not so easy to override through prompting. This is one of the clearest examples I\u0026rsquo;ve found so far of the limitations of this learning system.\nThe 7B model has limitations. My base model is a fine-tuned Qwen 2.5 7B. It\u0026rsquo;s good, but it occasionally misunderstands, hallucinates, or generates malformed tool calls. A larger model would likely perform better, but I wanted something that runs locally on my MacBook.\nThis process adds significant latency to the chat process. The RAG retrieval, meta-prompting, and reflection processes all take time. Using a model that\u0026rsquo;s running on my laptop means it can feel painfully slow at times.\nWhat this means This was a fun experiment, but what does this mean? Is there any use for systems like these in industry?\nI\u0026rsquo;m far from an AI expert, but off the top of my head I can think of a couple potential applications of systems like these to develop real AI systems that solve real-world problems. Giving an LLM the ability to learn from experience opens the door to all kinds of systems we don’t really have today.\nThe obvious application is personalization: assistants that remember your preferences and workflows, which is what KenLM is. However, I think the implications go beyond “your AI knows your favorite sports team.” For example, imagine an educational model that can learn how you learn and what you know, remember your strong and weak areas, and adapt its explanations to your thinking style. This model could act as a more helpful learning companion than a model with no such memory capabilities.\nAnother application I can think of is creating models that improve themselves by optimizing for factors outside of human interaction. Imagine a model that initially struggles with chain-of-thought reasoning or tool usage—say something that consistently runs into the same error when trying to write python code or execute a web search. Currently, AI systems have no way to notice these things and improve. Human engineers need to monitor and step in, adjusting prompts, doing reinforcement learning, or collecting data with which to fine-tune in order to eke out a little bit better performance.\nA memory-equipped model changes this dynamic. For anything it\u0026rsquo;s trying to optimize for, be that reasoning or tool use, it can:\nnotice failed patterns by reasoning about them and remembering past experiences see successful patterns internalize the better method and adjust for future tasks Instead of updating billions of weights, the model updates a small set of learned strategies derived from lived experience. Over time, the model becomes a better problem solver not because it grew larger, but because it learned.\nThis all can potentially allow for smaller, more efficient models to rapidly adapt to specific use cases, reducing the industry\u0026rsquo;s reliance on frontier, compute-heavy models and allowing for more innovation and experimentation at the edge. Instead of treating model improvement as something that can only happen upstream, by massive labs with huge quantities of (often stolen) data, you get a world where models improve downstream, close to the user, close to the task, and close to the environment they operate in.\nIn that world, you don’t need a 400B-parameter model for everything. You need a reasonably capable base model with the right scaffolding: a memory system, a reflection system, and a mechanism for gradually internalizing the lessons it learns. Given enough experience, my intuition is that a modest model can outperform a much larger one on the tasks it specializes for, because it has context, history, and self-derived strategies the bigger model simply doesn’t have.\nIn the end, this really was just a weekend experiment, nothing more than an excuse to poke at a problem that’s been stuck in my head. I do, however, think that there are probably real ideas worth exploring here. I don’t know exactly where it will go yet, but I’m excited to keep experimenting and see how far a small model can go when you let it learn from its own experiences.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e\n\u003cp\u003eHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As \u003ca href=\"https://abehrouz.github.io/files/NL.pdf\"\u003ethis paper\u003c/a\u003e from Google points out, LLMs lack \u003cstrong\u003eneuroplasticity\u003c/strong\u003e—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes \u003cem\u003efrozen\u003c/em\u003e. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes we\u0026rsquo;re trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge from short-term to long-term storage, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights). All the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions). Each meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals). A distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass passes information between these components, using its working memory (context) as the vehicle. Let me break down how it works piece by piece.\nLayer 1: Episodic Memory This piece of the architecture is the most straightforward. To enable the model to remember specific experiences, every interaction is logged as an episode. Every episode captures the user prompt, agent response, and some metadata for memory management. It looks something like this:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-01T14:30:00\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;Can you help me write a README for this project?\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;Sure. Here\u0026#39;s a concise template...\u0026#34;, \u0026#34;access_count\u0026#34;: 3, \u0026#34;last_accessed\u0026#34;: \u0026#34;2025-12-03T10:15:00\u0026#34;, \u0026#34;is_protected\u0026#34;: false } To enable the model to retrieve relevant episodes at inference time, I utilize a technique known as retrieval-augmented generation (RAG). Simply put, I store the episode data in a vector store called ChromaDB. Embeddings are generated by sentence-transformers after every conversation. Every time I send a message, KenLM embeds my query and searches for the top-k most semantically similar episodes in its memory. This allows it to reference episodes related to my query, so if I ask about sports, it will pull up discussions where we talked about sports. If I ask about documentation, it will remember episodes where we talked about writing documentation.\nThis gives the model something resembling episodic recall: the ability to remember relevant experiences and bring them into working memory (the context window) when they\u0026rsquo;re useful.\nLayer 2: Reflective Memory Episodic memory alone would essentially be just a fancy search engine. Something that humans do that enables more complex learning is reflect on past episodes and draw generalized conclusions based on them. Taking inspiration from this process, I built an agentic reflection process into KenLM.\nWhen I exit a chat session, KenLM runs a reflection loop. A KenLM \u0026ldquo;reflection agent\u0026rdquo; analyzes the session\u0026rsquo;s episodes, searches for related patterns in past conversations, and updates a profile of hypotheses and learning goals.\nHere\u0026rsquo;s what the reflection system actually does:\nReviews the session — What patterns appeared in my behavior? What did I ask about? How did I respond to KenLM\u0026rsquo;s answers?\nSearches for evidence — The reflection agent can call a search_episodes(query) tool to find supporting patterns across all past conversations. This prevents overfitting to a single session.\nUpdates hypotheses — For each insight, it calls upsert_hypothesis() with a confidence score. Crucially, confidence only increases when patterns repeat across multiple episodes.\nMaintains learning goals — If the model notices something it doesn\u0026rsquo;t understand about me yet, it creates a learning goal—a question to explore in future interactions.\nHere\u0026rsquo;s what my actual profile looks like after some use:\n{ \u0026#34;hypotheses\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_007\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;accuracy\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth is detail-oriented and expects accurate and precise information.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.6, \u0026#34;evidence_episode_ids\u0026#34;: [ 90, 91, 89, 92 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_008\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;transparency\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth values transparency and honesty in communication.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 89 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_009\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;task_compliance\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth expects tasks to be completed accurately and without unnecessary steps.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 154, 155, 156, 157, 158 ] } ], \u0026#34;learning_goals\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;learning_goal_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;How can we ensure that Kenneth\u0026#39;s instructions are followed without unnecessary repetition or recapitulation?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;closed\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-11-30T15:10:55.704097\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;lg_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;What are Kenneth\u0026#39;s specific expectations for the precision and detail in project updates?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-12-02T15:51:56.904564\u0026#34; } ] } The reflection prompt is designed to be conservative. I explicitly instruct the model to start hypotheses with low confidence (0.2–0.4) and only raise confidence when it sees repeated evidence. This should hopefully prevent the kind of overconfident conclusions that can come from a single unusual interaction.\nLayer 3: The Forgetting System A pretty important piece of learning is actually the process by which we decide which pieces of information aren\u0026rsquo;t relevant. If we were to allow our episodic vector database to grow unbounded with every user interaction, it would quickly become slow and stop being useful during inference or reflection. Moreover, humans also don\u0026rsquo;t retain every bit of information that we are exposed to—we have complex systems that decide what information is worth retaining based on various factors.\nMy algorithm is a bit more rudimentary than the process that the human brain has, but it seems to be working fine so far. It essentially calculates an \u0026ldquo;importance\u0026rdquo; score for every memory, which is just a weighted sum of a couple factors:\nimportance = w1 * log(1 + access_count) # Access frequency + w2 * is_protected # Protection bonus + w3 * is_evidence # Evidence bonus + w4 * exp(-age_days / tau) # Recency decay This system assigns a high score to frequently accessed memories, memories that the AI has decided it wants to \u0026ldquo;protect,\u0026rdquo; memories that are evidence for hypotheses that it has reasoned about, and memories that are recent. Memories that are old, infrequently accessed, and not useful for any of the learning goals or hypotheses the AI has created become candidates for forgetting.\nThe forgetting process is actually better understood as a summarization process. Instead of just deleting irrelevant memories, the system compacts them. It does this by summarizing \u0026ldquo;forgotten\u0026rdquo; episodes into condensed summaries and deleting the originals. This preserves knowledge while bounding storage growth—similar to how human memories become \u0026ldquo;fuzzy\u0026rdquo; over time.\nHow It Works End-to-End Let me walk through a complete cycle to show how these pieces fit together.\nStep 1: Message Received: I type something like: \u0026ldquo;Can you write me a short essay about my favorite sports team?\u0026rdquo;\nStep 2: RAG Retrieval: KenLM embeds my query and searches the episodic index. It finds 3 relevant past conversations about sports and essay preferences.\nStep 3: Context Assembly: The system prompt is built from:\nCore identity document (who KenLM is, its purpose) Current profile (hypotheses + learning goals) RAG episodes (the relevant past conversations) Behavioral instructions (meta-prompt generated at session start) Step 4: Deliberation: KenLM plans its approach: \u0026ldquo;Kenneth wants an essay about his favorite sports team. Based on my hypothesis about preferring conciseness, I should provide a short essay without excessive explanation. Based on our past conversations, his favorite sports team is the Seattle Sounders. I don\u0026rsquo;t need any external tools for this.\u0026rdquo;\nStep 5: Response Generation: KenLM generates a response, informed by the plan and all the context. The response is concise and focused—because that\u0026rsquo;s what it\u0026rsquo;s learned that I prefer.\nStep 6: Episode Logging: The conversation is saved as a new episode and indexed in the vector store.\nStep 7: Session End → Reflection: When I exit the chat, the reflection agent wakes up. It reviews the session, searches for patterns, and decides whether to update any hypotheses or learning goals.\nStep 8: Next Session → Meta-Prompting: The next time I start a chat, KenLM reads the (potentially updated) profile and generates fresh behavioral instructions. Then, the cycle continues.\nResults and Observations After using this system for a little bit, here are some of the results I\u0026rsquo;ve observed:\nWhat\u0026rsquo;s working Certain hypotheses stick very well. For example, early on, I found myself repeatedly asking KenLM to \u0026ldquo;be more concise\u0026rdquo; and to stop repeating itself. The reflection system noticed this pattern, formed a hypothesis, and now the model defaults to shorter responses. I rarely have to give feedback about the length of its responses anymore.\nRAG retrieval is surprisingly useful. When I ask about something I discussed in a past conversation, relevant context often appears in the model\u0026rsquo;s awareness. It doesn\u0026rsquo;t always explicitly reference the past conversation, but you can tell it\u0026rsquo;s informed by it. I can refer to things like \u0026ldquo;my favorite sports team\u0026rdquo; or \u0026ldquo;where I live\u0026rdquo; and the model remembers what those things are as they relate to me.\nMeta-prompting effectively influences personality. Each session feels more like what I\u0026rsquo;m looking for from a conversation with an LLM. It\u0026rsquo;s dropped the overly sycophantic tendencies, phrases like \u0026ldquo;Certainly!\u0026rdquo; or \u0026ldquo;You\u0026rsquo;re absolutely right!\u0026rdquo;, and other LLM-isms that have annoyed me. The behavioral instructions provide a stable \u0026ldquo;personality\u0026rdquo; for that session, and that personality is consistently adapting to my preferences.\nWhat\u0026rsquo;s Challenging Certain habits are hard to break. The model really loves markdown. I mean, it really loves outputting markdown formatting. No matter how many times I tell it it\u0026rsquo;s a command line tool and markdown is not helpful, it still really loves outputting neat markdown responses. Now, it HAS gotten a little better after much feedback, but I anticipate its preference for markdown is baked into the model weights itself, and not so easy to override through prompting. This is one of the clearest examples I\u0026rsquo;ve found so far of the limitations of this learning system.\nThe 7B model has limitations. My base KenLM model is a fine-tuned Qwen 2.5 7B. It\u0026rsquo;s good, but it occasionally misunderstands, hallucinates, or generates malformed tool calls. A larger model would likely perform better, but I wanted something that runs locally on my MacBook.\nThis process adds significant latency to the chat process. The RAG retrieval, meta-prompting, and reflection processes all take time. Using a model that\u0026rsquo;s running on my laptop means it can feel painfully slow at times.\nWhat this means This was a fun experiment, but what does this mean? Is there any use for systems like these in industry?\nI\u0026rsquo;m far from an AI expert, but off the top of my head I can think of a couple potential applications of systems like these to develop real AI systems that solve real-world problems. Giving an LLM the ability to learn from experience opens the door to all kinds of systems we don’t really have today.\nThe obvious application is personalization: assistants that remember your preferences and workflows, which is what KenLM is. However, I think the implications go beyond “your AI knows your favorite sports team.” For example, imagine an educational model that can learn how you learn and what you know, remember your strong and weak areas, and adapt its explanations to your thinking style. This model could act as a more helpful learning companion than a model with no such memory capabilities.\nAnother application I can think of is creating models that improve themselves by optimizing for factors outside of human interaction. Imagine a model that initially struggles with chain-of-thought reasoning or tool usage—say something that consistently runs into the same error when trying to write python code or execute a web search. Currently, AI systems have no way to notice these things and improve. Human engineers need to monitor and step in, adjusting prompts, doing reinforcement learning, or collecting data with which to fine-tune in order to eke out a little bit better performance.\nA memory-equipped model changes this dynamic. For anything it\u0026rsquo;s trying to optimize for, be that reasoning or tool use, it can:\nnotice failed patterns by reasoning about them and remembering past experiences see successful patterns internalize the better method and adjust for future tasks Instead of updating billions of weights, the model updates a small set of learned strategies derived from lived experience. Over time, the model becomes a better problem solver not because it grew larger, but because it learned.\nThis all can potentially allow for smaller, more efficient models to rapidly adapt to specific use cases, reducing the industry\u0026rsquo;s reliance on frontier, compute-heavy models and allowing for more innovation and experimentation at the edge. Instead of treating model improvement as something that can only happen upstream, by massive labs with huge quantities of (often stolen) data, you get a world where models improve downstream, close to the user, close to the task, and close to the environment they operate in.\nIn that world, you don’t need a 400B-parameter model for everything. You need a reasonably capable base model with the right scaffolding: a memory system, a reflection system, and a mechanism for gradually internalizing the lessons it learns. Given enough experience, my intuition is that a modest model can outperform a much larger one on the tasks it specializes for, because it has context, history, and self-derived strategies the bigger model simply doesn’t have.\nIn the end, this really was just a weekend experiment, nothing more than an excuse to poke at a problem that’s been stuck in my head. I do, however, think that there are probably real ideas worth exploring here. I don’t know exactly where it will go yet, but I’m excited to keep experimenting and see how far a small model can go when you let it learn from its own experiences.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e\n\u003cp\u003eHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As \u003ca href=\"https://abehrouz.github.io/files/NL.pdf\"\u003ethis paper\u003c/a\u003e from Google points out, LLMs lack \u003cstrong\u003eneuroplasticity\u003c/strong\u003e—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes \u003cem\u003efrozen\u003c/em\u003e. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes we\u0026rsquo;re trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge from short-term to long-term storage, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights). All the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions). Each meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals). A distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass passes information between these components, using its working memory (context) as the vehicle. Let me break down how it works piece by piece.\nLayer 1: Episodic Memory This piece of the architecture is the most straightforward. To enable the model to remember specific experiences, every interaction is logged as an episode. Every episode captures the user prompt, agent response, and some metadata for memory management. It looks something like this:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-01T14:30:00\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;Can you help me write a README for this project?\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;Sure. Here\u0026#39;s a concise template...\u0026#34;, \u0026#34;access_count\u0026#34;: 3, \u0026#34;last_accessed\u0026#34;: \u0026#34;2025-12-03T10:15:00\u0026#34;, \u0026#34;is_protected\u0026#34;: false } To enable the model to retrieve relevant episodes at inference time, I utilize a technique known as retrieval-augmented generation (RAG). Simply put, I store the episode data in a vector store called ChromaDB. Embeddings are generated by sentence-transformers after every conversation. Every time I send a message, KenLM embeds my query and searches for the top-k most semantically similar episodes in its memory. This allows it to reference episodes related to my query, so if I ask about sports, it will pull up discussions where we talked about sports. If I ask about documentation, it will remember episodes where we talked about writing documentation.\nThis gives the model something resembling episodic recall: the ability to remember relevant experiences and bring them into working memory (the context window) when they\u0026rsquo;re useful.\nLayer 2: Reflective Memory Episodic memory alone would essentially be just a fancy search engine. Something that humans do that enables more complex learning is reflect on past episodes and draw generalized conclusions based on them. Taking inspiration from this process, I built an agentic reflection process into KenLM.\nWhen I exit a chat session, KenLM runs a reflection loop. A KenLM \u0026ldquo;reflection agent\u0026rdquo; analyzes the session\u0026rsquo;s episodes, searches for related patterns in past conversations, and updates a profile of hypotheses and learning goals.\nHere\u0026rsquo;s what the reflection system actually does:\nReviews the session — What patterns appeared in my behavior? What did I ask about? How did I respond to KenLM\u0026rsquo;s answers?\nSearches for evidence — The reflection agent can call a search_episodes(query) tool to find supporting patterns across all past conversations. This prevents overfitting to a single session.\nUpdates hypotheses — For each insight, it calls upsert_hypothesis() with a confidence score. Crucially, confidence only increases when patterns repeat across multiple episodes.\nMaintains learning goals — If the model notices something it doesn\u0026rsquo;t understand about me yet, it creates a learning goal—a question to explore in future interactions.\nHere\u0026rsquo;s what my actual profile looks like after some use:\n{ \u0026#34;hypotheses\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_007\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;accuracy\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth is detail-oriented and expects accurate and precise information.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.6, \u0026#34;evidence_episode_ids\u0026#34;: [ 90, 91, 89, 92 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_008\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;transparency\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth values transparency and honesty in communication.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 89 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_009\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;task_compliance\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth expects tasks to be completed accurately and without unnecessary steps.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 154, 155, 156, 157, 158 ] } ], \u0026#34;learning_goals\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;learning_goal_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;How can we ensure that Kenneth\u0026#39;s instructions are followed without unnecessary repetition or recapitulation?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;closed\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-11-30T15:10:55.704097\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;lg_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;What are Kenneth\u0026#39;s specific expectations for the precision and detail in project updates?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-12-02T15:51:56.904564\u0026#34; } ] } The reflection prompt is designed to be conservative. I explicitly instruct the model to start hypotheses with low confidence (0.2–0.4) and only raise confidence when it sees repeated evidence. This should hopefully prevent the kind of overconfident conclusions that can come from a single unusual interaction.\nLayer 3: The Forgetting System A pretty important piece of learning is actually the process by which we decide which pieces of information aren\u0026rsquo;t relevant. If we were to allow our episodic vector database to grow unbounded with every user interaction, it would quickly become slow and stop being useful during inference or reflection. Moreover, humans also don\u0026rsquo;t retain every bit of information that we are exposed to—we have complex systems that decide what information is worth retaining based on various factors.\nMy algorithm is a bit more rudimentary than the process that the human brain has, but it seems to be working fine so far. It essentially calculates an \u0026ldquo;importance\u0026rdquo; score for every memory, which is just a weighted sum of a couple factors:\nimportance = w1 * log(1 + access_count) # Access frequency + w2 * is_protected # Protection bonus + w3 * is_evidence # Evidence bonus + w4 * exp(-age_days / tau) # Recency decay This system assigns a high score to frequently accessed memories, memories that the AI has decided it wants to \u0026ldquo;protect,\u0026rdquo; memories that are evidence for hypotheses that it has reasoned about, and memories that are recent. Memories that are old, infrequently accessed, and not useful for any of the learning goals or hypotheses the AI has created become candidates for forgetting.\nThe forgetting process is actually better understood as a summarization process. Instead of just deleting irrelevant memories, the system compacts them. It does this by summarizing \u0026ldquo;forgotten\u0026rdquo; episodes into condensed summaries and deleting the originals. This preserves knowledge while bounding storage growth—similar to how human memories become \u0026ldquo;fuzzy\u0026rdquo; over time.\nHow It Works End-to-End Let me walk through a complete cycle to show how these pieces fit together.\nStep 1: Message Received: I type something like: \u0026ldquo;Can you write me a short essay about my favorite sports team?\u0026rdquo;\nStep 2: RAG Retrieval: KenLM embeds my query and searches the episodic index. It finds 3 relevant past conversations about sports and essay preferences.\nStep 3: Context Assembly: The system prompt is built from:\nCore identity document (who KenLM is, its purpose) Current profile (hypotheses + learning goals) RAG episodes (the relevant past conversations) Behavioral instructions (meta-prompt generated at session start) Step 4: Deliberation: KenLM plans its approach: \u0026ldquo;Kenneth wants an essay about his favorite sports team. Based on my hypothesis about preferring conciseness, I should provide a short essay without excessive explanation. Based on our past conversations, his favorite sports team is the Seattle Sounders. I don\u0026rsquo;t need any external tools for this.\u0026rdquo;\nStep 5: Response Generation: KenLM generates a response, informed by the plan and all the context. The response is concise and focused—because that\u0026rsquo;s what it\u0026rsquo;s learned that I prefer.\nStep 6: Episode Logging: The conversation is saved as a new episode and indexed in the vector store.\nStep 7: Session End → Reflection: When I exit the chat, the reflection agent wakes up. It reviews the session, searches for patterns, and decides whether to update any hypotheses or learning goals.\nStep 8: Next Session → Meta-Prompting: The next time I start a chat, KenLM reads the (potentially updated) profile and generates fresh behavioral instructions. Then, the cycle continues.\nResults and Observations After using this system for a little bit, here are some of the results I\u0026rsquo;ve observed:\nWhat\u0026rsquo;s working Certain hypotheses stick very well. For example, early on, I found myself repeatedly asking KenLM to \u0026ldquo;be more concise\u0026rdquo; and to stop repeating itself. The reflection system noticed this pattern, formed a hypothesis, and now the model defaults to shorter responses. I rarely have to give feedback about the length of its responses anymore.\nRAG retrieval is surprisingly useful. When I ask about something I discussed in a past conversation, relevant context often appears in the model\u0026rsquo;s awareness. It doesn\u0026rsquo;t always explicitly reference the past conversation, but you can tell it\u0026rsquo;s informed by it. I can refer to things like \u0026ldquo;my favorite sports team\u0026rdquo; or \u0026ldquo;where I live\u0026rdquo; and the model remembers what those things are as they relate to me.\nMeta-prompting effectively influences personality. Each session feels more like what I\u0026rsquo;m looking for from a conversation with an LLM. It\u0026rsquo;s dropped the overly sycophantic tendencies, phrases like \u0026ldquo;Certainly!\u0026rdquo; or \u0026ldquo;You\u0026rsquo;re absolutely right!\u0026rdquo;, and other LLM-isms that have annoyed me. The behavioral instructions provide a stable \u0026ldquo;personality\u0026rdquo; for that session, and that personality is consistently adapting to my preferences.\nWhat\u0026rsquo;s Challenging Certain habits are hard to break. The model really loves markdown. I mean, it really loves outputting markdown formatting. No matter how many times I tell it it\u0026rsquo;s a command line tool and markdown is not helpful, it still really loves outputting neat markdown responses. Now, it HAS gotten a little better after much feedback, but I anticipate its preference for markdown is baked into the model weights itself, and not so easy to override through prompting. This is one of the clearest examples I\u0026rsquo;ve found so far of the limitations of this learning system.\nThe 7B model has limitations. My base KenLM model is a fine-tuned Qwen 2.5 7B. It\u0026rsquo;s good, but it occasionally misunderstands, hallucinates, or generates malformed tool calls. A larger model would likely perform better, but I wanted something that runs locally on my MacBook.\nThis process adds significant latency to the chat process. The RAG retrieval, meta-prompting, and reflection processes all take time. Using a model that\u0026rsquo;s running on my laptop means it can feel painfully slow at times.\nWhat this means This was a fun experiment, but what does this mean? Is there any use for systems like these in industry?\nI\u0026rsquo;m far from an expert, but off the top of my head I can think of a couple potential applications of systems like these to develop real AI systems that solve real-world problems. Giving an LLM the ability to learn from experience opens the door to all kinds of systems we don’t really have today.\nThe obvious application is personalization: assistants that remember your preferences and workflows, which is what KenLM is. However, I think the implications go beyond “your AI knows your favorite sports team.” For example, imagine an educational model that can learn how you learn and what you know, remember your strong and weak areas, and adapt its explanations to your thinking style. This model could act as a more helpful learning companion than a model with no such memory capabilities.\nAnother application I can think of is creating models that improve themselves by optimizing for factors outside of human interaction. Imagine a model that initially struggles with chain-of-thought reasoning or tool usage—say something that consistently runs into the same error when trying to write python code or execute a web search. Currently, AI systems have no way to notice these things and improve. Human engineers need to monitor and step in, adjusting prompts, doing reinforcement learning, or collecting data with which to fine-tune in order to eke out a little bit better performance.\nA memory-equipped model changes this dynamic. For anything it\u0026rsquo;s trying to optimize for, be that reasoning or tool use, it can:\nnotice failed patterns by reasoning about them and remembering past experiences see successful patterns internalize the better method and adjust for future tasks Instead of updating billions of weights, the model updates a small set of learned strategies derived from lived experience. Over time, the model becomes a better problem solver not because it grew larger, but because it learned.\nThis all can potentially allow for smaller, more efficient models to rapidly adapt to specific use cases, reducing the industry\u0026rsquo;s reliance on frontier, compute-heavy models and allowing for more innovation and experimentation at the edge. Instead of treating model improvement as something that can only happen upstream, by massive labs with huge quantities of (often stolen) data, you get a world where models improve downstream, close to the user, close to the task, and close to the environment they operate in.\nIn that world, you don’t need a 400B-parameter model for everything. You need a reasonably capable base model with the right scaffolding: a memory system, a reflection system, and a mechanism for gradually internalizing the lessons it learns. Given enough experience, my intuition is that a modest model can outperform a much larger one on the tasks it specializes for, because it has context, history, and self-derived strategies the bigger model simply doesn’t have.\nIn the end, this really was just a weekend experiment, nothing more than an excuse to poke at a problem that’s been stuck in my head. I do, however, think that there are probably real ideas worth exploring here. I don’t know exactly where it will go yet, but I’m excited to keep experimenting and see how far a small model can go when you let it learn from its own experiences.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e\n\u003cp\u003eHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As \u003ca href=\"https://abehrouz.github.io/files/NL.pdf\"\u003ethis paper\u003c/a\u003e from Google points out, LLMs lack \u003cstrong\u003eneuroplasticity\u003c/strong\u003e—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes \u003cem\u003efrozen\u003c/em\u003e. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes we\u0026rsquo;re trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge from short-term to long-term storage, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights). All the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions). Each meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals). A distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass passes information between these components, using its working memory (context) as the vehicle. Let me break down how it works piece by piece.\nLayer 1: Episodic Memory This piece of the architecture is the most straightforward. To enable the model to remember specific experiences, every interaction is logged as an episode. Every episode captures the user prompt, agent response, and some metadata for memory management. It looks something like this:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-01T14:30:00\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;Can you help me write a README for this project?\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;Sure. Here\u0026#39;s a concise template...\u0026#34;, \u0026#34;access_count\u0026#34;: 3, \u0026#34;last_accessed\u0026#34;: \u0026#34;2025-12-03T10:15:00\u0026#34;, \u0026#34;is_protected\u0026#34;: false } To enable the model to retrieve relevant episodes at inference time, I utilize a technique known as retrieval-augmented generation (RAG). Simply put, I store the episode data in a vector store called ChromaDB. Embeddings are generated by sentence-transformers after every conversation. Every time I send a message, KenLM embeds my query and searches for the top-k most semantically similar episodes in its memory. This allows it to reference episodes related to my query, so if I ask about sports, it will pull up discussions where we talked about sports. If I ask about documentation, it will remember episodes where we talked about writing documentation.\nThis gives the model something resembling episodic recall: the ability to remember relevant experiences and bring them into working memory (the context window) when they\u0026rsquo;re useful.\nLayer 2: Reflective Memory Episodic memory alone would essentially be just a fancy search engine. Something that humans do that enables more complex learning is reflect on past episodes and draw generalized conclusions based on them. Taking inspiration from this process, I built an agentic reflection process into KenLM.\nWhen I exit a chat session, KenLM runs a reflection loop. A KenLM \u0026ldquo;reflection agent\u0026rdquo; analyzes the session\u0026rsquo;s episodes, searches for related patterns in past conversations, and updates a profile of hypotheses and learning goals.\nHere\u0026rsquo;s what the reflection system actually does:\nReviews the session — What patterns appeared in my behavior? What did I ask about? How did I respond to KenLM\u0026rsquo;s answers?\nSearches for evidence — The reflection agent can call a search_episodes(query) tool to find supporting patterns across all past conversations. This prevents overfitting to a single session.\nUpdates hypotheses — For each insight, it calls upsert_hypothesis() with a confidence score. Crucially, confidence only increases when patterns repeat across multiple episodes.\nMaintains learning goals — If the model notices something it doesn\u0026rsquo;t understand about me yet, it creates a learning goal—a question to explore in future interactions.\nHere\u0026rsquo;s what my actual profile looks like after some use:\n{ \u0026#34;hypotheses\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_007\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;accuracy\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth is detail-oriented and expects accurate and precise information.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.6, \u0026#34;evidence_episode_ids\u0026#34;: [ 90, 91, 89, 92 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_008\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;transparency\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth values transparency and honesty in communication.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 89 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_009\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;task_compliance\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth expects tasks to be completed accurately and without unnecessary steps.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 154, 155, 156, 157, 158 ] } ], \u0026#34;learning_goals\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;learning_goal_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;How can we ensure that Kenneth\u0026#39;s instructions are followed without unnecessary repetition or recapitulation?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;closed\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-11-30T15:10:55.704097\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;lg_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;What are Kenneth\u0026#39;s specific expectations for the precision and detail in project updates?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-12-02T15:51:56.904564\u0026#34; } ] } The reflection prompt is designed to be conservative. I explicitly instruct the model to start hypotheses with low confidence (0.2–0.4) and only raise confidence when it sees repeated evidence. This should hopefully prevent the kind of overconfident conclusions that can come from a single unusual interaction.\nLayer 3: The Forgetting System A pretty important piece of learning is actually the process by which we decide which pieces of information aren\u0026rsquo;t relevant. If we were to allow our episodic vector database to grow unbounded with every user interaction, it would quickly become slow and stop being useful during inference or reflection. Moreover, humans also don\u0026rsquo;t retain every bit of information that we are exposed to—we have complex systems that decide what information is worth retaining based on various factors.\nMy algorithm is a bit more rudimentary than the process that the human brain has, but it seems to be working fine so far. It essentially calculates an \u0026ldquo;importance\u0026rdquo; score for every memory, which is just a weighted sum of a couple factors:\nimportance = w1 * log(1 + access_count) # Access frequency + w2 * is_protected # Protection bonus + w3 * is_evidence # Evidence bonus + w4 * exp(-age_days / tau) # Recency decay This system assigns a high score to frequently accessed memories, memories that the AI has decided it wants to \u0026ldquo;protect,\u0026rdquo; memories that are evidence for hypotheses that it has reasoned about, and memories that are recent. Memories that are old, infrequently accessed, and not useful for any of the learning goals or hypotheses the AI has created become candidates for forgetting.\nThe forgetting process is actually better understood as a summarization process. Instead of just deleting irrelevant memories, the system compacts them. It does this by summarizing \u0026ldquo;forgotten\u0026rdquo; episodes into condensed summaries and deleting the originals. This preserves knowledge while bounding storage growth—similar to how human memories become \u0026ldquo;fuzzy\u0026rdquo; over time.\nHow It Works End-to-End Let me walk through a complete cycle to show how these pieces fit together.\nStep 1: Message Received: I type something like: \u0026ldquo;Can you write me a short essay about my favorite sports team?\u0026rdquo;\nStep 2: RAG Retrieval: KenLM embeds my query and searches the episodic index. It finds 3 relevant past conversations about sports and essay preferences.\nStep 3: Context Assembly: The system prompt is built from:\nCore identity document (who KenLM is, its purpose) Current profile (hypotheses + learning goals) RAG episodes (the relevant past conversations) Behavioral instructions (meta-prompt generated at session start) Step 4: Deliberation: KenLM plans its approach: \u0026ldquo;Kenneth wants an essay about his favorite sports team. Based on my hypothesis about preferring conciseness, I should provide a short essay without excessive explanation. Based on our past conversations, his favorite sports team is the Seattle Sounders. I don\u0026rsquo;t need any external tools for this.\u0026rdquo;\nStep 5: Response Generation: KenLM generates a response, informed by the plan and all the context. The response is concise and focused—because that\u0026rsquo;s what it\u0026rsquo;s learned that I prefer.\nStep 6: Episode Logging: The conversation is saved as a new episode and indexed in the vector store.\nStep 7: Session End → Reflection: When I exit the chat, the reflection agent wakes up. It reviews the session, searches for patterns, and decides whether to update any hypotheses or learning goals.\nStep 8: Next Session → Meta-Prompting: The next time I start a chat, KenLM reads the (potentially updated) profile and generates fresh behavioral instructions. Then, the cycle continues.\nResults and Observations After using this system for a little bit, here are some of the results I\u0026rsquo;ve observed:\nWhat\u0026rsquo;s working Certain hypotheses stick very well. For example, early on, I found myself repeatedly asking KenLM to \u0026ldquo;be more concise\u0026rdquo; and to stop repeating itself. The reflection system noticed this pattern, formed a hypothesis, and now the model defaults to shorter responses. I rarely have to give feedback about the length of its responses anymore.\nRAG retrieval is surprisingly useful. When I ask about something I discussed in a past conversation, relevant context often appears in the model\u0026rsquo;s awareness. It doesn\u0026rsquo;t always explicitly reference the past conversation, but you can tell it\u0026rsquo;s informed by it. I can refer to things like \u0026ldquo;my favorite sports team\u0026rdquo; or \u0026ldquo;where I live\u0026rdquo; and the model remembers what those things are as they relate to me.\nMeta-prompting effectively influences personality. Each session feels more like what I\u0026rsquo;m looking for from a conversation with an LLM. It\u0026rsquo;s dropped the overly sycophantic tendencies, phrases like \u0026ldquo;Certainly!\u0026rdquo; or \u0026ldquo;You\u0026rsquo;re absolutely right!\u0026rdquo;, and other LLM-isms that have annoyed me. The behavioral instructions provide a stable \u0026ldquo;personality\u0026rdquo; for that session, and that personality is consistently adapting to my preferences.\nWhat\u0026rsquo;s Challenging Certain habits are hard to break. The model really loves markdown. I mean, it really loves outputting markdown formatting. No matter how many times I tell it it\u0026rsquo;s a command line tool and markdown is not helpful, it still really loves outputting neat markdown responses. Now, it HAS gotten a little better after much feedback, but I anticipate its preference for markdown is baked into the model weights itself, and not so easy to override through prompting. This is one of the clearest examples I\u0026rsquo;ve found so far of the limitations of this learning system.\nThe 7B model has limitations. My base KenLM model is a fine-tuned Qwen 2.5 7B. It\u0026rsquo;s good, but it occasionally misunderstands, hallucinates, or generates malformed tool calls. A larger model would likely perform better, but I wanted something that runs locally on my MacBook.\nThis process adds significant latency to the chat process. The RAG retrieval, meta-prompting, and reflection processes all take time. Using a model that\u0026rsquo;s running on my laptop means it can feel painfully slow at times.\nWhat this means This was a fun experiment, but what does this mean? Is there any use for systems like these in industry?\nI\u0026rsquo;m still far from an expert, but off the top of my head I can think of a couple potential applications of systems like these to develop real AI systems that solve real-world problems. Giving an LLM the ability to learn from experience opens the door to all kinds of systems we don’t really have today.\nThe obvious application is personalization: assistants that remember your preferences and workflows, which is what KenLM is. However, I think the implications go beyond “your AI knows your favorite sports team.” For example, imagine an educational model that can learn how you learn and what you know, remember your strong and weak areas, and adapt its explanations to your thinking style. This model could act as a more helpful learning companion than a model with no such memory capabilities.\nAnother application I can think of is creating models that improve themselves by optimizing for factors outside of human interaction. Imagine a model that initially struggles with chain-of-thought reasoning or tool usage—say something that consistently runs into the same error when trying to write python code or execute a web search. Currently, AI systems have no way to notice these things and improve. Human engineers need to monitor and step in, adjusting prompts, doing reinforcement learning, or collecting data with which to fine-tune in order to eke out a little bit better performance.\nA memory-equipped model changes this dynamic. For anything it\u0026rsquo;s trying to optimize for, be that reasoning or tool use, it can:\nnotice failed patterns by reasoning about them and remembering past experiences see successful patterns internalize the better method and adjust for future tasks Instead of updating billions of weights, the model updates a small set of learned strategies derived from lived experience. Over time, the model becomes a better problem solver not because it grew larger, but because it learned.\nThis all can potentially allow for smaller, more efficient models to rapidly adapt to specific use cases, reducing the industry\u0026rsquo;s reliance on frontier, compute-heavy models and allowing for more innovation and experimentation at the edge. Instead of treating model improvement as something that can only happen upstream, by massive labs with huge quantities of (often stolen) data, you get a world where models improve downstream, close to the user, close to the task, and close to the environment they operate in.\nIn that world, you don’t need a 400B-parameter model for everything. You need a reasonably capable base model with the right scaffolding: a memory system, a reflection system, and a mechanism for gradually internalizing the lessons it learns. Given enough experience, my intuition is that a modest model can outperform a much larger one on the tasks it specializes for, because it has context, history, and self-derived strategies the bigger model simply doesn’t have.\nIn the end, this really was just a weekend experiment, nothing more than an excuse to poke at a problem that’s been stuck in my head. I do, however, think that there are probably real ideas worth exploring here. I don’t know exactly where it will go yet, but I’m excited to keep experimenting and see how far a small model can go when you let it learn from its own experiences.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e\n\u003cp\u003eHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As \u003ca href=\"https://abehrouz.github.io/files/NL.pdf\"\u003ethis paper\u003c/a\u003e from Google points out, LLMs lack \u003cstrong\u003eneuroplasticity\u003c/strong\u003e—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes \u003cem\u003efrozen\u003c/em\u003e. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes we\u0026rsquo;re trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge from short-term to long-term storage, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights). All the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions). Each meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals). A distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass passes information between these components, using its working memory (context) as the vehicle. Let me break down how it works piece by piece.\nLayer 1: Episodic Memory This piece of the architecture is the most straightforward. To enable the model to remember specific experiences, every interaction is logged as an episode. Every episode captures the user prompt, agent response, and some metadata for memory management. It looks something like this:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-01T14:30:00\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;Can you help me write a README for this project?\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;Sure. Here\u0026#39;s a concise template...\u0026#34;, \u0026#34;access_count\u0026#34;: 3, \u0026#34;last_accessed\u0026#34;: \u0026#34;2025-12-03T10:15:00\u0026#34;, \u0026#34;is_protected\u0026#34;: false } To enable the model to retrieve relevant episodes at inference time, I utilize a technique known as retrieval-augmented generation (RAG). Simply put, I store the episode data in a vector store called ChromaDB. Embeddings are generated by sentence-transformers after every conversation. Every time I send a message, KenLM embeds my query and searches for the top-k most semantically similar episodes in its memory. This allows it to reference episodes related to my query, so if I ask about sports, it will pull up discussions where we talked about sports. If I ask about documentation, it will remember episodes where we talked about writing documentation.\nThis gives the model something resembling episodic recall: the ability to remember relevant experiences and bring them into working memory (the context window) when they\u0026rsquo;re useful.\nLayer 2: Reflective Memory Episodic memory alone would essentially be just a fancy search engine. Something that humans do that enables more complex learning is reflect on past episodes and draw generalized conclusions based on them. Taking inspiration from this process, I built an agentic reflection process into KenLM.\nWhen I exit a chat session, KenLM runs a reflection loop. A KenLM \u0026ldquo;reflection agent\u0026rdquo; analyzes the session\u0026rsquo;s episodes, searches for related patterns in past conversations, and updates a profile of hypotheses and learning goals.\nHere\u0026rsquo;s what the reflection system actually does:\nReviews the session — What patterns appeared in my behavior? What did I ask about? How did I respond to KenLM\u0026rsquo;s answers?\nSearches for evidence — The reflection agent can call a search_episodes(query) tool to find supporting patterns across all past conversations. This prevents overfitting to a single session.\nUpdates hypotheses — For each insight, it calls upsert_hypothesis() with a confidence score. Crucially, confidence only increases when patterns repeat across multiple episodes.\nMaintains learning goals — If the model notices something it doesn\u0026rsquo;t understand about me yet, it creates a learning goal—a question to explore in future interactions.\nHere\u0026rsquo;s what my actual profile looks like after some use:\n{ \u0026#34;hypotheses\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_007\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;accuracy\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth is detail-oriented and expects accurate and precise information.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.6, \u0026#34;evidence_episode_ids\u0026#34;: [ 90, 91, 89, 92 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_008\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;transparency\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth values transparency and honesty in communication.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 89 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_009\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;task_compliance\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth expects tasks to be completed accurately and without unnecessary steps.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 154, 155, 156, 157, 158 ] } ], \u0026#34;learning_goals\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;learning_goal_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;How can we ensure that Kenneth\u0026#39;s instructions are followed without unnecessary repetition or recapitulation?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;closed\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-11-30T15:10:55.704097\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;lg_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;What are Kenneth\u0026#39;s specific expectations for the precision and detail in project updates?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-12-02T15:51:56.904564\u0026#34; } ] } The reflection prompt is designed to be conservative. I explicitly instruct the model to start hypotheses with low confidence (0.2–0.4) and only raise confidence when it sees repeated evidence. This should hopefully prevent the kind of overconfident conclusions that can come from a single unusual interaction.\nLayer 3: The Forgetting System A pretty important piece of learning is actually the process by which we decide which pieces of information aren\u0026rsquo;t relevant. If we were to allow our episodic vector database to grow unbounded with every user interaction, it would quickly become slow and stop being useful during inference or reflection. Moreover, humans also don\u0026rsquo;t retain every bit of information that we are exposed to—we have complex systems that decide what information is worth retaining based on various factors.\nMy algorithm is a bit more rudimentary than the process that the human brain has, but it seems to be working fine so far. It essentially calculates an \u0026ldquo;importance\u0026rdquo; score for every memory, which is just a weighted sum of a couple factors:\nimportance = w1 * log(1 + access_count) # Access frequency + w2 * is_protected # Protection bonus + w3 * is_evidence # Evidence bonus + w4 * exp(-age_days / tau) # Recency decay This system assigns a high score to frequently accessed memories, memories that the AI has decided it wants to \u0026ldquo;protect,\u0026rdquo; memories that are evidence for hypotheses that it has reasoned about, and memories that are recent. Memories that are old, infrequently accessed, and not useful for any of the learning goals or hypotheses the AI has created become candidates for forgetting.\nThe forgetting process is actually better understood as a summarization process. Instead of just deleting irrelevant memories, the system compacts them. It does this by summarizing \u0026ldquo;forgotten\u0026rdquo; episodes into condensed summaries and deleting the originals. This preserves knowledge while bounding storage growth—similar to how human memories become \u0026ldquo;fuzzy\u0026rdquo; over time.\nHow It Works End-to-End Let me walk through a complete cycle to show how these pieces fit together.\nStep 1: Message Received: I type something like: \u0026ldquo;Can you write me a short essay about my favorite sports team?\u0026rdquo;\nStep 2: RAG Retrieval: KenLM embeds my query and searches the episodic index. It finds 3 relevant past conversations about sports and essay preferences.\nStep 3: Context Assembly: The system prompt is built from:\nCore identity document (who KenLM is, its purpose) Current profile (hypotheses + learning goals) RAG episodes (the relevant past conversations) Behavioral instructions (meta-prompt generated at session start) Step 4: Deliberation: KenLM plans its approach: \u0026ldquo;Kenneth wants an essay about his favorite sports team. Based on my hypothesis about preferring conciseness, I should provide a short essay without excessive explanation. Based on our past conversations, his favorite sports team is the Seattle Sounders. I don\u0026rsquo;t need any external tools for this.\u0026rdquo;\nStep 5: Response Generation: KenLM generates a response, informed by the plan and all the context. The response is concise and focused—because that\u0026rsquo;s what it\u0026rsquo;s learned that I prefer.\nStep 6: Episode Logging: The conversation is saved as a new episode and indexed in the vector store.\nStep 7: Session End → Reflection: When I exit the chat, the reflection agent wakes up. It reviews the session, searches for patterns, and decides whether to update any hypotheses or learning goals.\nStep 8: Next Session → Meta-Prompting: The next time I start a chat, KenLM reads the (potentially updated) profile and generates fresh behavioral instructions. Then, the cycle continues.\nResults and Observations After using this system for a little bit, here are some of the results I\u0026rsquo;ve observed:\nWhat\u0026rsquo;s working Certain hypotheses stick very well. For example, early on, I found myself repeatedly asking KenLM to \u0026ldquo;be more concise\u0026rdquo; and to stop repeating itself. The reflection system noticed this pattern, formed a hypothesis, and now the model defaults to shorter responses. I rarely have to give feedback about the length of its responses anymore.\nRAG retrieval is surprisingly useful. When I ask about something I discussed in a past conversation, relevant context often appears in the model\u0026rsquo;s awareness. It doesn\u0026rsquo;t always explicitly reference the past conversation, but you can tell it\u0026rsquo;s informed by it. I can refer to things like \u0026ldquo;my favorite sports team\u0026rdquo; or \u0026ldquo;where I live\u0026rdquo; and the model remembers what those things are as they relate to me.\nMeta-prompting effectively influences personality. Each session feels more like what I\u0026rsquo;m looking for from a conversation with an LLM. It\u0026rsquo;s dropped the overly sycophantic tendencies, phrases like \u0026ldquo;Certainly!\u0026rdquo; or \u0026ldquo;You\u0026rsquo;re absolutely right!\u0026rdquo;, and other LLM-isms that have annoyed me. The behavioral instructions provide a stable \u0026ldquo;personality\u0026rdquo; for that session, and that personality is consistently adapting to my preferences.\nWhat\u0026rsquo;s Challenging Certain habits are hard to break. The model really loves markdown. I mean, it really loves outputting markdown formatting. No matter how many times I tell it it\u0026rsquo;s a command line tool and markdown is not helpful, it still really loves outputting neat markdown responses. Now, it HAS gotten a little better after much feedback, but I anticipate its preference for markdown is baked into the model weights itself, and not so easy to override through prompting. This is one of the clearest examples I\u0026rsquo;ve found so far of the limitations of this learning system.\nThe 7B model has limitations. My base KenLM model is a fine-tuned Qwen 2.5 7B. It\u0026rsquo;s good, but it occasionally misunderstands, hallucinates, or generates malformed tool calls. A larger model would likely perform better, but I wanted something that runs locally on my MacBook.\nThis process adds significant latency to the chat process. The RAG retrieval, meta-prompting, and reflection processes all take time. Using a model that\u0026rsquo;s running on my laptop means it can feel painfully slow at times.\nWhat this means This was a fun experiment, but what does this mean? Is there any use for systems like these in industry?\nI\u0026rsquo;m still far from an expert, but off the top of my head I can think of a couple potential applications of systems like these to develop real AI systems that solve real-world problems. Giving an LLM the ability to learn from experience opens the door to all kinds of systems we don’t really have today.\nThe obvious application is personalization: assistants that remember your preferences and workflows, which is what KenLM is. However, I think the implications go beyond “your AI knows your favorite sports team.” For example, imagine an educational model that can learn how you learn and what you know, remember your strong and weak areas, and adapt its explanations to your thinking style. This model could act as a more helpful learning companion than a model with no such memory capabilities.\nAnother application I can think of is creating models that improve themselves by optimizing for factors outside of human interaction. Imagine a model that initially struggles with chain-of-thought reasoning or tool usage—say something that consistently runs into the same error when trying to write python code or execute a web search. Currently, AI systems have no way to notice these things and improve. Human engineers need to monitor and step in, adjusting prompts, doing reinforcement learning, or collecting data with which to fine-tune in order to eke out a little bit better performance.\nA memory-equipped model changes this dynamic. For anything it\u0026rsquo;s trying to optimize for, be that reasoning or tool use, it can:\nnotice failed patterns by reasoning about them and remembering past experiences see successful patterns internalize the better method and adjust for future tasks Instead of updating billions of weights, the model updates a small set of learned strategies derived from lived experience. Over time, the model becomes a better problem solver not because it grew larger, but because it learned.\nThis all can potentially allow for smaller, more efficient models to rapidly adapt to specific use cases, reducing the industry\u0026rsquo;s reliance on frontier, compute-heavy models and allowing for more innovation and experimentation at the edge. Instead of treating model improvement as something that can only happen upstream, by massive labs with huge quantities of (often stolen) data, you get a world where models improve downstream, close to the user, close to the task, and close to the environment they operate in.\nIn that world, you don’t need a 400B-parameter model for everything. You need a reasonably capable base model with the right scaffolding: a memory system, a reflection system, and a mechanism for gradually internalizing the lessons it learns. Given enough experience, my intuition is that a modest model can outperform a much larger one on the tasks it specializes for, because it has context, history, and self-derived strategies the bigger model simply doesn’t have.\nIn the end, this really was just a weekend experiment, nothing more than an excuse to poke at a problem that’s been stuck in my head. I do, however, think that there are probably real ideas worth exploring here. I don’t know exactly where it will go yet, but I’m excited to keep experimenting and seeing how far a small model can go when you let it learn from its own experiences.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e\n\u003cp\u003eHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As \u003ca href=\"https://abehrouz.github.io/files/NL.pdf\"\u003ethis paper\u003c/a\u003e from Google points out, LLMs lack \u003cstrong\u003eneuroplasticity\u003c/strong\u003e—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes \u003cem\u003efrozen\u003c/em\u003e. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes we\u0026rsquo;re trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge from short-term to long-term storage, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights). All the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions). Each meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals). A distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass passes information between these components, using its working memory (context) as the vehicle. Let me break down how it works piece by piece.\nLayer 1: Episodic Memory This piece of the architecture is the most straightforward. To enable the model to remember specific experiences, every interaction is logged as an episode. Every episode captures the user prompt, agent response, and some metadata for memory management. It looks something like this:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-01T14:30:00\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;Can you help me write a README for this project?\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;Sure. Here\u0026#39;s a concise template...\u0026#34;, \u0026#34;access_count\u0026#34;: 3, \u0026#34;last_accessed\u0026#34;: \u0026#34;2025-12-03T10:15:00\u0026#34;, \u0026#34;is_protected\u0026#34;: false } To enable the model to retrieve relevant episodes at inference time, I utilize a technique known as retrieval-augmented generation (RAG). Simply put, I store the episode data in a vector store called ChromaDB. Embeddings are generated by sentence-transformers after every conversation. Every time I send a message, KenLM embeds my query and searches for the top-k most semantically similar episodes in its memory. This allows it to reference episodes related to my query, so if I ask about sports, it will pull up discussions where we talked about sports. If I ask about documentation, it will remember episodes where we talked about writing documentation.\nThis gives the model something resembling episodic recall: the ability to remember relevant experiences and bring them into working memory (the context window) when they\u0026rsquo;re useful.\nLayer 2: Reflective Memory Episodic memory alone would essentially be just a fancy search engine. Something that humans do that enables more complex learning is reflect on past episodes and draw generalized conclusions based on them. Taking inspiration from this process, I built an agentic reflection process into KenLM.\nWhen I exit a chat session, KenLM runs a reflection loop. A KenLM \u0026ldquo;reflection agent\u0026rdquo; analyzes the session\u0026rsquo;s episodes, searches for related patterns in past conversations, and updates a profile of hypotheses and learning goals.\nHere\u0026rsquo;s what the reflection system actually does:\nReviews the session — What patterns appeared in my behavior? What did I ask about? How did I respond to KenLM\u0026rsquo;s answers?\nSearches for evidence — The reflection agent can call a search_episodes(query) tool to find supporting patterns across all past conversations. This prevents overfitting to a single session.\nUpdates hypotheses — For each insight, it calls upsert_hypothesis() with a confidence score. Crucially, confidence only increases when patterns repeat across multiple episodes.\nMaintains learning goals — If the model notices something it doesn\u0026rsquo;t understand about me yet, it creates a learning goal—a question to explore in future interactions.\nHere\u0026rsquo;s what my actual profile looks like after some use:\n{ \u0026#34;hypotheses\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_007\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;accuracy\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth is detail-oriented and expects accurate and precise information.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.6, \u0026#34;evidence_episode_ids\u0026#34;: [ 90, 91, 89, 92 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_008\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;transparency\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth values transparency and honesty in communication.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 89 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_009\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;task_compliance\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth expects tasks to be completed accurately and without unnecessary steps.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 154, 155, 156, 157, 158 ] } ], \u0026#34;learning_goals\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;learning_goal_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;How can we ensure that Kenneth\u0026#39;s instructions are followed without unnecessary repetition or recapitulation?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;closed\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-11-30T15:10:55.704097\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;lg_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;What are Kenneth\u0026#39;s specific expectations for the precision and detail in project updates?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-12-02T15:51:56.904564\u0026#34; } ] } The reflection prompt is designed to be conservative. I explicitly instruct the model to start hypotheses with low confidence (0.2–0.4) and only raise confidence when it sees repeated evidence. This should hopefully prevent the kind of overconfident conclusions that can come from a single unusual interaction.\nLayer 3: The Forgetting System A pretty important piece of learning is actually the process by which we decide which pieces of information aren\u0026rsquo;t relevant. If we were to allow our episodic vector database to grow unbounded with every user interaction, it would quickly become slow and stop being useful during inference or reflection. Moreover, humans also don\u0026rsquo;t retain every bit of information that we are exposed to—we have complex systems that decide what information is worth retaining based on various factors.\nMy algorithm is a bit more rudimentary than the process that the human brain has, but it seems to be working fine so far. It essentially calculates an \u0026ldquo;importance\u0026rdquo; score for every memory, which is just a weighted sum of a couple factors:\nimportance = w1 * log(1 + access_count) # Access frequency + w2 * is_protected # Protection bonus + w3 * is_evidence # Evidence bonus + w4 * exp(-age_days / tau) # Recency decay This system assigns a high score to frequently accessed memories, memories that the AI has decided it wants to \u0026ldquo;protect,\u0026rdquo; memories that are evidence for hypotheses that it has reasoned about, and memories that are recent. Memories that are old, infrequently accessed, and not useful for any of the learning goals or hypotheses the AI has created become candidates for forgetting.\nThe forgetting process is actually better understood as a summarization process. Instead of just deleting irrelevant memories, the system compacts them. It does this by summarizing \u0026ldquo;forgotten\u0026rdquo; episodes into condensed summaries and deleting the originals. This preserves knowledge while bounding storage growth—similar to how human memories become \u0026ldquo;fuzzy\u0026rdquo; over time.\nHow It Works End-to-End Let me walk through a complete cycle to show how these pieces fit together.\nStep 1: Message Received: I type something like: \u0026ldquo;Can you write me a short essay about my favorite sports team?\u0026rdquo;\nStep 2: RAG Retrieval: KenLM embeds my query and searches the episodic index. It finds 3 relevant past conversations about sports and essay preferences.\nStep 3: Context Assembly: The system prompt is built from:\nCore identity document (who KenLM is, its purpose) Current profile (hypotheses + learning goals) RAG episodes (the relevant past conversations) Behavioral instructions (meta-prompt generated at session start) Step 4: Deliberation: KenLM plans its approach: \u0026ldquo;Kenneth wants an essay about his favorite sports team. Based on my hypothesis about preferring conciseness, I should provide a short essay without excessive explanation. Based on our past conversations, his favorite sports team is the Seattle Sounders. I don\u0026rsquo;t need any external tools for this.\u0026rdquo;\nStep 5: Response Generation: KenLM generates a response, informed by the plan and all the context. The response is concise and focused—because that\u0026rsquo;s what it\u0026rsquo;s learned that I prefer.\nStep 6: Episode Logging: The conversation is saved as a new episode and indexed in the vector store.\nStep 7: Session End → Reflection: When I exit the chat, the reflection agent wakes up. It reviews the session, searches for patterns, and decides whether to update any hypotheses or learning goals.\nStep 8: Next Session → Meta-Prompting: The next time I start a chat, KenLM reads the (potentially updated) profile and generates fresh behavioral instructions. Then, the cycle continues.\nResults and Observations After using this system for a little bit, here are some of the results I\u0026rsquo;ve observed:\nWhat\u0026rsquo;s working Certain hypotheses stick very well. For example, early on, I found myself repeatedly asking KenLM to \u0026ldquo;be more concise\u0026rdquo; and to stop repeating itself. The reflection system noticed this pattern, formed a hypothesis, and now the model defaults to shorter responses. I rarely have to give feedback about the length of its responses anymore.\nRAG retrieval is surprisingly useful. When I ask about something I discussed in a past conversation, relevant context often appears in the model\u0026rsquo;s awareness. It doesn\u0026rsquo;t always explicitly reference the past conversation, but you can tell it\u0026rsquo;s informed by it. I can refer to things like \u0026ldquo;my favorite sports team\u0026rdquo; or \u0026ldquo;where I live\u0026rdquo; and the model remembers what those things are as they relate to me.\nMeta-prompting effectively influences personality. Each session feels more like what I\u0026rsquo;m looking for from a conversation with an LLM. It\u0026rsquo;s dropped the overly sycophantic tendencies, phrases like \u0026ldquo;Certainly!\u0026rdquo; or \u0026ldquo;You\u0026rsquo;re absolutely right!\u0026rdquo;, and other LLM-isms that have annoyed me. The behavioral instructions provide a stable \u0026ldquo;personality\u0026rdquo; for that session, and that personality is consistently adapting to my preferences.\nWhat\u0026rsquo;s Challenging Certain habits are hard to break. The model really loves markdown. I mean, it really loves outputting markdown formatting. No matter how many times I tell it it\u0026rsquo;s a command line tool and markdown is not helpful, it still really loves outputting neat markdown responses. Now, it HAS gotten a little better after much feedback, but I anticipate its preference for markdown is baked into the model weights itself, and not so easy to override through prompting. This is one of the clearest examples I\u0026rsquo;ve found so far of the limitations of this learning system.\nThe 7B model has limitations. My base KenLM model is a fine-tuned Qwen 2.5 7B. It\u0026rsquo;s good, but it occasionally misunderstands, hallucinates, or generates malformed tool calls. A larger model would likely perform better, but I wanted something that runs locally on my MacBook.\nThis process adds significant latency to the chat process. The RAG retrieval, meta-prompting, and reflection processes all take time. Using a model that\u0026rsquo;s running on my laptop means it can feel painfully slow at times.\nWhat this means This was a fun experiment, but what does this mean? Is there any use for systems like these in industry?\nI\u0026rsquo;m still far from an expert, but off the top of my head I can think of a couple potential applications of systems like these to develop real AI systems that solve real-world problems. Giving an LLM the ability to learn from experience opens the door to all kinds of systems we don’t really have today.\nThe obvious application is personalization: assistants that remember your preferences and workflows, which is what KenLM is. However, I think the implications go beyond “your AI knows your favorite sports team.” For example, imagine an educational model that can learn how you learn and what you know, remember your strong and weak areas, and adapt its explanations to your thinking style. This model could act as a more helpful learning companion than a model with no such memory capabilities.\nAnother application I can think of is creating models that improve themselves by optimizing for factors outside of human interaction. Imagine a model that initially struggles with chain-of-thought reasoning or tool usage—say something that consistently runs into the same error when trying to write python code or execute a web search. Currently, AI systems have no way to notice these things and improve. Human engineers need to monitor and step in, adjusting prompts, doing reinforcement learning, or collecting data with which to fine-tune in order to eke out a little bit better performance.\nA memory-equipped model changes this dynamic. For anything it\u0026rsquo;s trying to optimize for, be that reasoning or tool use, it can:\nnotice failed patterns by reasoning about them and remembering past experiences see successful patterns internalize the better method and adjust for future tasks Instead of updating billions of weights, the model updates a small set of learned strategies derived from lived experience. Over time, the model becomes a better problem solver not because it grew larger, but because it learned.\nThis all can potentially allow for smaller, more efficient models to rapidly adapt to specific use cases, reducing the industry\u0026rsquo;s reliance on frontier, compute-heavy models and allowing for more innovation and experimentation at the edge. Instead of treating model improvement as something that can only happen upstream, by massive labs with huge quantities of (often stolen) data, you get a world where models improve downstream, close to the user, close to the task, and close to the environment they operate in.\nIn that world, you don’t need a 400B-parameter model for everything. You need a reasonably capable base model with the right scaffolding: a memory system, a reflection system, and a mechanism for gradually internalizing the lessons it learns. Given enough experience, my intuition is that a modest model can outperform a much larger one on the tasks it specializes for, because it has context, history, and self-derived strategies the bigger model simply doesn’t have.\nIn the end, this really was just a weekend experiment, nothing more than an excuse to poke at a problem that’s been stuck in my head. I do, however, think that there are probably real ideas worth exploring here. I don’t know exactly where it will go yet, but I’m excited to keep experimenting and seeing how far a small model can go when you let it learn from its own experiences.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e\n\u003cp\u003eHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As \u003ca href=\"https://abehrouz.github.io/files/NL.pdf\"\u003ethis paper\u003c/a\u003e from Google points out, LLMs lack \u003cstrong\u003eneuroplasticity\u003c/strong\u003e—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes \u003cem\u003efrozen\u003c/em\u003e. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"","permalink":"http://localhost:1313/posts/hello-world/","summary":"","title":"Hello, World!"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes we\u0026rsquo;re trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge from short-term to long-term storage, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights). All the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions). Each meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals). A distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass passes information between these components, using its working memory (context) as the vehicle. Let me break down how it works piece by piece.\nLayer 1: Episodic Memory This piece of the architecture is the most straightforward. To enable the model to remember specific experiences, every interaction is logged as an episode. Every episode captures the user prompt, agent response, and some metadata for memory management. It looks something like this:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-01T14:30:00\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;Can you help me write a README for this project?\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;Sure. Here\u0026#39;s a concise template...\u0026#34;, \u0026#34;access_count\u0026#34;: 3, \u0026#34;last_accessed\u0026#34;: \u0026#34;2025-12-03T10:15:00\u0026#34;, \u0026#34;is_protected\u0026#34;: false } To enable the model to retrieve relevant episodes at inference time, I utilize a technique known as retrieval-augmented generation (RAG). Simply put, I store the episode data in a vector store called ChromaDB. Embeddings are generated by sentence-transformers after every conversation. Every time I send a message, KenLM embeds my query and searches for the top-k most semantically similar episodes in its memory. This allows it to reference episodes related to my query, so if I ask about sports, it will pull up discussions where we talked about sports. If I ask about documentation, it will remember episodes where we talked about writing documentation.\nThis gives the model something resembling episodic recall: the ability to remember relevant experiences and bring them into working memory (the context window) when they\u0026rsquo;re useful.\nLayer 2: Reflective Memory Episodic memory alone would essentially be just a fancy search engine. Something that humans do that enables more complex learning is reflect on past episodes and draw generalized conclusions based on them. Taking inspiration from this process, I built an agentic reflection process into KenLM.\nWhen I exit a chat session, KenLM runs a reflection loop. A KenLM \u0026ldquo;reflection agent\u0026rdquo; analyzes the session\u0026rsquo;s episodes, searches for related patterns in past conversations, and updates a profile of hypotheses and learning goals.\nHere\u0026rsquo;s what the reflection system actually does:\nReviews the session — What patterns appeared in my behavior? What did I ask about? How did I respond to KenLM\u0026rsquo;s answers?\nSearches for evidence — The reflection agent can call a search_episodes(query) tool to find supporting patterns across all past conversations. This prevents overfitting to a single session.\nUpdates hypotheses — For each insight, it calls upsert_hypothesis() with a confidence score. Crucially, confidence only increases when patterns repeat across multiple episodes.\nMaintains learning goals — If the model notices something it doesn\u0026rsquo;t understand about me yet, it creates a learning goal—a question to explore in future interactions.\nHere\u0026rsquo;s what my actual profile looks like after some use:\n{ \u0026#34;hypotheses\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_007\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;accuracy\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth is detail-oriented and expects accurate and precise information.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.6, \u0026#34;evidence_episode_ids\u0026#34;: [ 90, 91, 89, 92 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_008\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;transparency\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth values transparency and honesty in communication.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 89 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_009\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;task_compliance\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth expects tasks to be completed accurately and without unnecessary steps.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 154, 155, 156, 157, 158 ] } ], \u0026#34;learning_goals\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;learning_goal_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;How can we ensure that Kenneth\u0026#39;s instructions are followed without unnecessary repetition or recapitulation?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;closed\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-11-30T15:10:55.704097\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;lg_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;What are Kenneth\u0026#39;s specific expectations for the precision and detail in project updates?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-12-02T15:51:56.904564\u0026#34; } ] } The reflection prompt is designed to be conservative. I explicitly instruct the model to start hypotheses with low confidence (0.2–0.4) and only raise confidence when it sees repeated evidence. This should hopefully prevent the kind of overconfident conclusions that can come from a single unusual interaction.\nLayer 3: The Forgetting System A pretty important piece of learning is actually the process by which we decide which pieces of information aren\u0026rsquo;t relevant. If we were to allow our episodic vector database to grow unbounded with every user interaction, it would quickly become slow and stop being useful during inference or reflection. Moreover, humans also don\u0026rsquo;t retain every bit of information that we are exposed to—we have complex systems that decide what information is worth retaining based on various factors.\nMy algorithm is a bit more rudimentary than the process that the human brain has, but it seems to be working fine so far. It essentially calculates an \u0026ldquo;importance\u0026rdquo; score for every memory, which is just a weighted sum of a couple factors:\nimportance = w1 * log(1 + access_count) # Access frequency + w2 * is_protected # Protection bonus + w3 * is_evidence # Evidence bonus + w4 * exp(-age_days / tau) # Recency decay This system assigns a high score to frequently accessed memories, memories that the AI has decided it wants to \u0026ldquo;protect,\u0026rdquo; memories that are evidence for hypotheses that it has reasoned about, and memories that are recent. Memories that are old, infrequently accessed, and not useful for any of the learning goals or hypotheses the AI has created become candidates for forgetting.\nThe forgetting process is actually better understood as a summarization process. Instead of just deleting irrelevant memories, the system compacts them. It does this by summarizing \u0026ldquo;forgotten\u0026rdquo; episodes into condensed summaries and deleting the originals. This preserves knowledge while bounding storage growth—similar to how human memories become \u0026ldquo;fuzzy\u0026rdquo; over time.\nHow It Works End-to-End Let me walk through a complete cycle to show how these pieces fit together.\nStep 1: Message Received: I type something like: \u0026ldquo;Can you write me a short essay about my favorite sports team?\u0026rdquo;\nStep 2: RAG Retrieval: KenLM embeds my query and searches the episodic index. It finds 3 relevant past conversations about sports and essay preferences.\nStep 3: Context Assembly: The system prompt is built from:\nCore identity document (who KenLM is, its purpose) Current profile (hypotheses + learning goals) RAG episodes (the relevant past conversations) Behavioral instructions (meta-prompt generated at session start) Step 4: Deliberation: KenLM plans its approach: \u0026ldquo;Kenneth wants an essay about his favorite sports team. Based on my hypothesis about preferring conciseness, I should provide a short essay without excessive explanation. Based on our past conversations, his favorite sports team is the Seattle Sounders. I don\u0026rsquo;t need any external tools for this.\u0026rdquo;\nStep 5: Response Generation: KenLM generates a response, informed by the plan and all the context. The response is concise and focused—because that\u0026rsquo;s what it\u0026rsquo;s learned that I prefer.\nStep 6: Episode Logging: The conversation is saved as a new episode and indexed in the vector store.\nStep 7: Session End → Reflection: When I exit the chat, the reflection agent wakes up. It reviews the session, searches for patterns, and decides whether to update any hypotheses or learning goals.\nStep 8: Next Session → Meta-Prompting: The next time I start a chat, KenLM reads the (potentially updated) profile and generates fresh behavioral instructions. Then, the cycle continues.\nResults and Observations After using this system for a little bit, here are some of the results I\u0026rsquo;ve observed:\nWhat\u0026rsquo;s working Certain hypotheses stick very well. For example, early on, I found myself repeatedly asking KenLM to \u0026ldquo;be more concise\u0026rdquo; and to stop repeating itself. The reflection system noticed this pattern, formed a hypothesis, and now the model defaults to shorter responses. I rarely have to give feedback about the length of its responses anymore.\nRAG retrieval is surprisingly useful. When I ask about something I discussed in a past conversation, relevant context often appears in the model\u0026rsquo;s awareness. It doesn\u0026rsquo;t always explicitly reference the past conversation, but you can tell it\u0026rsquo;s informed by it. I can refer to things like \u0026ldquo;my favorite sports team\u0026rdquo; or \u0026ldquo;where I live\u0026rdquo; and the model remembers what those things are as they relate to me.\nMeta-prompting effectively influences personality. Each session feels more like what I\u0026rsquo;m looking for from a conversation with an LLM. It\u0026rsquo;s dropped the overly sycophantic tendencies, phrases like \u0026ldquo;Certainly!\u0026rdquo; or \u0026ldquo;You\u0026rsquo;re absolutely right!\u0026rdquo;, and other LLM-isms that have annoyed me. The behavioral instructions provide a stable \u0026ldquo;personality\u0026rdquo; for that session, and that personality is consistently adapting to my preferences.\nWhat\u0026rsquo;s Challenging Certain habits are hard to break. The model really loves markdown. I mean, it really loves outputting markdown formatting. No matter how many times I tell it it\u0026rsquo;s a command line tool and markdown is not helpful, it still really loves outputting neat markdown responses. Now, it HAS gotten a little better after much feedback, but I anticipate its preference for markdown is baked into the model weights itself, and not so easy to override through prompting. This is one of the clearest examples I\u0026rsquo;ve found so far of the limitations of this learning system.\nThe 7B model has limitations. My base KenLM model is a fine-tuned Qwen 2.5 7B. It\u0026rsquo;s good, but it occasionally misunderstands, hallucinates, or generates malformed tool calls. A larger model would likely perform better, but I wanted something that runs locally on my MacBook.\nThis process adds significant latency to the chat process. The RAG retrieval, meta-prompting, and reflection processes all take time. Using a model that\u0026rsquo;s running on my laptop means it can feel painfully slow at times.\nWhat this means This was a fun experiment, but what does this mean? Is there any use for systems like these in industry?\nI\u0026rsquo;m still far from an expert, but off the top of my head I can think of a couple potential applications of systems like these to develop real AI systems that solve real-world problems. Giving an LLM the ability to learn from experience opens the door to all kinds of systems we don’t really have today.\nThe obvious application is personalization: assistants that remember your preferences and workflows, which is what KenLM is. However, I think the implications go beyond “your AI knows your favorite sports team.” For example, imagine an educational model that can learn how you learn and what you know, remember your strong and weak areas, and adapt its explanations to your thinking style. This model could act as a more helpful learning companion than a model with no such memory capabilities.\nAnother application I can think of is creating models that improve themselves by optimizing for factors outside of human interaction. Imagine a model that initially struggles with chain-of-thought reasoning or tool usage—say something that consistently runs into the same error when trying to write python code or execute a web search. Currently, AI systems have no way to notice these things and improve. Human engineers need to monitor and step in, adjusting prompts, doing reinforcement learning, or collecting data with which to fine-tune in order to eke out a little bit better performance.\nA memory-equipped model changes this dynamic. For anything it\u0026rsquo;s trying to optimize for, be that reasoning or tool use, it can:\nnotice failed patterns by reasoning about them and remembering past experiences see successful patterns internalize the better method and adjust for future tasks Instead of updating billions of weights, the model updates a small set of learned strategies derived from lived experience. Over time, the model becomes a better problem solver not because it grew larger, but because it learned.\nThis all can potentially allow for smaller, more efficient models to rapidly adapt to specific use cases, reducing the industry\u0026rsquo;s reliance on frontier, compute-heavy models and allowing for more innovation and experimentation at the edge. Instead of treating model improvement as something that can only happen upstream, by massive labs with huge quantities of (often stolen) data, you get a world where models improve downstream, close to the user, close to the task, and close to the environment they operate in.\nIn that world, you don’t need a 400B-parameter model for everything. You need a reasonably capable base model with the right scaffolding: a memory system, a reflection system, and a mechanism for gradually internalizing the lessons it learns. Given enough experience, my intuition is that a modest model can outperform a much larger one on the tasks it specializes for, because it has context, history, and self-derived strategies the bigger model simply doesn’t have.\nIn the end, this really was just a weekend experiment, nothing more than an excuse to poke at a problem that’s been stuck in my head. I do, however, think that there are probably real ideas worth exploring here. I don’t know exactly where it will go yet, but I’m excited to keep experimenting and seeing how far a small model can go when you let it learn from its own experiences.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e\n\u003cp\u003eHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As \u003ca href=\"https://abehrouz.github.io/files/NL.pdf\"\u003ethis paper\u003c/a\u003e from Google points out, LLMs lack \u003cstrong\u003eneuroplasticity\u003c/strong\u003e—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes \u003cem\u003efrozen\u003c/em\u003e. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"This is a sample experiment post! Use this as a template for documenting your experiments.\nThe Idea Describe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\nGoal: Build a personal AI assistant that can help with daily tasks.\nThe Approach Walk through your methodology:\nResearch — What existing solutions did you look at? Design — How did you structure your solution? Implementation — What tools and techniques did you use? Code Example # Example code snippet def hello_experiment(): print(\u0026#34;Hello from my experiment!\u0026#34;) return \u0026#34;success\u0026#34; Results What did you learn? Did it work? What surprised you?\nMetric Expected Actual Speed Fast ✅ Fast Accuracy Good ⚠️ Needs work What\u0026rsquo;s Next Iteration ideas Follow-up experiments Things to explore further Takeaways Summarize the key learnings. What would you do differently? What would you recommend to others?\nThis post is tagged with experiments so it appears in the Experiments section.\n","permalink":"http://localhost:1313/posts/sample-experiment/","summary":"\u003cp\u003eThis is a sample experiment post! Use this as a template for documenting your experiments.\u003c/p\u003e\n\u003ch2 id=\"the-idea\"\u003eThe Idea\u003c/h2\u003e\n\u003cp\u003eDescribe what you\u0026rsquo;re trying to build or test. What problem are you solving? What hypothesis are you testing?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Build a personal AI assistant that can help with daily tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"the-approach\"\u003eThe Approach\u003c/h2\u003e\n\u003cp\u003eWalk through your methodology:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eResearch\u003c/strong\u003e — What existing solutions did you look at?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDesign\u003c/strong\u003e — How did you structure your solution?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e — What tools and techniques did you use?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"code-example\"\u003eCode Example\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example code snippet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_experiment\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello from my experiment!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;success\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eWhat did you learn? Did it work? What surprised you?\u003c/p\u003e","title":"My First Experiment: Building a Personal AI Assistant"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes we\u0026rsquo;re trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge from short-term to long-term storage, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights). All the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions). Each meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals). A distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass passes information between these components, using its working memory (context) as the vehicle. Let me break down how it works piece by piece.\nLayer 1: Episodic Memory This piece of the architecture is the most straightforward. To enable the model to remember specific experiences, every interaction is logged as an episode. Every episode captures the user prompt, agent response, and some metadata for memory management. It looks something like this:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-01T14:30:00\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;Can you help me write a README for this project?\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;Sure. Here\u0026#39;s a concise template...\u0026#34;, \u0026#34;access_count\u0026#34;: 3, \u0026#34;last_accessed\u0026#34;: \u0026#34;2025-12-03T10:15:00\u0026#34;, \u0026#34;is_protected\u0026#34;: false } To enable the model to retrieve relevant episodes at inference time, I utilize a technique known as retrieval-augmented generation (RAG). Simply put, I store the episode data in a vector store called ChromaDB. Embeddings are generated by sentence-transformers after every conversation. Every time I send a message, KenLM embeds my query and searches for the top-k most semantically similar episodes in its memory. This allows it to reference episodes related to my query, so if I ask about sports, it will pull up discussions where we talked about sports. If I ask about documentation, it will remember episodes where we talked about writing documentation.\nThis gives the model something resembling episodic recall: the ability to remember relevant experiences and bring them into working memory (the context window) when they\u0026rsquo;re useful.\nLayer 2: Reflective Memory Episodic memory alone would essentially be just a fancy search engine. Something that humans do that enables more complex learning is reflect on past episodes and draw generalized conclusions based on them. Taking inspiration from this process, I built an agentic reflection process into KenLM.\nWhen I exit a chat session, KenLM runs a reflection loop. A KenLM \u0026ldquo;reflection agent\u0026rdquo; analyzes the session\u0026rsquo;s episodes, searches for related patterns in past conversations, and updates a profile of hypotheses and learning goals.\nHere\u0026rsquo;s what the reflection system actually does:\nReviews the session — What patterns appeared in my behavior? What did I ask about? How did I respond to KenLM\u0026rsquo;s answers?\nSearches for evidence — The reflection agent can call a search_episodes(query) tool to find supporting patterns across all past conversations. This prevents overfitting to a single session.\nUpdates hypotheses — For each insight, it calls upsert_hypothesis() with a confidence score. Crucially, confidence only increases when patterns repeat across multiple episodes.\nMaintains learning goals — If the model notices something it doesn\u0026rsquo;t understand about me yet, it creates a learning goal—a question to explore in future interactions.\nHere\u0026rsquo;s what my actual profile looks like after some use:\n{ \u0026#34;hypotheses\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_007\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;accuracy\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth is detail-oriented and expects accurate and precise information.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.6, \u0026#34;evidence_episode_ids\u0026#34;: [ 90, 91, 89, 92 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_008\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;transparency\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth values transparency and honesty in communication.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 89 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_009\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;task_compliance\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth expects tasks to be completed accurately and without unnecessary steps.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 154, 155, 156, 157, 158 ] } ], \u0026#34;learning_goals\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;learning_goal_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;How can we ensure that Kenneth\u0026#39;s instructions are followed without unnecessary repetition or recapitulation?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;closed\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-11-30T15:10:55.704097\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;lg_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;What are Kenneth\u0026#39;s specific expectations for the precision and detail in project updates?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-12-02T15:51:56.904564\u0026#34; } ] } The reflection prompt is designed to be conservative. I explicitly instruct the model to start hypotheses with low confidence (0.2–0.4) and only raise confidence when it sees repeated evidence. This should hopefully prevent the kind of overconfident conclusions that can come from a single unusual interaction.\nLayer 3: The Forgetting System A pretty important piece of learning is actually the process by which we decide which pieces of information aren\u0026rsquo;t relevant. If we were to allow our episodic vector database to grow unbounded with every user interaction, it would quickly become slow and stop being useful during inference or reflection. Moreover, humans also don\u0026rsquo;t retain every bit of information that we are exposed to—we have complex systems that decide what information is worth retaining based on various factors.\nMy algorithm is a bit more rudimentary than the process that the human brain has, but it seems to be working fine so far. It essentially calculates an \u0026ldquo;importance\u0026rdquo; score for every memory, which is just a weighted sum of a couple factors:\nimportance = w1 * log(1 + access_count) # Access frequency + w2 * is_protected # Protection bonus + w3 * is_evidence # Evidence bonus + w4 * exp(-age_days / tau) # Recency decay This system assigns a high score to frequently accessed memories, memories that the AI has decided it wants to \u0026ldquo;protect,\u0026rdquo; memories that are evidence for hypotheses that it has reasoned about, and memories that are recent. Memories that are old, infrequently accessed, and not useful for any of the learning goals or hypotheses the AI has created become candidates for forgetting.\nThe forgetting process is actually better understood as a summarization process. Instead of just deleting irrelevant memories, the system compacts them. It does this by summarizing \u0026ldquo;forgotten\u0026rdquo; episodes into condensed summaries and deleting the originals. This preserves knowledge while bounding storage growth—similar to how human memories become \u0026ldquo;fuzzy\u0026rdquo; over time.\nHow It Works End-to-End Let me walk through a complete cycle to show how these pieces fit together.\nStep 1: Message Received: I type something like: \u0026ldquo;Can you write me a short essay about my favorite sports team?\u0026rdquo;\nStep 2: RAG Retrieval: KenLM embeds my query and searches the episodic index. It finds 3 relevant past conversations about sports and essay preferences.\nStep 3: Context Assembly: The system prompt is built from:\nCore identity document (who KenLM is, its purpose) Current profile (hypotheses + learning goals) RAG episodes (the relevant past conversations) Behavioral instructions (meta-prompt generated at session start) Step 4: Deliberation: KenLM plans its approach: \u0026ldquo;Kenneth wants an essay about his favorite sports team. Based on my hypothesis about preferring conciseness, I should provide a short essay without excessive explanation. Based on our past conversations, his favorite sports team is the Seattle Sounders. I don\u0026rsquo;t need any external tools for this.\u0026rdquo;\nStep 5: Response Generation: KenLM generates a response, informed by the plan and all the context. The response is concise and focused—because that\u0026rsquo;s what it\u0026rsquo;s learned that I prefer.\nStep 6: Episode Logging: The conversation is saved as a new episode and indexed in the vector store.\nStep 7: Session End → Reflection: When I exit the chat, the reflection agent wakes up. It reviews the session, searches for patterns, and decides whether to update any hypotheses or learning goals.\nStep 8: Next Session → Meta-Prompting: The next time I start a chat, KenLM reads the (potentially updated) profile and generates fresh behavioral instructions. Then, the cycle continues.\nResults and Observations After using this system for a little bit, here are some of the results I\u0026rsquo;ve observed:\nWhat\u0026rsquo;s working Certain hypotheses stick very well. For example, early on, I found myself repeatedly asking KenLM to \u0026ldquo;be more concise\u0026rdquo; and to stop repeating itself. The reflection system noticed this pattern, formed a hypothesis, and now the model defaults to shorter responses. I rarely have to give feedback about the length of its responses anymore.\nRAG retrieval is surprisingly useful. When I ask about something I discussed in a past conversation, relevant context often appears in the model\u0026rsquo;s awareness. It doesn\u0026rsquo;t always explicitly reference the past conversation, but you can tell it\u0026rsquo;s informed by it. I can refer to things like \u0026ldquo;my favorite sports team\u0026rdquo; or \u0026ldquo;where I live\u0026rdquo; and the model remembers what those things are as they relate to me.\nMeta-prompting effectively influences personality. Each session feels more like what I\u0026rsquo;m looking for from a conversation with an LLM. It\u0026rsquo;s dropped the overly sycophantic tendencies, phrases like \u0026ldquo;Certainly!\u0026rdquo; or \u0026ldquo;You\u0026rsquo;re absolutely right!\u0026rdquo;, and other LLM-isms that have annoyed me. The behavioral instructions provide a stable \u0026ldquo;personality\u0026rdquo; for that session, and that personality is consistently adapting to my preferences.\nWhat\u0026rsquo;s Challenging Certain habits are hard to break. The model really loves markdown. I mean, it really loves outputting markdown formatting. No matter how many times I tell it it\u0026rsquo;s a command line tool and markdown is not helpful, it still really loves outputting neat markdown responses. Now, it HAS gotten a little better after much feedback, but I anticipate its preference for markdown is baked into the model weights itself, and not so easy to override through prompting. This is one of the clearest examples I\u0026rsquo;ve found so far of the limitations of this learning system.\nThe 7B model has limitations. My base KenLM model is a fine-tuned Qwen 2.5 7B. It\u0026rsquo;s good, but it occasionally misunderstands, hallucinates, or generates malformed tool calls. A larger model would likely perform better, but I wanted something that runs locally on my MacBook.\nThis process adds significant latency to the chat process. The RAG retrieval, meta-prompting, and reflection processes all take time. Using a model that\u0026rsquo;s running on my laptop means it can feel painfully slow at times.\nWhat this means This was a fun experiment, but what does this mean? Is there any use for systems like these in industry?\nI\u0026rsquo;m still far from an expert, but off the top of my head I can think of a couple potential applications of systems like these to develop real AI systems that solve real-world problems. Giving an LLM the ability to learn from experience opens the door to all kinds of systems we don’t really have today.\nThe obvious application is personalization: assistants that remember your preferences and workflows, which is what KenLM is. However, I think the implications go beyond “your AI knows your favorite sports team.” For example, imagine an educational model that can learn how you learn and what you know, remember your strong and weak areas, and adapt its explanations to your thinking style. This model could act as a more helpful learning companion than a model with no such memory capabilities.\nAnother application I can think of is creating models that improve themselves by optimizing for factors outside of human interaction. Imagine a model that initially struggles with chain-of-thought reasoning or tool usage—say something that consistently runs into the same error when trying to write python code or execute a web search. Currently, AI systems have no way to notice these things and improve. Human engineers need to monitor and step in, adjusting prompts, doing reinforcement learning, or collecting data with which to fine-tune in order to eke out a little bit better performance.\nA memory-equipped model changes this dynamic. For anything it\u0026rsquo;s trying to optimize for, be that reasoning or tool use, it can:\nnotice failed patterns by reasoning about them and remembering past experiences see successful patterns internalize the better method and adjust for future tasks Instead of updating billions of weights, the model updates a small set of learned strategies derived from lived experience. Over time, the model becomes a better problem solver not because it grew larger, but because it learned.\nThis all can potentially allow for smaller, more efficient models to rapidly adapt to specific use cases, reducing the industry\u0026rsquo;s reliance on frontier, compute-heavy models and allowing for more innovation and experimentation at the edge. Instead of treating model improvement as something that can only happen upstream, by massive labs with huge quantities of (often stolen) data, you get a world where models improve downstream, close to the user, close to the task, and close to the environment they operate in.\nIn that world, you don’t need a 400B-parameter model for everything. You need a reasonably capable base model with the right scaffolding: a memory system, a reflection system, and a mechanism for gradually internalizing the lessons it learns. Given enough experience, my intuition is that a modest model can outperform a much larger one on the tasks it specializes for, because it has context, history, and self-derived strategies the bigger model simply doesn’t have.\nIn the end, this really was just a weekend experiment, nothing more than an excuse to poke at a problem that’s been stuck in my head. I do, however, think that there are probably real ideas worth exploring here. I don’t know exactly where it will go yet, but I’m excited to keep experimenting and seeing how far a small model can go when you let it learn from its own experiences.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e\n\u003cp\u003eHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As \u003ca href=\"https://abehrouz.github.io/files/NL.pdf\"\u003ethis paper\u003c/a\u003e from Google points out, LLMs lack \u003cstrong\u003eneuroplasticity\u003c/strong\u003e—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes \u003cem\u003efrozen\u003c/em\u003e. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes we\u0026rsquo;re trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge from short-term to long-term storage, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights). All the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions). Each meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals). A distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass passes information between these components, using its working memory (context) as the vehicle. Let me break down how it works piece by piece.\nLayer 1: Episodic Memory This piece of the architecture is the most straightforward. To enable the model to remember specific experiences, every interaction is logged as an episode. Every episode captures the user prompt, agent response, and some metadata for memory management. It looks something like this:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-01T14:30:00\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;Can you help me write a README for this project?\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;Sure. Here\u0026#39;s a concise template...\u0026#34;, \u0026#34;access_count\u0026#34;: 3, \u0026#34;last_accessed\u0026#34;: \u0026#34;2025-12-03T10:15:00\u0026#34;, \u0026#34;is_protected\u0026#34;: false } To enable the model to retrieve relevant episodes at inference time, I utilize a technique known as retrieval-augmented generation (RAG). Simply put, I store the episode data in a vector store called ChromaDB. Embeddings are generated by sentence-transformers after every conversation. Every time I send a message, KenLM embeds my query and searches for the top-k most semantically similar episodes in its memory. This allows it to reference episodes related to my query, so if I ask about sports, it will pull up discussions where we talked about sports. If I ask about documentation, it will remember episodes where we talked about writing documentation.\nThis gives the model something resembling episodic recall: the ability to remember relevant experiences and bring them into working memory (the context window) when they\u0026rsquo;re useful.\nLayer 2: Reflective Memory Episodic memory alone would essentially be just a fancy search engine. Something that humans do that enables more complex learning is reflect on past episodes and draw generalized conclusions based on them. Taking inspiration from this process, I built an agentic reflection process into KenLM.\nWhen I exit a chat session, KenLM runs a reflection loop. A KenLM \u0026ldquo;reflection agent\u0026rdquo; analyzes the session\u0026rsquo;s episodes, searches for related patterns in past conversations, and updates a profile of hypotheses and learning goals.\nHere\u0026rsquo;s what the reflection system actually does:\nReviews the session — What patterns appeared in my behavior? What did I ask about? How did I respond to KenLM\u0026rsquo;s answers?\nSearches for evidence — The reflection agent can call a search_episodes(query) tool to find supporting patterns across all past conversations. This prevents overfitting to a single session.\nUpdates hypotheses — For each insight, it calls upsert_hypothesis() with a confidence score. Crucially, confidence only increases when patterns repeat across multiple episodes.\nMaintains learning goals — If the model notices something it doesn\u0026rsquo;t understand about me yet, it creates a learning goal—a question to explore in future interactions.\nHere\u0026rsquo;s what my actual profile looks like after some use:\n{ \u0026#34;hypotheses\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_007\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;accuracy\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth is detail-oriented and expects accurate and precise information.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.6, \u0026#34;evidence_episode_ids\u0026#34;: [ 90, 91, 89, 92 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_008\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;transparency\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth values transparency and honesty in communication.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 89 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_009\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;task_compliance\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth expects tasks to be completed accurately and without unnecessary steps.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 154, 155, 156, 157, 158 ] } ], \u0026#34;learning_goals\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;learning_goal_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;How can we ensure that Kenneth\u0026#39;s instructions are followed without unnecessary repetition or recapitulation?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;closed\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-11-30T15:10:55.704097\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;lg_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;What are Kenneth\u0026#39;s specific expectations for the precision and detail in project updates?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-12-02T15:51:56.904564\u0026#34; } ] } The reflection prompt is designed to be conservative. I explicitly instruct the model to start hypotheses with low confidence (0.2–0.4) and only raise confidence when it sees repeated evidence. This should hopefully prevent the kind of overconfident conclusions that can come from a single unusual interaction.\nLayer 3: The Forgetting System A pretty important piece of learning is actually the process by which we decide which pieces of information aren\u0026rsquo;t relevant. If we were to allow our episodic vector database to grow unbounded with every user interaction, it would quickly become slow and stop being useful during inference or reflection. Moreover, humans also don\u0026rsquo;t retain every bit of information that we are exposed to—we have complex systems that decide what information is worth retaining based on various factors.\nMy algorithm is a bit more rudimentary than the process that the human brain has, but it seems to be working fine so far. It essentially calculates an \u0026ldquo;importance\u0026rdquo; score for every memory, which is just a weighted sum of a couple factors:\nimportance = w1 * log(1 + access_count) # Access frequency + w2 * is_protected # Protection bonus + w3 * is_evidence # Evidence bonus + w4 * exp(-age_days / tau) # Recency decay This system assigns a high score to frequently accessed memories, memories that the AI has decided it wants to \u0026ldquo;protect,\u0026rdquo; memories that are evidence for hypotheses that it has reasoned about, and memories that are recent. Memories that are old, infrequently accessed, and not useful for any of the learning goals or hypotheses the AI has created become candidates for forgetting.\nThe forgetting process is actually better understood as a summarization process. Instead of just deleting irrelevant memories, the system compacts them. It does this by summarizing \u0026ldquo;forgotten\u0026rdquo; episodes into condensed summaries and deleting the originals. This preserves knowledge while bounding storage growth—similar to how human memories become \u0026ldquo;fuzzy\u0026rdquo; over time.\nHow It Works End-to-End Let me walk through a complete cycle to show how these pieces fit together.\nStep 1: Message Received: I type something like: \u0026ldquo;Can you write me a short essay about my favorite sports team?\u0026rdquo;\nStep 2: RAG Retrieval: KenLM embeds my query and searches the episodic index. It finds 3 relevant past conversations about sports and essay preferences.\nStep 3: Context Assembly: The system prompt is built from:\nCore identity document (who KenLM is, its purpose) Current profile (hypotheses + learning goals) RAG episodes (the relevant past conversations) Behavioral instructions (meta-prompt generated at session start) Step 4: Deliberation: KenLM plans its approach: \u0026ldquo;Kenneth wants an essay about his favorite sports team. Based on my hypothesis about preferring conciseness, I should provide a short essay without excessive explanation. Based on our past conversations, his favorite sports team is the Seattle Sounders. I don\u0026rsquo;t need any external tools for this.\u0026rdquo;\nStep 5: Response Generation: KenLM generates a response, informed by the plan and all the context. The response is concise and focused—because that\u0026rsquo;s what it\u0026rsquo;s learned that I prefer.\nStep 6: Episode Logging: The conversation is saved as a new episode and indexed in the vector store.\nStep 7: Session End → Reflection: When I exit the chat, the reflection agent wakes up. It reviews the session, searches for patterns, and decides whether to update any hypotheses or learning goals.\nStep 8: Next Session → Meta-Prompting: The next time I start a chat, KenLM reads the (potentially updated) profile and generates fresh behavioral instructions. Then, the cycle continues.\nResults and Observations After using this system for a little bit, here are some of the results I\u0026rsquo;ve observed:\nWhat\u0026rsquo;s working Certain hypotheses stick very well. For example, early on, I found myself repeatedly asking KenLM to \u0026ldquo;be more concise\u0026rdquo; and to stop repeating itself. The reflection system noticed this pattern, formed a hypothesis, and now the model defaults to shorter responses. I rarely have to give feedback about the length of its responses anymore.\nRAG retrieval is surprisingly useful. When I ask about something I discussed in a past conversation, relevant context often appears in the model\u0026rsquo;s awareness. It doesn\u0026rsquo;t always explicitly reference the past conversation, but you can tell it\u0026rsquo;s informed by it. I can refer to things like \u0026ldquo;my favorite sports team\u0026rdquo; or \u0026ldquo;where I live\u0026rdquo; and the model remembers what those things are as they relate to me.\nMeta-prompting effectively influences personality. Each session feels more like what I\u0026rsquo;m looking for from a conversation with an LLM. It\u0026rsquo;s dropped the overly sycophantic tendencies, phrases like \u0026ldquo;Certainly!\u0026rdquo; or \u0026ldquo;You\u0026rsquo;re absolutely right!\u0026rdquo;, and other LLM-isms that have annoyed me. The behavioral instructions provide a stable \u0026ldquo;personality\u0026rdquo; for that session, and that personality is consistently adapting to my preferences.\nWhat\u0026rsquo;s Challenging Certain habits are hard to break. The model really loves markdown. I mean, it really loves outputting markdown formatting. No matter how many times I tell it it\u0026rsquo;s a command line tool and markdown is not helpful, it still really loves outputting neat markdown responses. Now, it HAS gotten a little better after much feedback, but I anticipate its preference for markdown is baked into the model weights itself, and not so easy to override through prompting. This is one of the clearest examples I\u0026rsquo;ve found so far of the limitations of this learning system.\nThe 7B model has limitations. My base KenLM model is a fine-tuned Qwen 2.5 7B. It\u0026rsquo;s good, but it occasionally misunderstands, hallucinates, or generates malformed tool calls. A larger model would likely perform better, but I wanted something that runs locally on my MacBook.\nThis process adds significant latency to the chat process. The RAG retrieval, meta-prompting, and reflection processes all take time. Using a model that\u0026rsquo;s running on my laptop means it can feel painfully slow at times.\nWhat this means This was a fun experiment, but what does this mean? Is there any use for systems like these in industry?\nI\u0026rsquo;m still far from an expert, but off the top of my head I can think of a couple potential applications of systems like these to develop real AI systems that solve real-world problems. Giving an LLM the ability to learn from experience opens the door to all kinds of systems we don’t really have today.\nThe obvious application is personalization: assistants that remember your preferences and workflows, which is what KenLM is. However, I think the implications go beyond “your AI knows your favorite sports team.” For example, imagine an educational model that can learn how you learn and what you know, remember your strong and weak areas, and adapt its explanations to your thinking style. This model could act as a more helpful learning companion than a model with no such memory capabilities.\nAnother application I can think of is creating models that improve themselves by optimizing for factors outside of human interaction. Imagine a model that initially struggles with chain-of-thought reasoning or tool usage—say something that consistently runs into the same error when trying to write python code or execute a web search. Currently, AI systems have no way to notice these things and improve. Human engineers need to monitor and step in, adjusting prompts, doing reinforcement learning, or collecting data with which to fine-tune in order to eke out a little bit better performance.\nA memory-equipped model changes this dynamic. For anything it\u0026rsquo;s trying to optimize for, be that reasoning or tool use, it can:\nnotice failed patterns by reasoning about them and remembering past experiences see successful patterns internalize the better method and adjust for future tasks Instead of updating billions of weights, the model updates a small set of learned strategies derived from lived experience. Over time, the model becomes a better problem solver not because it grew larger, but because it learned.\nThis all can potentially allow for smaller, more efficient models to rapidly adapt to specific use cases, reducing the industry\u0026rsquo;s reliance on frontier, compute-heavy models and allowing for more innovation and experimentation at the edge. Instead of treating model improvement as something that can only happen upstream, by massive labs with huge quantities of (often stolen) data, you get a world where models improve downstream, close to the user, close to the task, and close to the environment they operate in.\nIn that world, you don’t need a 400B-parameter model for everything. You need a reasonably capable base model with the right scaffolding: a memory system, a reflection system, and a mechanism for gradually internalizing the lessons it learns. Given enough experience, my intuition is that a modest model can outperform a much larger one on the tasks it specializes for, because it has context, history, and self-derived strategies the bigger model simply doesn’t have.\nIn the end, this really was just a weekend experiment, nothing more than an excuse to poke at a problem that’s been stuck in my head. I do, however, think that there are probably real ideas worth exploring here. I don’t know exactly where it will go yet, but I’m excited to keep experimenting and seeing how far a small model can go when you let it learn from its own experiences.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e\n\u003cp\u003eHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As \u003ca href=\"https://abehrouz.github.io/files/NL.pdf\"\u003ethis paper\u003c/a\u003e from Google points out, LLMs lack \u003cstrong\u003eneuroplasticity\u003c/strong\u003e—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes \u003cem\u003efrozen\u003c/em\u003e. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"},{"content":"Introduction LLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\nHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As this paper from Google points out, LLMs lack neuroplasticity—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes frozen. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\nThis limitation has been my personal obsession for a couple weeks, so I decided to try something. I wanted to build a system that allows my personal LLM (which I\u0026rsquo;ve named KenLM) to have a form of episodic memory, and to update its behavior based on prior interactions. Along the way, I added a reflection mechanism that lets the model distill recent episodes into self-maintained learning goals, enabling it to make inferences about the user and adjust its behavior in more helpful directions.\nThe Idea My goal is to introduce a loop into the inference process whereby my local model can retrieve relevant episodic memories, improve its reasoning because of that history, reflect on new experiences to update its own learning goals, and eventually fine-tune on those improvements. Through this process, we can turn short-term interactions into long-term semantic memory, mimicking biological neuroplasticity (in an admittedly quite rudimentary way). As you’ll see in this post, it’s a surprisingly powerful step toward an AI that actually learns from the person who uses it.\nGoal: Build a personal AI assistant that learns from its experiences\nHow do humans learn? Before we dive into what I built, I think it\u0026rsquo;s worth understanding what the core processes we\u0026rsquo;re trying to mimic actually are. The word \u0026ldquo;memory\u0026rdquo; is actually an umbrella term for several different processes that enable human learning and recall, so let\u0026rsquo;s talk about them briefly.\nTypes of Memory Humans don’t have a single monolithic memory system. What we call \u0026ldquo;memory\u0026rdquo; is actually our brains constantly moving information between several different memory systems, each serving a different purpose:\nSensory memory holds raw impressions for a fraction of a second. Short-term (or working) memory is where active reasoning and conscious thought happen. Long-term memory itself has multiple subtypes: Semantic memory is our stable knowledge, like facts or concepts. Episodic memory is our lived experiences, situated in time and place. Procedural memory is what enables us to build skills and habits. For the purposes of building a personal AI that actually learns, the two memory types that I focused on are semantic and episodic. Semantic memory is the stuff we \u0026ldquo;just know\u0026rdquo;, like how to drive, how to multiply, or what an apple is. Episodic memory is everything we personally experienced that shaped what we know, such as your first day at a new job, the last conversation you had with a friend, or the moment something finally clicked.\nHumans are constantly moving information between these two systems. You have an experience, you reflect on it (either consciously or unconsciously), and your brain gradually consolidates whatever is useful into long-term knowledge. This loop is what lets us adapt, grow, and develop intelligent behavior over time.\nHow do LLMs learn? LLMs, on the other hand, have a much more rudimentary memory system. They have:\nSemantic memory, which is the facts and concepts that are encoded into their weights through pretraining. Working memory, which is whatever context is fed to them in their prompts. With only these two systems, and no way to transfer knowledge from short-term to long-term storage, LLMs have no ability to remember experiences that happened to them after pretraining but before the current context window. As the Google paper puts it, it\u0026rsquo;s as if they suffer from anterograde amnesia.\nThe TL;DR of it all is this: LLMs learn exclusively through pretraining and have no ability to learn through experiences. Humans, on the other hand, learn exclusively through lived experiences. If I could give an LLM the ability to learn through experiences the way that humans do, I think it could potentially accelerate the rate at which these models can become specialized and useful across the industry.\nWhat I Wanted KenLM to Do To prove this concept, I wanted to build systems around a model such that it would exhibit a couple key behaviors:\nRemember past interactions. Ideally, it would have a mechanism to remember past interactions and a mechanism to forget infrequently accessed memories as well, so it could keep just the experiences that are useful for self-improvement. This would hopefully enable it to remember in an episodic sense what the user\u0026rsquo;s preferences, goals, style choices, and projects in flight are—the sorts of things humans naturally remember about people and can reference in a conversation.\nUse that memory to shape future reasoning. For example, if I\u0026rsquo;ve already chatted with my assistant about my favorite sports team or ice cream flavor, it should remember that and tend towards producing output that includes things that I like. If I\u0026rsquo;ve already explained how I like code comments formatted, I shouldn\u0026rsquo;t have to restate it every time. Remembering things is pointless unless it actually uses those memories to shape its output in some way.\nReflect on new experiences. The model should be able to recognize patterns across interactions and form hypotheses about how I prefer it to behave.\nConsolidate those reflections into stable long-term learning. This could be explicit “learning goals,” fed in through context, or, eventually, actual weight updates through periodic fine-tuning.\nGrow more aligned, personalized, and helpful over time. The model should accomplish this not by bolting on more rules, but through the same loop humans use: experience → reflection → consolidation.\nThe more I think about it, the more I think this whole project is a kind of experiment in computational psychology. What happens if you give a small local LLM the scaffolding humans use to improve themselves over time? How far can we push the illusion of personal growth without touching the base model’s weights during inference?\nTo answer that question, we need to dive into the architecture.\nThe Architecture At a high level, KenLM now has three distinct \u0026ldquo;memory systems\u0026rdquo; that work together during inference:\nSemantic Memory (the model weights). All the static knowledge from pretraining + any fine-tunes.\nEpisodic Memory (a vector store / database of past interactions). Each meaningful “experience” is saved as a structured record with metadata.\nReflective Memory (self-maintained learning goals). A distilled set of insights the model has inferred about me—and about itself—over time.\nEach inference pass passes information between these components, using its working memory (context) as the vehicle. Let me break down how it works piece by piece.\nLayer 1: Episodic Memory This piece of the architecture is the most straightforward. To enable the model to remember specific experiences, every interaction is logged as an episode. Every episode captures the user prompt, agent response, and some metadata for memory management. It looks something like this:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-01T14:30:00\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;Can you help me write a README for this project?\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;Sure. Here\u0026#39;s a concise template...\u0026#34;, \u0026#34;access_count\u0026#34;: 3, \u0026#34;last_accessed\u0026#34;: \u0026#34;2025-12-03T10:15:00\u0026#34;, \u0026#34;is_protected\u0026#34;: false } To enable the model to retrieve relevant episodes at inference time, I utilize a technique known as retrieval-augmented generation (RAG). Simply put, I store the episode data in a vector store called ChromaDB. Embeddings are generated by sentence-transformers after every conversation. Every time I send a message, KenLM embeds my query and searches for the top-k most semantically similar episodes in its memory. This allows it to reference episodes related to my query, so if I ask about sports, it will pull up discussions where we talked about sports. If I ask about documentation, it will remember episodes where we talked about writing documentation.\nThis gives the model something resembling episodic recall: the ability to remember relevant experiences and bring them into working memory (the context window) when they\u0026rsquo;re useful.\nLayer 2: Reflective Memory Episodic memory alone would essentially be just a fancy search engine. Something that humans do that enables more complex learning is reflect on past episodes and draw generalized conclusions based on them. Taking inspiration from this process, I built an agentic reflection process into KenLM.\nWhen I exit a chat session, KenLM runs a reflection loop. A KenLM \u0026ldquo;reflection agent\u0026rdquo; analyzes the session\u0026rsquo;s episodes, searches for related patterns in past conversations, and updates a profile of hypotheses and learning goals.\nHere\u0026rsquo;s what the reflection system actually does:\nReviews the session — What patterns appeared in my behavior? What did I ask about? How did I respond to KenLM\u0026rsquo;s answers?\nSearches for evidence — The reflection agent can call a search_episodes(query) tool to find supporting patterns across all past conversations. This prevents overfitting to a single session.\nUpdates hypotheses — For each insight, it calls upsert_hypothesis() with a confidence score. Crucially, confidence only increases when patterns repeat across multiple episodes.\nMaintains learning goals — If the model notices something it doesn\u0026rsquo;t understand about me yet, it creates a learning goal—a question to explore in future interactions.\nHere\u0026rsquo;s what my actual profile looks like after some use:\n{ \u0026#34;hypotheses\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_007\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;accuracy\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth is detail-oriented and expects accurate and precise information.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.6, \u0026#34;evidence_episode_ids\u0026#34;: [ 90, 91, 89, 92 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_008\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;transparency\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth values transparency and honesty in communication.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 89 ] }, { \u0026#34;id\u0026#34;: \u0026#34;hypothesis_009\u0026#34;, \u0026#34;dimension\u0026#34;: \u0026#34;task_compliance\u0026#34;, \u0026#34;claim\u0026#34;: \u0026#34;Kenneth expects tasks to be completed accurately and without unnecessary steps.\u0026#34;, \u0026#34;confidence\u0026#34;: 0.8, \u0026#34;evidence_episode_ids\u0026#34;: [ 154, 155, 156, 157, 158 ] } ], \u0026#34;learning_goals\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;learning_goal_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;How can we ensure that Kenneth\u0026#39;s instructions are followed without unnecessary repetition or recapitulation?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;closed\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-11-30T15:10:55.704097\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;lg_003\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;What are Kenneth\u0026#39;s specific expectations for the precision and detail in project updates?\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;last_considered\u0026#34;: \u0026#34;2025-12-02T15:51:56.904564\u0026#34; } ] } The reflection prompt is designed to be conservative. I explicitly instruct the model to start hypotheses with low confidence (0.2–0.4) and only raise confidence when it sees repeated evidence. This should hopefully prevent the kind of overconfident conclusions that can come from a single unusual interaction.\nLayer 3: The Forgetting System A pretty important piece of learning is actually the process by which we decide which pieces of information aren\u0026rsquo;t relevant. If we were to allow our episodic vector database to grow unbounded with every user interaction, it would quickly become slow and stop being useful during inference or reflection. Moreover, humans also don\u0026rsquo;t retain every bit of information that we are exposed to—we have complex systems that decide what information is worth retaining based on various factors.\nMy algorithm is a bit more rudimentary than the process that the human brain has, but it seems to be working fine so far. It essentially calculates an \u0026ldquo;importance\u0026rdquo; score for every memory, which is just a weighted sum of a couple factors:\nimportance = w1 * log(1 + access_count) # Access frequency + w2 * is_protected # Protection bonus + w3 * is_evidence # Evidence bonus + w4 * exp(-age_days / tau) # Recency decay This system assigns a high score to frequently accessed memories, memories that the AI has decided it wants to \u0026ldquo;protect,\u0026rdquo; memories that are evidence for hypotheses that it has reasoned about, and memories that are recent. Memories that are old, infrequently accessed, and not useful for any of the learning goals or hypotheses the AI has created become candidates for forgetting.\nThe forgetting process is actually better understood as a summarization process. Instead of just deleting irrelevant memories, the system compacts them. It does this by summarizing \u0026ldquo;forgotten\u0026rdquo; episodes into condensed summaries and deleting the originals. This preserves knowledge while bounding storage growth—similar to how human memories become \u0026ldquo;fuzzy\u0026rdquo; over time.\nHow It Works End-to-End Let me walk through a complete cycle to show how these pieces fit together.\nStep 1: Message Received: I type something like: \u0026ldquo;Can you write me a short essay about my favorite sports team?\u0026rdquo;\nStep 2: RAG Retrieval: KenLM embeds my query and searches the episodic index. It finds 3 relevant past conversations about sports and essay preferences.\nStep 3: Context Assembly: The system prompt is built from:\nCore identity document (who KenLM is, its purpose) Current profile (hypotheses + learning goals) RAG episodes (the relevant past conversations) Behavioral instructions (meta-prompt generated at session start) Step 4: Deliberation: KenLM plans its approach: \u0026ldquo;Kenneth wants an essay about his favorite sports team. Based on my hypothesis about preferring conciseness, I should provide a short essay without excessive explanation. Based on our past conversations, his favorite sports team is the Seattle Sounders. I don\u0026rsquo;t need any external tools for this.\u0026rdquo;\nStep 5: Response Generation: KenLM generates a response, informed by the plan and all the context. The response is concise and focused—because that\u0026rsquo;s what it\u0026rsquo;s learned that I prefer.\nStep 6: Episode Logging: The conversation is saved as a new episode and indexed in the vector store.\nStep 7: Session End → Reflection: When I exit the chat, the reflection agent wakes up. It reviews the session, searches for patterns, and decides whether to update any hypotheses or learning goals.\nStep 8: Next Session → Meta-Prompting: The next time I start a chat, KenLM reads the (potentially updated) profile and generates fresh behavioral instructions. Then, the cycle continues.\nResults and Observations After using this system for a little bit, here are some of the results I\u0026rsquo;ve observed:\nWhat\u0026rsquo;s working Certain hypotheses stick very well. For example, early on, I found myself repeatedly asking KenLM to \u0026ldquo;be more concise\u0026rdquo; and to stop repeating itself. The reflection system noticed this pattern, formed a hypothesis, and now the model defaults to shorter responses. I rarely have to give feedback about the length of its responses anymore.\nRAG retrieval is surprisingly useful. When I ask about something I discussed in a past conversation, relevant context often appears in the model\u0026rsquo;s awareness. It doesn\u0026rsquo;t always explicitly reference the past conversation, but you can tell it\u0026rsquo;s informed by it. I can refer to things like \u0026ldquo;my favorite sports team\u0026rdquo; or \u0026ldquo;where I live\u0026rdquo; and the model remembers what those things are as they relate to me.\nMeta-prompting effectively influences personality. Each session feels more like what I\u0026rsquo;m looking for from a conversation with an LLM. It\u0026rsquo;s dropped the overly sycophantic tendencies, phrases like \u0026ldquo;Certainly!\u0026rdquo; or \u0026ldquo;You\u0026rsquo;re absolutely right!\u0026rdquo;, and other LLM-isms that have annoyed me. The behavioral instructions provide a stable \u0026ldquo;personality\u0026rdquo; for that session, and that personality is consistently adapting to my preferences.\nWhat\u0026rsquo;s Challenging Certain habits are hard to break. The model really loves markdown. I mean, it really loves outputting markdown formatting. No matter how many times I tell it it\u0026rsquo;s a command line tool and markdown is not helpful, it still really loves outputting neat markdown responses. Now, it HAS gotten a little better after much feedback, but I anticipate its preference for markdown is baked into the model weights itself, and not so easy to override through prompting. This is one of the clearest examples I\u0026rsquo;ve found so far of the limitations of this learning system.\nThe 7B model has limitations. My base KenLM model is a fine-tuned Qwen 2.5 7B. It\u0026rsquo;s good, but it occasionally misunderstands, hallucinates, or generates malformed tool calls. A larger model would likely perform better, but I wanted something that runs locally on my MacBook.\nThis process adds significant latency to the chat process. The RAG retrieval, meta-prompting, and reflection processes all take time. Using a model that\u0026rsquo;s running on my laptop means it can feel painfully slow at times.\nWhat this means This was a fun experiment, but what does this mean? Is there any use for systems like these in industry?\nI\u0026rsquo;m still far from an expert, but off the top of my head I can think of a couple potential applications of systems like these to develop real AI systems that solve real-world problems. Giving an LLM the ability to learn from experience opens the door to all kinds of systems we don’t really have today.\nThe obvious application is personalization: assistants that remember your preferences and workflows, which is what KenLM is. However, I think the implications go beyond “your AI knows your favorite sports team.” For example, imagine an educational model that can learn how you learn and what you know, remember your strong and weak areas, and adapt its explanations to your thinking style. This model could act as a more helpful learning companion than a model with no such memory capabilities.\nAnother application I can think of is creating models that improve themselves by optimizing for factors outside of human interaction. Imagine a model that initially struggles with chain-of-thought reasoning or tool usage—say something that consistently runs into the same error when trying to write python code or execute a web search. Currently, AI systems have no way to notice these things and improve. Human engineers need to monitor and step in, adjusting prompts, doing reinforcement learning, or collecting data with which to fine-tune in order to eke out a little bit better performance.\nA memory-equipped model changes this dynamic. For anything it\u0026rsquo;s trying to optimize for, be that reasoning or tool use, it can:\nnotice failed patterns by reasoning about them and remembering past experiences see successful patterns internalize the better method and adjust for future tasks Instead of updating billions of weights, the model updates a small set of learned strategies derived from lived experience. Over time, the model becomes a better problem solver not because it grew larger, but because it learned.\nThis all can potentially allow for smaller, more efficient models to rapidly adapt to specific use cases, reducing the industry\u0026rsquo;s reliance on frontier, compute-heavy models and allowing for more innovation and experimentation at the edge. Instead of treating model improvement as something that can only happen upstream, by massive labs with huge quantities of (often stolen) data, you get a world where models improve downstream, close to the user, close to the task, and close to the environment they operate in.\nIn that world, you don’t need a 400B-parameter model for everything. You need a reasonably capable base model with the right scaffolding: a memory system, a reflection system, and a mechanism for gradually internalizing the lessons it learns. Given enough experience, my intuition is that a modest model can outperform a much larger one on the tasks it specializes for, because it has context, history, and self-derived strategies the bigger model simply doesn’t have.\nIn the end, this really was just a weekend experiment, nothing more than an excuse to poke at a problem that’s been stuck in my head. I do, however, think that there are probably real ideas worth exploring here. I don’t know exactly where it will go yet, but I’m excited to keep experimenting and seeing how far a small model can go when you let it learn from its own experiences.\n","permalink":"http://localhost:1313/posts/giving-my-local-llm-a-memory/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLLMs are incredible at many things. If you stop and think about it for a moment, they\u0026rsquo;re absolutely remarkable. Through a relatively simple decoder-only transformer architecture, we’ve created what is essentially a natural-language computer that mimics intelligence. With the right orchestration, these models can act as tools, reasoning engines, or even full agents that automate entire chunks of our personal and professional lives.\u003c/p\u003e\n\u003cp\u003eHowever, as powerful as they are, today’s LLMs still fall short of anything resembling human intelligence. As \u003ca href=\"https://abehrouz.github.io/files/NL.pdf\"\u003ethis paper\u003c/a\u003e from Google points out, LLMs lack \u003cstrong\u003eneuroplasticity\u003c/strong\u003e—the ability to change their structure over time in response to new experiences. In practical terms, once an LLM is trained, its memory becomes \u003cem\u003efrozen\u003c/em\u003e. The model ends up with long-term semantic knowledge baked into its weights and a short-term working memory constrained by its context window, but no built-in mechanism for converting useful short-term experiences back into lasting knowledge.\u003c/p\u003e","title":"Giving My Local LLM a Memory"},{"content":"Hey, I\u0026rsquo;m Kenneth! I\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\nMy Background Software Engineer - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect! Build AI Systems — I create personal tools to explore how AI can enhance personal productivity and creativity Run Experiments — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project Write \u0026amp; Share — I document my learnings here so others can benefit from the journey My Interests When I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\nReading about machine learning and AI advancements Working on creative side projects Hanging out with my beautiful family Get in Touch Want to chat about AI, experiments, or just say hi? Feel free to reach out!\nEmail: kennethlayton99@gmail.com Twitter (X): kennethlayton13 LinkedIn: kenneth-layton\n","permalink":"http://localhost:1313/about/","summary":"\u003ch2 id=\"hey-im-kenneth\"\u003eHey, I\u0026rsquo;m Kenneth!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about building AI systems, running experiments, and sharing what I learn along the way.\u003c/p\u003e\n\u003ch3 id=\"my-background\"\u003eMy Background\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware Engineer\u003c/strong\u003e - I currently work full-time as a software engineer, so many of my experiments and blog posts come from that perspective. If you\u0026rsquo;re also a software engineer, I\u0026rsquo;d love to connect!\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild AI Systems\u003c/strong\u003e — I create personal tools to explore how AI can enhance personal productivity and creativity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Experiments\u003c/strong\u003e — I love testing ideas, whether it\u0026rsquo;s a new ML technique or a creative project\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite \u0026amp; Share\u003c/strong\u003e — I document my learnings here so others can benefit from the journey\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"my-interests\"\u003eMy Interests\u003c/h3\u003e\n\u003cp\u003eWhen I\u0026rsquo;m not tinkering with code and models, you\u0026rsquo;ll find me:\u003c/p\u003e","title":"About"}]